I"¿'<h1 id="recurrent-neural-networks-for-time-series">Recurrent Neural Networks for Time Series</h1>

<h2 id="intro">Intro</h2>

<ul>
  <li>Recurrent Neural Networks ì™€ Long Short Term Memory Networks ëŠ” ì‹œê³„ì—´ ë°ì´í„°ì˜ ì˜ˆì¸¡ê³¼ ë¶„ë¥˜ì— ë§¤ìš° ìœ ìš©í•¨</li>
  <li>Lambda Layer : ì‹ ê²½ë§ ë‚´ ì„ì˜ì˜ ì½”ë“œë¥¼ ë ˆì´ì–´ë¡œ í™œìš©í•  ìˆ˜ ìˆìŒ (ì „ì²˜ë¦¬ì™€ í›„ì²˜ë¦¬)
    <ul>
      <li>ëª…ì‹œì ì¸ ì „ì²˜ë¦¬ ë‹¨ê³„ë¡œ ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§í•œ ë‹¤ìŒ ì‹ ê²½ë§ì— ë„£ëŠ” ê²Œ ì•„ë‹ˆë¼ Lambda ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<h2 id="conceptual-overview">Conceptual overview</h2>

<ul>
  <li>RNN : ìˆœí™˜ ë ˆì´ì–´ë¥¼ í¬í•¨í•œ ì‹ ê²½ë§
    <ul>
      <li>ì‹œí€€ìŠ¤ ì…ë ¥ê°’ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„</li>
      <li>ì…ë ¥ê°’ì˜ í˜•íƒœ : ë°°ì¹˜ ì‚¬ì´ì¦ˆ, íƒ€ì„ìŠ¤íƒ¬í”„ (ìœˆë„ìš°ì‚¬ì´ì¦ˆ), ì»¬ëŸ¼ë””ë©˜ì „ (ë‹¤ë³€ëŸ‰) = 3ì°¨ì›
        <ul>
          <li>ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©í•œ ì…ë ¥ê°’ í˜•íƒœ : ë°°ì¹˜ ì‚¬ì´ì¦ˆ, ì…ë ¥ê°’ íŠ¹ì§• ìˆ˜ (ìœˆë„ìš° ì‚¬ì´ì¦ˆ)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>RNN Cell
    <ul>
      <li>ê²‰ìœ¼ë¡œ ë³´ê¸°ì—ëŠ” ì…€ì´ ë§ì€ ê²ƒ ê°™ì§€ë§Œ, ì…€ì€ í•˜ë‚˜ ë¿ì´ê³  ì´ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ê°’ì„ ì‚°ì¶œ</li>
      <li>ì…ë ¥ê°’ì´ 2ê°œ (X ê°’ê³¼ ìƒíƒœë²¡í„° H ê°’) - ìƒíƒœë²¡í„°ê°’ì„ ì´ìš©í•´ ì´ì „ ì…ë ¥ê°’ì˜ ì”ì¡´ ë°ì´í„°ë¥¼ ì „ë‹¬ë°›ìŒ</li>
      <li>ì…ë ¥ì°¨ì› (ì˜ˆ: íƒ€ì„ìŠ¤íƒ¬í”„ê°€ 30ê°œ) ë§Œí¼ ë°˜ë³µ</li>
    </ul>
  </li>
</ul>

<h2 id="rnn-notebook">RNN Notebook</h2>

<ul>
  <li>jupyter notebook ìë£Œ</li>
</ul>

<h2 id="shape-of-the-inputs-to-the-rnn">Shape of the inputs to the RNN</h2>

<ul>
  <li>ë°ì´í„°ì˜ í˜•íƒœ, ë°ì´í„°ë¥¼ ë¶„í• í•œ ë°°ì¹˜
    <ul>
      <li>ì˜ˆì‹œ
        <ul>
          <li>Window size ê°€ 30 : ì‹œê°„ ë‹¨ê³„ê°€ 30</li>
          <li>4ê°œë¡œ ì¼ê´„ ì²˜ë¦¬ : ë°°ì¹˜ê°’ 4</li>
          <li>ì…ë ¥ í˜•íƒœëŠ” 4 * 30 * 1</li>
        </ul>
      </li>
      <li>ì…€ì˜ ê´€ì 
        <ul>
          <li>í•˜ë‚˜ì˜ ì…€ì€ ê³ ì •ëœ ì‹œê°„ ë‹¨ê³„ì—ì„œ (Batch Size : 4 * 1) ì˜ ì…ë ¥ì„ ë°›ìŒ</li>
          <li>ë ˆì´ì–´ ë‚´ ë©”ëª¨ë¦¬ì…€ì´ 3ê°œì˜ ë‰´ëŸ°ìœ¼ë¡œ êµ¬ì„±ëœë‹¤ë©´</li>
          <li>ì¶œë ¥ê°’ í–‰ë ¬ì€ 4 * 3</li>
          <li>ì¶œë ¥ í˜•íƒœëŠ” 4(Batch Size) * 30(Window Size) * 3(Unit Size)</li>
        </ul>
      </li>
      <li>ë‹¨ìˆœ RNN ì—ì„œì˜ ìƒíƒœ ì¶œë ¥ê°’ H ëŠ” ì¶œë ¥ê°’ í–‰ë ¬ Y ì™€ ë™ì¼í•¨</li>
      <li>ì¼ë¶€ ê²½ìš°ì—ëŠ” ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥í•˜ë˜, ì¶œë ¥ê°’ì˜ ê²½ìš° ë°°ì¹˜ ë‚´ ê° ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ ë‹¨ì¼ ë²¡í„°ë¥¼ ì–»ê³  ì‹¶ì€ ê²½ìš°ê°€ ìˆìŒ
        <ul>
          <li>ë§ˆì§€ë§‰ (ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ìŠ¤í…-Window) ì„ ì œì™¸í•˜ê³  ëª¨ë“  ì¶œë ¥ê°’ì„ ë¬´ì‹œ</li>
          <li>ì‹œí€€ìŠ¤ ì¶œë ¥ê°’ì„ ë„ì¶œí•˜ë ¤ë©´ ë ˆì´ì–´ë¥¼ ìƒì„±í•  ë•Œ return_sequences ë¥¼ True ë¡œ ì§€ì •í•´ì•¼ í•¨
            <ul>
              <li>í•˜ë‚˜ì˜ RNN ë ˆì´ì–´ë¥¼ ë‹¤ë¥¸ ë ˆì´ì–´ ìœ„ì— ìŠ¤íƒœí‚¹ í• ë•Œ ì´ ì‘ì—…ì´ ë°˜ë“œì‹œ í•„ìš”</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="outputting-a-sequence">Outputting a sequence</h2>

<ul>
  <li>
    <p>ì ì¸µ ì˜ˆì‹œ</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  model = tf.keras.models.Sequential([
	tf.keras.layers.SimpleRNN(40, return_sequences=True, input_shape = [None,1]),
	tf.keras.layers.SimpleRNN(40),
	tf.keras.layers.Dense(1),
  ])
</code></pre></div>    </div>

    <ul>
      <li>íƒ€ RNN ë ˆì´ì–´ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼ í•˜ëŠ” RNN ë ˆì´ì–´ì— return_sequences ë¥¼ True ë¡œ ì„¤ì •</li>
      <li>Dense ë ˆì´ì–´ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼ í•˜ëŠ” RNN ë ˆì´ì–´ëŠ” ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ë‹¨ê³„ì˜ ê²°ê³¼ê°’ë§Œì„ ì¶œë ¥</li>
      <li>input_shape (ë°°ì¹˜ ì‚¬ì´ì¦ˆ) ë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ : ì–´ë–¤ í¬ê¸°ë“  ìƒê´€ì´ ì—†ìœ¼ë‹ˆ ì •ì˜í•  í•„ìš”ê°€ ì—†ìŒ</li>
      <li>Timestamp ê°’ì„ None ìœ¼ë¡œ ì„¤ì • : ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ê´€ê³„ ì—†ì´ ì…ë ¥ê°’ì„ ë°›ìŒ</li>
      <li>ë§ˆì§€ë§‰ ì°¨ì›ì´ 1ë¡œ ë˜ì–´ìˆëŠ” ì´ìœ  : ì¼ë³€ëŸ‰ ì‹œê³„ì—´ì„ ë‹¤ë£¨ê¸° ë•Œë¬¸</li>
      <li>ë‘ë²ˆì§¸ ì¸µ RNN ë ˆì´ì–´ì— return_sequences ê°’ì„ True ë¡œ ì„¤ì •í•  ê²½ìš°
        <ul>
          <li>ì‹œí€€ìŠ¤ ê°’ì´ ì¶œë ¥ë¨</li>
          <li>Keras ëŠ” ê° ì‹œê°„ ë‹¨ê³„ë³„ë¡œ ë™ì¼í•œ Dense ë ˆì´ì–´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í™œìš©í•¨</li>
          <li>ì…ë ¥ê°’ì´ ì‹œí€€ìŠ¤ì´ê³  ì¶œë ¥ê°’ ë˜í•œ ì‹œí€€ìŠ¤ì¼ ê²½ìš° : ì‹œí€€ìŠ¤ to ì‹œí€€ìŠ¤ RNN</li>
          <li>ì°¨ì›ì˜ ê°’ì€ RNN ë ˆì´ì–´ì˜ ìœ ë‹› ê°’ì— ë”°ë¼ ë³€ë™ë  ìˆ˜ ìˆìŒ</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lambda-layers">Lambda layers</h2>

<ul>
  <li>
    <p>ì˜ˆì‹œ</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  model = tf.keras.models.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),
                        input_shape=[window_size]),
    tf.keras.layers.SimpleRNN(40, return_sequences=True),
    tf.keras.layers.SimpleRNN(40),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	
</code></pre></div>    </div>

    <ul>
      <li>ì²« lambda ë ˆì´ì–´ <code class="language-plaintext highlighter-rouge">tf.expand_dims(x, axis=-1)</code> : ê¸°ì¡´ window ìƒì„± function ì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ê¸° ìœ„í•´ ì°¨ì›ì„ í•˜ë‚˜ ëŠ˜ë¦¼ (2ì°¨ì›-&gt;3ì°¨ì›)</li>
      <li>ë§ˆì§€ë§‰ lambda ë ˆì´ì–´ <code class="language-plaintext highlighter-rouge">lambda x: x * 100.0</code> : RNN ì˜ ê¸°ë³¸ í™œì„±í•¨ìˆ˜ tanh ì˜ ì¶œë ¥ê°’ -1 ~ 1 &gt; ì‹œê³„ì—´ ê°’ì€ 10ê°œ ë‹¨ìœ„ë¡œ êµ¬ì„±ë˜ê³ , ë¹„ìŠ·í•œ ê°’ìœ¼ë¡œ ì¶œë ¥ê°’ì„ ì˜¬ë¦¬ë©´ í•™ìŠµì— ë„ì›€ì´ ë¨</li>
    </ul>
  </li>
</ul>

<h2 id="adjusting-the-learning-rate-dynamically">Adjusting the learning rate dynamically</h2>

<ul>
  <li>
    <p>ì˜ˆì‹œ</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  train_set = windowed_dataset(x_train, window_size, batch_size=128,
      shuffle_buffer=shuffle_buffer_size)

  model = tf.keras.models.Sequential([
          tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1), input_shape=[None]),
          tf.keras.layers.SimpleRNN(40, return_sequences=True),
          tf.keras.layers.SimpleRNN(40),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 100.0)
      ])

  lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))

  optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)

  model.compile(loss=tf.kears.losses.Huber(),
                  optimizer=optimizer,
                  metrics=["mae"])

  history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])
</code></pre></div>    </div>

    <ul>
      <li>callback í•¨ìˆ˜ë¥¼ í™œìš©, epoch ì§„í–‰ ë³„ë¡œ í•™ìŠµë¥ ì„ ì•½ê°„ ë³€ê²½</li>
      <li>Huber ì†ì‹¤í•¨ìˆ˜ : ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ëŠ” ì†ì‹¤í•¨ìˆ˜, ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ë§ì´ ì„ì—¬ìˆì„ ë•Œ ì‹œë„í•´ë³¼ë§Œ í•¨
        <ul>
          <li>squared error loss ë³´ë‹¤ ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•¨</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lstm">LSTM</h2>

<ul>
  <li>RNN
    <ul>
      <li>X ê°€ ì…€ì— íˆ¬ì…ë˜ë©´ Y ê²°ê³¼ê°’ê³¼ H ìƒíƒœë²¡í„°ê°€ ì¶œë ¥ë˜ê³ , ì´ëŠ” ë‹¤ìŒ ì…€ì— ì˜í–¥ì„ ì¤Œ</li>
      <li>Step ì´ ì§„í–‰ë˜ë©´ì„œ ì´ˆê¸° H ìƒíƒœë²¡í„°ì˜ ì˜í–¥ë„ëŠ” ì ì  ì‘ì•„ì§</li>
    </ul>
  </li>
  <li>LSTM
    <ul>
      <li>ì „ì²´ í›ˆë ¨ ê¸°ê°„ ë™ì•ˆ ìƒíƒœë¥¼ ìœ ì§€í•´ì£¼ëŠ” ì…€ ìƒíƒœë¥¼ ì¶”ê°€í•¨</li>
      <li>ìƒíƒœ ê°’ì´ ì…€ ê°„ì— ì´ë™ì„ í•˜ê³  Step ì‚¬ì´ë¥¼ ì´ë™í•˜ë©´ì„œ ë” ì˜ ìœ ì§€ë  ìˆ˜ ìˆê²Œí•¨ - ì• ë‹¨ê³„ì— ìˆë˜ ë°ì´í„°ê°€ ì „ì²´ ì¶”ì •ì¹˜ì— ë” í° ì˜í–¥ì„ ì¤Œ</li>
      <li>ìƒíƒœëŠ” ì–‘ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¼ ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<h2 id="coding-lstms">Coding LSTMs</h2>

<ul>
  <li>
    <p>ì˜ˆì‹œ</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
</code></pre></div>    </div>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">tf.keras.backend.clear_session()</code> : ë‚´ë¶€ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”. ì´í›„ ë²„ì „ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³  ì—¬ëŸ¬ ëª¨ë¸ì„ ì‹œí—˜í•´ ë³¼ ìˆ˜ ìˆìŒ</li>
      <li><code class="language-plaintext highlighter-rouge">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))</code> : 32ê°œ ì…€ì˜ ë‹¨ì¼ LSTM ë ˆì´ì–´ ì¶”ê°€. ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ì–‘ë°©í–¥ìœ¼ë¡œ ë§Œë“¦</li>
    </ul>
  </li>
  <li>
    <p>ì˜ˆì‹œ</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
</code></pre></div>    </div>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))</code> : LSTM ë ˆì´ì–´ë¥¼ í•œì¸µ ë” ìŒ“ìŒ, retrun_sequences ë¥¼ True ë¡œ ì„¤ì •í•´ì•¼ë§Œ í•¨</li>
    </ul>
  </li>
</ul>
:ET