<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://bluesplatter.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bluesplatter.com/" rel="alternate" type="text/html" /><updated>2023-07-07T14:11:03+09:00</updated><id>https://bluesplatter.com/feed.xml</id><title type="html">Bluesplatter</title><subtitle>전문적이지 않은 정보들, 감상, 즉흥적인 내용들</subtitle><author><name>HY03</name><email>hyunik03@gmail.com</email></author><entry><title type="html">Sample-based Learning Methods - 01. Week 1. Monte Carlo Methods for Prediction &amp;amp; Control</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1/" rel="alternate" type="text/html" title="Sample-based Learning Methods - 01. Week 1. Monte Carlo Methods for Prediction &amp;amp; Control" /><published>2023-06-12T15:00:00+09:00</published><updated>2023-06-12T15:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1/">&lt;h2 id=&quot;관련-자료-rlbook2018-pages-91-104&quot;&gt;관련 자료 (RLbook2018 Pages 91-104)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Monte Carlo Methods
    &lt;ul&gt;
      &lt;li&gt;개요
        &lt;ul&gt;
          &lt;li&gt;이번 챕터에서는 환경에 대한 완전한 지식을 가정하지 않는다.&lt;/li&gt;
          &lt;li&gt;몬테 카를로 방식은 경험만을 필요로 한다.
            &lt;ul&gt;
              &lt;li&gt;환경과의 실제 혹은 가상의 상호작용을 통한 샘플 시퀀스 (상태, 행동, 보상)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;실제 경험을 통해 배우는 것은 엄청난 이점인데, 환경 역학에 대한 선행 지식 없이 최적 행동을 얻을 수 있기 때문이다.&lt;/li&gt;
          &lt;li&gt;가상 경험을 통해 배우는 것도 마찬가지이다.
            &lt;ul&gt;
              &lt;li&gt;비록 모델이 필요하지만, 모델은 오직 샘플 전환만을 생성하면 되고, DP 처럼 모든 가능한 전환의 완전한 확률 분포가 필요한 것은 아니다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;많은 경우 원하는 확률 분포에 따라 샘플링된 경험을 생성하는 것은 쉽지만, 명시적인 형태로 분포를 얻는 것은 불가능하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Monte Carlo Methods
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 방식은 강화학습 문제를 샘플링된 리턴 값의 평균 기반으로 해결하는 방식이다.
            &lt;ul&gt;
              &lt;li&gt;잘 정의된 리턴 값이 가능하도록, 우리는 몬테카를로 방식을 episodic tasks 에만 정의한다.
                &lt;ul&gt;
                  &lt;li&gt;각 경험은 에피소드 단위로 나눈다.&lt;/li&gt;
                  &lt;li&gt;모든 에피소드는 어떠한 행동을 선택하더라도 필연적으로 종료된다.&lt;/li&gt;
                  &lt;li&gt;에피소드 완료 시에만 가치 추정 및 정책이 변경된다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;따라서 몬테카를로 방식은 에피소드 별로 증분이 된다.
                &lt;ul&gt;
                  &lt;li&gt;step-by-step (online) 방식은 아니다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;몬테카를로 라는 용어는 작업에 상당한 무작위 요소가 포함된 추정 방법에 광범위하게 사용되는 경우가 많다.&lt;/li&gt;
          &lt;li&gt;여기에서 우리는 전체 수익의 평균을 기반으로 하는 방법에 대하여 몬테카를로 방식이라 한다.
            &lt;ul&gt;
              &lt;li&gt;추후 학습할 부분 수익으로 학습하는 방법과 반대&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;몬테카를로 방식과 Bandit 방식
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 방식은 강화학습에서 상태-행동 쌍마다 샘플링하고 평균 반환값을 계산한다.
            &lt;ul&gt;
              &lt;li&gt;이는 2장에서 다룬 밴딧 방법과 유사하게 각 행동에 대한 샘플링과 평균 보상을 계산하는 것임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;차이점은 몬테카를로 방식에서는 여러 상태가 있고, 각각이 다른 밴딧 문제와 유사하게 작동하며, 각 문제들은 상호 관련되어 있다는 점이다.
            &lt;ul&gt;
              &lt;li&gt;즉, 하나의 상태에서 행동을 수행한 후, 이후 상태에서 수행한 행동에 따라 반환값이 영향을 받음.&lt;/li&gt;
              &lt;li&gt;모든 행동선택이 학습을 진행하고 있는 중이기에 이전 상태의 관점에선 비정상성 (nonstationary) 의 변화하는 문제가 됨.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;몬테카를로 방식과 GPI (Generalized Policy Iteration)
        &lt;ul&gt;
          &lt;li&gt;비정상성 (nonstationary) 를 해결하기 위해, GPI 의 아이디어를 적용한다.
            &lt;ul&gt;
              &lt;li&gt;기존 GPI 에선 MDP 의 사전지식을 통해 가치함수를 계산했다면, 여기서는 MDP 에서 주는 샘플 리턴값을 통해 가치함수를 학습한다.&lt;/li&gt;
              &lt;li&gt;가치함수와 상응하는 정책은 여전히 최적성을 얻기 위해 본질적으로 같은 방법 (GPI) 으로 상호작용한다.&lt;/li&gt;
              &lt;li&gt;GPI 에서 했던 동일한 과정으로, 몬테카를로 케이스의 경우 (경험만 샘플링 가능한 상황) 에도 적용한다.
                &lt;ul&gt;
                  &lt;li&gt;Prediction Problem&lt;/li&gt;
                  &lt;li&gt;Policy Improvement&lt;/li&gt;
                  &lt;li&gt;Control Problem&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Monte Carlo Prediction
    &lt;ul&gt;
      &lt;li&gt;개요
        &lt;ul&gt;
          &lt;li&gt;우리는 시작으로, 몬테카를로 방식을 이용한 주어진 정책의 상태가치함수의 학습을 고려해 볼 것이다.&lt;/li&gt;
          &lt;li&gt;상태의 값은 예상되는 리턴 값 (미래의 할인된 기대 보상 합) 임을 상기한다.
            &lt;ul&gt;
              &lt;li&gt;단순히 해당 상태에 방문하고, 리턴값을 관찰하는 것이 경험을 통해 측정하는 명백한 방법이다.&lt;/li&gt;
              &lt;li&gt;더 많은 리턴 값이 관측되면, 평균은 반드시 예측 값에 수렴한다.&lt;/li&gt;
              &lt;li&gt;위의 아이디어가 몬테카를로 방식의 기반이 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;몬테카를로 예측
        &lt;ul&gt;
          &lt;li&gt;우리가 특별히 $v_\pi (s)$ 를 예측, 즉 정책 $\pi$ 아래의 s 상태의 값을 $\pi$ 정책을 따르고, s 상태를 지나는 에피소드 셋을 통해 예측한다고 가정&lt;/li&gt;
          &lt;li&gt;에피소드 내에서 각각의 상태 s 에 발생하는 것을 s 에 방문한다 (visit to s) 라 한다.
            &lt;ul&gt;
              &lt;li&gt;s 는 같은 에피소드 내에서도 여러 번 방문될 수 있다.&lt;/li&gt;
              &lt;li&gt;첫 방문을 first visit to s 라 칭하도록 한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;몬테카를로 방식 (MC method) 을 나누는 기준
            &lt;ul&gt;
              &lt;li&gt;every-visit MC method
                &lt;ul&gt;
                  &lt;li&gt;s 를 방문할 때마다의 리턴 값의 평균을 구한다.&lt;/li&gt;
                  &lt;li&gt;함수 근사 (function approximation) 와 적격성 추적 (eligibility traces) 확장에 더 자연스럽다.&lt;/li&gt;
                  &lt;li&gt;나중에 (Chapter 9, 12) 다룰 방식&lt;/li&gt;
                  &lt;li&gt;아래 first-visit 방식에서 $S_t$ 가 에피소드 초반에 일어났는지 체크하는 부분을 제외하고 동일하다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_1_1_first_visit_mc_prediction.png&quot; alt=&quot;5_1_1_first_visit_mc_prediction&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;first-visit MC method
                &lt;ul&gt;
                  &lt;li&gt;s 의 첫 방문의 리턴 값의 평균을 구한다.&lt;/li&gt;
                  &lt;li&gt;1940년대 부터 가장 많이 연구 되는 방식&lt;/li&gt;
                  &lt;li&gt;이 장에서 다룰 방식&lt;/li&gt;
                  &lt;li&gt;Termination 상태부터 할인된 보상합을 계산하되, 이 값이 $S_t$ 가 되면 이전 에피소드 보상합에 포함시켜 평균을 구한다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;first-visit MC 와 every-visit MC 모두 $s$ 상태에 대한 방문 (혹은 첫 방문) 의 수가 무한대에 가까워질 경우 $v_\pi (s)$ 로 수렴한다.
                &lt;ul&gt;
                  &lt;li&gt;first-visit MC 의 경우&lt;/li&gt;
                  &lt;li&gt;각 리턴 값은 유한한 분산을 가진 $v_\pi (s)$ 의 독립적이고 동일하게 분포된 추정치이다.&lt;/li&gt;
                  &lt;li&gt;대수의 법칙에 따라 이러한 추정치의 평균 시퀀스는 기대값으로 수렴한다.&lt;/li&gt;
                  &lt;li&gt;각 평균은 그 자체로 편향되지 않은 추정치이며, 그 오차의 표준 편차는 $\frac{1}{\sqrt{n}}$ 으로 감소한다.
                    &lt;ul&gt;
                      &lt;li&gt;n 은 평균을 구한 반환값의 개수이다.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;every-visit MC 는 덜 직관적이지만, 그 추정치 또한 $v_\pi (s)$ 로 2차 수렴한다. (Singh and Sutton, 1996)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Example 5.1 : Blackjack
        &lt;ul&gt;
          &lt;li&gt;게임의 설명
            &lt;ul&gt;
              &lt;li&gt;블랙잭이라는 대중적인 카지노 카드 게임의 목적은 수치의 합이 21을 초과하지 않고 가능한 한 큰 카드를 얻는 것&lt;/li&gt;
              &lt;li&gt;모든 FACE 카드는 10으로 계산되며 에이스는 1 또는 11로 계산될 수 있습니다.&lt;/li&gt;
              &lt;li&gt;각 플레이어가 딜러와 독립적으로 경쟁하는 버전을 고려함.&lt;/li&gt;
              &lt;li&gt;게임은 딜러와 플레이어 모두에게 두 장의 카드를 나눠주는 것으로 시작됨.&lt;/li&gt;
              &lt;li&gt;딜러의 카드 중 하나는 위를 향하고 다른 하나는 아래를 향함.&lt;/li&gt;
              &lt;li&gt;플레이어가 즉시 21개(에이스와 10장의 카드)를 가지고 있으면 이를 내추럴이라고 함.&lt;/li&gt;
              &lt;li&gt;딜러도 내추럴을 가지고 있지 않는 한 플레이어가 이기며, 가지고 있을 경우 게임은 무승부가 됨.&lt;/li&gt;
              &lt;li&gt;플레이어가 내츄럴 카드를 가지고 있지 않다면, 21을 초과 (bust) 하지 않는 선에서, 추가 카드 한장을 요청 (hits) 하거나 멈출 (sticks) 수 있음.&lt;/li&gt;
              &lt;li&gt;21을 초과 (bust) 하면 패배&lt;/li&gt;
              &lt;li&gt;멈추면 (sticks) 딜러의 턴이 됨.&lt;/li&gt;
              &lt;li&gt;딜러는 선택의 여지 없이 고정된 전략에 따라 히트하거나 스틱함. 그는 17 이상의 합계이면 스틱하고, 그렇지 않으면 히트함.&lt;/li&gt;
              &lt;li&gt;딜러가 21을 초과 (bust) 하면 플레이어가 승리함.&lt;/li&gt;
              &lt;li&gt;그렇지 않은 경우 결과 (승리, 무승부) 는 최종 합이 21에 가까운 사람이 승리함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;MDP 정의
            &lt;ul&gt;
              &lt;li&gt;블랙잭을 플레이하는 것은 자연스럽게 일시적인 유한 MDP로 공식화됨.&lt;/li&gt;
              &lt;li&gt;각 게임은 에피소드&lt;/li&gt;
              &lt;li&gt;보상은 승리, 패배, 무승부에 따라 각각 +1, -1 또는 0 이 주어짐.
                &lt;ul&gt;
                  &lt;li&gt;이 종료 보상은 수익이기도 함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;게임 내의 모든 보상은 0. 보상 할인은 사용하지 않음 ($\gamma = 1$)&lt;/li&gt;
              &lt;li&gt;플레이어의 행동은 힛 또는 스틱임.&lt;/li&gt;
              &lt;li&gt;상태는 플레이어의 카드와 딜러의 표시 카드임.&lt;/li&gt;
              &lt;li&gt;이미 처리된 카드를 카운팅하는 이점이 없도록 무한한 덱 (교체 포함) 에서 카드가 처리된다고 가정함.&lt;/li&gt;
              &lt;li&gt;플레이어가 버스트되지 않고 11로 셀 수 있는 에이스를 보유하고 있으면 그 에이스는 사용 가능하다고 한다.
                &lt;ul&gt;
                  &lt;li&gt;이 경우 1로 계산하면 합계가 11 이하가 되기 때문에 항상 11로 계산됨.&lt;/li&gt;
                  &lt;li&gt;1로 계산하는 경우 플레이어는 항상 힛을 해야 하기 때문에 결정을 내릴 수 없음.&lt;/li&gt;
                  &lt;li&gt;힛을 했을 때 버스트 되는 경우 에이스를 1로 계산할 수 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;따라서 플레이어는 자신의 현재 합계(12–21), 딜러가 보여주는 카드(ace–10), 사용 가능한 에이스 보유 여부의 세 가지 변수를 기반으로 결정을 내림.
                &lt;ul&gt;
                  &lt;li&gt;이렇게 하면 총 200개의 상태가 됨.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;정책의 설정
            &lt;ul&gt;
              &lt;li&gt;플레이어의 합계가 20 또는 21이면 스틱, 그렇지 않으면 힛을 하는 정책을 고려한다.&lt;/li&gt;
              &lt;li&gt;몬테카를로 접근방식으로 이 정책에 대한 상태가치함수를 찾기 위해, 해당 정책으로 많은 블랙잭 게임을 시뮬레이션하고 각 상태에 따른 수익을 평균화함.&lt;/li&gt;
              &lt;li&gt;사용 가능한 에이스가 있는 상태에 대한 추정치는 이러한 상태가 덜 일반적이기 때문에 덜 확실하고 덜 규칙적임.&lt;/li&gt;
              &lt;li&gt;어쨌든 500,000 게임 후에 가치 함수는 매우 잘 근사됨.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_1_2_example_5_1_blackjack_1.png&quot; alt=&quot;5_1_2_example_5_1_blackjack_1&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;DP 와 비교한 몬테카를로의 장점
        &lt;ul&gt;
          &lt;li&gt;위의 블랙잭 게임에서 우리는 환경에 대한 완벽한 지식을 갖고 있지만, DP 를 사용하여 가치함수를 계산하는 것은 쉽지 않은 문제이다.
            &lt;ul&gt;
              &lt;li&gt;DP 방식의 경우 다음 이벤트의 분포를 필요 (특히, 4개의 인수를 가지는 환경역학 $p(s’,r | s,a)$)&lt;/li&gt;
              &lt;li&gt;블랙잭에 대해 이를 결정하는 것이 쉽지 않음.&lt;/li&gt;
              &lt;li&gt;예를 들어 플레이어의 합이 14이고, 힛을 선택하는 경우 딜러의 공개 카드에 따른 보상이 +1 이 되는 확률&lt;/li&gt;
              &lt;li&gt;DP를 적용하기 위해서는 모든 확률을 계산해야 하며, 이러한 계산은 종종 복잡하고 오류가 발생할 수 있음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;반면에, 몬테카를로 방식에서 필요한 샘플 게임을 생성하는 것은 쉽다.
            &lt;ul&gt;
              &lt;li&gt;샘플 에피소드만으로 작업하는 몬테카를로 방법이 환경 역학에 완전한 지식을 갖고 있는 경우에도 상당한 장점이 될 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_1_3_mc_diagram_1.png&quot; alt=&quot;5_1_3_mc_diagram_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;몬테카를로에서는 백업 다이어그램의 개념을 사용할 수 없음.
            &lt;ul&gt;
              &lt;li&gt;백업 다이어그램 : 업데이트될 루트 노드를 상단에 표시하고, 업데이트에 기여하는 보상, 예측값을 가진 모든 전이, 리프 노드를 아래에 표기&lt;/li&gt;
              &lt;li&gt;$v_\pi$ 의 몬테카를로 추정의 경우 루트는 상태노드이고, 그 아래에 최종 상태로 끝나는 특정 단일 에피소드의 전체 전이 궤적이 있다.&lt;/li&gt;
              &lt;li&gt;DP 다이어그램은 가능한 모든 전이를 보여주는 반면, 몬테카를로 다이어그램은 하나의 에피소드에서 샘플링된 전이만을 보여줌.&lt;/li&gt;
              &lt;li&gt;DP 다이어그램은 한 단계 전이만 포함하지만, 몬테카를로 다이어그램은 에피소드의 끝까지 이어짐.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;몬테카를로 방식의 중요한 사실은 각 상태의 추정치가 서로 독립적이라는 것임.
            &lt;ul&gt;
              &lt;li&gt;현 상태의 추정치는 다른 상태에 의존하지 않음. (DP에서 정의한 부트스트랩 방식을 사용하지 않음.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;하나의 상태의 값을 추정하는 계산 비용은 상태의 수와 독립적임.
            &lt;ul&gt;
              &lt;li&gt;즉, 몬테카를로 방식은 하나 또는 일부의 상태의 값만 필요한 경우 특히 유리함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Monte Carlo Estimation of Action Values&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;모델이 없을 경우에서의 몬테카를로 방식
        &lt;ul&gt;
          &lt;li&gt;모델을 사용할 수 없는 경우, 상태 값이 아닌 행동 값 (상태-행동 쌍) 을 추정하는 것이 특히 유용함.
            &lt;ul&gt;
              &lt;li&gt;모델이 있는 경우, 상태 값만으로 정책을 결정하기 충분함.
                &lt;ul&gt;
                  &lt;li&gt;DP 에 대해 다룰 때, 단순히 한 단계 앞을 보고, 보상과 다음 상태의 최상의 조합으로 이어지는 행동을 선택하면 됐었음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;그러나 모델이 없을 경우, 상태값만으로 충분하지 않음.
                &lt;ul&gt;
                  &lt;li&gt;정책을 제안하는 데 유용하게 사용하기 위해 각 행동의 가치를 명시적으로 추정해야 함.&lt;/li&gt;
                  &lt;li&gt;따라서 몬테카를로 방법의 주요 목표 중 하나는 $q_*$ 를 추정하는 것임.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;행동 값에 대한 정책 평가 문제를 고려하기
        &lt;ul&gt;
          &lt;li&gt;상태 $s$ 에서 시작하여 행동 $a$ 를 취한 후 정책 $\pi$ 를 따를 때 기대되는 보상인 $q_\pi (s,a)$ 를 추정하는 것.&lt;/li&gt;
          &lt;li&gt;이를 위한 몬테카를로 방법은 위의 상태가치함수를 구하는 것과 본질적으로 동일하나 이제 상태가 아닌 상태-행동 쌍에 대한 방문에 대해 이야기함.
            &lt;ul&gt;
              &lt;li&gt;상태-행동 쌍, $s,a$ 는 상태 $s$ 에서 행동 $a$ 를 취할 때 에피소드 내에서 방문했다 (visited) 라고 표현함.&lt;/li&gt;
              &lt;li&gt;every-visit MC 의 경우 모든 방문에 따른 리턴 값의 평균으로 상태-행동 쌍의 값을 추정함.&lt;/li&gt;
              &lt;li&gt;first-visit MC 의 경우 각 에피소드 내에서 처음으로 상태 $s$ 에 방문하고 행동 $a$ 를 취한 리턴 값의 평균을 구함.&lt;/li&gt;
              &lt;li&gt;이러한 방법은 방문 횟수가 무한대로 접근할 때 이전과 같이 2차적으로 수렴함. (변화가 점점 안정화 되는 패턴)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;유일한 복잡성은 많은 상태-행동 쌍이 방문되지 않을 수도 있다는 점임.
            &lt;ul&gt;
              &lt;li&gt;$\pi$ 가 결정론적 정책인 경우, $\pi$ 를 따를 때, 각 상태에서는 행동 중 하나에 대한 리턴값만 관찰하게 될 것임.&lt;/li&gt;
              &lt;li&gt;평균화할 리턴값이 없으므로, 다른 행동들의 몬테카를로 추정은 경험을 통해 개선되지 않을 것임.&lt;/li&gt;
              &lt;li&gt;행동 값을 학습하는 목적이 각 상태에서 사용할 수 있는 행동 중에서의 선택을 돕는 것이기 때문에 이것은 심각한 문제임.&lt;/li&gt;
              &lt;li&gt;행동의 대안을 비교하려면 현재 선호하는 것 뿐만 아니라 각 상태의 모든 행동의 가치를 추정해야 한다.&lt;/li&gt;
              &lt;li&gt;이것은 2장의 k-armed bandit 문제의 맥락에서 논의된 탐색 유지 문제의 일반적인 문제이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;지속적인 탐색의 보장
        &lt;ul&gt;
          &lt;li&gt;행동 값에 대한 정책 팽가(policy evaluation) 를 위해서는 모든 상태-행동 쌍이 방문되는 것을 보장해야 함.&lt;/li&gt;
          &lt;li&gt;이를 위한 한 가지 방법은 에피소드가 상태-행동 쌍에서 시작되도록 지정하고, 모든 쌍이 시작점으로 선택될 확률이 0이 아니게 하는 것이다.
            &lt;ul&gt;
              &lt;li&gt;이렇게 하면 모든 상태-행동 쌍이 에피소드의 무한한 횟수에 따라 방문되도록 보장됨.&lt;/li&gt;
              &lt;li&gt;이를 탐색 시작 가정 (assumption of exploring starts) 이라 한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;탐색 시작 가정은 때때로 유용하나, 일반적으로, 특히 환경과의 실제 상호 작용에서 직접 학습할 때에는 의존할 수 없게 된다.
            &lt;ul&gt;
              &lt;li&gt;이 경우, 시작 조건이 도움이 되지 않음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;모든 상태-행동 쌍이 발생하도록 보장하는 가장 일반적인 대안 접근 방식은 각 상태에서 모든 행동을 선택할 확률이 0이 아닌 확률로, 확률론적 정책만 고려하는 것이다.
            &lt;ul&gt;
              &lt;li&gt;이후 섹션에서 이 접근 방식의 두 가지 중요한 변형에 대해 설명할 것이나, 지금은 탐색 시작 가정을 유지한 전체 몬테카를로 제어 방법을 소개한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Monte Carlo Control
    &lt;ul&gt;
      &lt;li&gt;몬테카를로 제어(Control) 와 GPI (Generalized Policy Iteration)
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 추정 (Monte Carlo estimation) 이 제어, 즉 최적의 정책에 근접하는 데 어떻게 사용될 수 있는지 고려해보자.&lt;/li&gt;
          &lt;li&gt;전반적인 아이디어는 동적프로그래밍(DP) 와 동일한 패턴, 즉 일반화된 정책 반복(GPI:Generalized Policy Iteration)과 동일하다.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;GPI 에서는 근사 정책과 근사 가치함수를 모두 유지한다.&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_3_1_GPI_Diagram.png&quot; alt=&quot;5_3_1_GPI_Diagram&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;가치함수는 현재 정책의 가치함수에 더 근접하도록 반복적으로 변경되며, 정책은 현재 가치함수에 대해 반복적으로 개선됨.&lt;/li&gt;
              &lt;li&gt;이 두 종류의 변화는 각각 서로에게 움직이는 목표를 생성하기 때문에, 어느 정도 서로에게 불리하게 작용하지만, 정책과 가치함수 모두 최적에 접근하게 됨.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_3_2_traditional_policy_iteration.png&quot; alt=&quot;5_3_2_traditional_policy_iteration&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;우선, 고전적인 정책 반복의 몬테카를로 버전을 살펴보면 임의의 정책 $\pi_0$ 에서 시작하여 최적 정책 및 최적 행동가치함수로 끝나는 정책 평가 및 정책 개선의 전체 단계를 번갈아 가며 수행한다.&lt;/li&gt;
              &lt;li&gt;여기서 $\overset{E}{\rightarrow}$ 는 완전한 정책 평가를 나타내며, $\overset{I}{\rightarrow}$ 는 완전한 정책 개선을 나타낸다.&lt;/li&gt;
              &lt;li&gt;정책 평가는 이전 섹션에서 설명한 대로 수행된다.&lt;/li&gt;
              &lt;li&gt;실제 행동가치함수에 점근적으로 접근하기 위해, 많은 에피소드를 경험하게 된다.&lt;/li&gt;
              &lt;li&gt;우리는 탐색 시작(exploring starts) 으로 생성되는 무한한 수의 에피소드를 관찰한다고 가정한다.
                &lt;ul&gt;
                  &lt;li&gt;이러한 가정 하에 몬테카를로 방식은 임의의 $\pi_k$ 에 대해 $q_{\pi_k}$ 를 정확하게 계산한다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;정책 개선은 현재 가치함수 기반으로 정책을 탐욕화 하여 이루어짐&lt;/li&gt;
          &lt;li&gt;이 경우 우리는 행동가치함수를 가지고 있으므로, 탐욕 정책을 만드는데 환경 모델은 불필요하다.&lt;/li&gt;
          &lt;li&gt;어떠한 행동가치함수 $q$ 에서도, 상응하는 탐욕 정책은 하나이다. (모든 $s \in S$ 에 대해 결정론적으로 최대값을 가지는 행동을 선택할 경우)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_3_3_greedy_policy_q_function.png&quot; alt=&quot;5_3_3_greedy_policy_q_function&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;정책 개선은 $q_{\pi_k}$ 에 대한 탐욕 정책 $\pi_{k+1}$ 을 설계함으로서 이루어진다.&lt;/li&gt;
          &lt;li&gt;이 경우 policy improvement theorem 이 적용된다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_3_4_policy_improvement_theorem.png&quot; alt=&quot;5_3_4_policy_improvement_theorem&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;앞 장에서 이야기했던 것처럼 위 정리는 $\pi_{k+1}$ 이 $\pi_k$ 보다 균일하게 낫거나, 같음 (최적 정책일 경우) 을 보장한다.&lt;/li&gt;
          &lt;li&gt;위의 내용을 기반으로 전체 프로세스가 최적 정책과 최적 가치함수로 수렴을 진행한다는 것을 확신할 수 있음.&lt;/li&gt;
          &lt;li&gt;이러한 방식으로 몬테카를로 방식을 사용하여 환경 역학 지식 없이 샘플 에피소드만으로 최적의 정책을 찾을 수 있음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;몬테카를로 방식을 사용하기 위한 가정과 실제 해결책
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 수렴 보장을 위한 현실적이지 않은 가정
            &lt;ul&gt;
              &lt;li&gt;에피소드가 탐색 시작 (exploring starts) 을 한다는 점&lt;/li&gt;
              &lt;li&gt;무한히 많은 에피소드로 정책 평가를 할 수 있다는 점&lt;/li&gt;
              &lt;li&gt;실용적인 알고리즘을 얻으려면 위 두 가지 가정을 모두 제거해야 함.&lt;/li&gt;
              &lt;li&gt;이 장의 뒷부분 까지 우리는 첫 번째 가정에 대한 고려를 연기한다.&lt;/li&gt;
              &lt;li&gt;지금은 정책 평가가 무한한 수의 에피소드에서 작동한다는 가정에 초점을 맞춘다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;무한한 수의 에피소드 진행을 피하는 방법
            &lt;ul&gt;
              &lt;li&gt;사실 반복 정책 평가 (iterative policy evaluation) 와 같은 고전적인 DP 방법에서도 동일한 문제가 발생함. (이 역시 참 가치 함수에 점근적으로만 수렴한다.)&lt;/li&gt;
              &lt;li&gt;DP 와 몬테카를로 모두, 문제를 해결하는 두 가지 방법이 있다.&lt;/li&gt;
              &lt;li&gt;각 정책 평가에서 $q_{\pi_k}$ 를 근사화한다는 아이디어를 확고히 유지하고, 측정과 가정을 통해 추정치의 오차 크기와 확률에 대한 한계를 얻고, 그런 다음 각 정책 평가 중에 충분한 단계를 취하여 이러한 한계가 충분히 작음을 보장함.
                &lt;ul&gt;
                  &lt;li&gt;어느 정도의 근사 수준까지 올바른 수렴을 보장&lt;/li&gt;
                  &lt;li&gt;그러나 가장 작은 문제들을 제외한 모든 실 문제에 유용하려면 너무 많은 에피소드가 필요할 수도 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;정책 개선으로 돌아가기 전에 정책 평가를 완료하려는 시도를 포기함.
                &lt;ul&gt;
                  &lt;li&gt;각 평가 단계에서 가치함수를 $q_{\pi_k}$ 쪽으로 이동시키지만, 많은 단계를 거쳐야만 가까워짐.&lt;/li&gt;
                  &lt;li&gt;GPI 개념과 동일한 방식의 아이디어를 사용함.&lt;/li&gt;
                  &lt;li&gt;예를들어 value iteration 과 같이 정책 개선 사이에 정책 평가를 한 번만 수행. (in-place 버전의 경우 몇몇의 상태만 정책평가를 한 후 정책 개선을 진행)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;몬테카를로 ES (Monte Carlo with Exploring Starts)
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 정책 반복의 경우 에피소드 별로 평가와 개선을 번갈아 가며 수행하는 것이 자연스러움.&lt;/li&gt;
          &lt;li&gt;각 에피소드 후에 관찰된 리턴값은 정책 평가에 사용되며 에피소드에서 방문한 모든 상태에서 정책이 개선됨.&lt;/li&gt;
          &lt;li&gt;몬테카를로 ES (Monte Carlo with Exploring Starts) 라고 부르는 간단한 알고리즘은 아래의 psuedo-code 와 같은 형태이다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_3_5_monte_carlo_es.png&quot; alt=&quot;5_3_5_monte_carlo_es&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;몬테카를로 ES 에서 각 상태-행동 쌍에 대한 모든 리턴값은 관찰 당시 시행중인 정책에 관계 없이 누적 되고 평균화됨.&lt;/li&gt;
          &lt;li&gt;즉 몬테카를로 ES 는 차선 정책으로 수렴할 수 없음.
            &lt;ul&gt;
              &lt;li&gt;만약 그럴 경우 가치함수가 차선 정책으로 수렴하면서 정책이 변경됨.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;안정성은 정책과 가치함수 모두 최적일 때만 달성됨.
            &lt;ul&gt;
              &lt;li&gt;행동가치함수의 변화가 시간이 지남에 따라 감소하면서, 최적의 고정점으로의 수렴은 불가피해 보이지만 아직 공식적으로 증명되지 않았음.
                &lt;ul&gt;
                  &lt;li&gt;이것은 강화 학습에서 가장 기본적인 미해결 이론적 질문 중 하나임.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Example 5.3 : Solving Blackjack
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 ES 를 블랙잭에 적용하는 것은 간단함.&lt;/li&gt;
          &lt;li&gt;에피소드는 모두 시뮬레이션 게임이기 때문에 모든 가능성을 포함하는 탐색 시작의 준비가 쉬움.
            &lt;ul&gt;
              &lt;li&gt;이 경우 딜러의 카드, 플레이어의 합계, 플레이어가 사용 가능한 에이스를 가지고 있는지 여부를 모두 동일 확률로 무작위 선택함.&lt;/li&gt;
              &lt;li&gt;초기 정책으로 이전 블랙잭의 예시에서 평가된 정책을 사용하며, 20 또는 21에만 적용됨.&lt;/li&gt;
              &lt;li&gt;초기 행동가치함수는 모든 상태-행동 쌍에 대해 0 로 설정.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_3_6_blackjack_with_monte_carlo_es.png&quot; alt=&quot;5_3_6_blackjack_with_monte_carlo_es&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Figure 5.2 는 몬테카를로 ES 가 찾아낸 최적의 정책을 보여줌.&lt;/li&gt;
          &lt;li&gt;이 정책은 사용 가능한 에이스에 대한 정책에서 가장 왼쪽 노치를 제외하고 Thorp(1966)의 “기본” 전략과 동일함.
            &lt;ul&gt;
              &lt;li&gt;이러한 불일치의 이유는 확실하지 않지만, 여기에 표기된 것이 실제로 우리가 설명한 블랙잭 버전의 최적 정책이라고 확신할 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Monte Carlo Control without Exploring Starts
    &lt;ul&gt;
      &lt;li&gt;On-policy 방식과 Off-policy 방식
        &lt;ul&gt;
          &lt;li&gt;탐색 시작 (Exploring starts) 이라는 희박한 가정을 피하기 위해, 모든 행동이 무한정으로 자주 선택되도록 에이전트가 해당 행동을 계속 선택하는 방식이 있다.&lt;/li&gt;
          &lt;li&gt;이를 보장하는 두 가지 접근 방식이 있으며, 이를 On-policy 방식과 Off-policy 방식이라고 부른다.
            &lt;ul&gt;
              &lt;li&gt;On-policy 방식 : 학습 정책과 탐색 정책이 동일한 경우를 가리킴 (탐색 정책을 사용하여 데이터를 수집하고 학습(정책개선)을 진행)&lt;/li&gt;
              &lt;li&gt;Off-policy 방식 : 학습 정책과 탐색 정책이 다른 경우 (학습 정책은 실제로 개선하고자 하는 정책, 탐색 정책은 환경 탐색, 경험을 얻기 위한 정책)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위의 몬테카를로 ES 방식은 On-policy 방식의 한 예이다.&lt;/li&gt;
          &lt;li&gt;이 섹션에서는 On-policy 몬테카를로 제어 방법의 설계에 대해 다루며, Off-policy 정책은 다음 섹션에서 고려한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;On-policy 제어 방식
        &lt;ul&gt;
          &lt;li&gt;일반적으로 on-policy 제어 방식은 soft 하다.
            &lt;ul&gt;
              &lt;li&gt;즉, $\pi (a | s) &amp;gt; 0$ for all $s \in S$ and all $a \in A(s)$&lt;/li&gt;
              &lt;li&gt;점차적으로 결정론적 최적 정책에 가까워진다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;이 장에서의 on-policy 방식은 $\varepsilon -greedy$ 정책을 사용한다.
            &lt;ul&gt;
              &lt;li&gt;대부분은 행동가치 추정값이 최대인 행동을 선택하나, $\varepsilon$ 의 확률로 임의의 행동을 선택한다.&lt;/li&gt;
              &lt;li&gt;모든 탐욕적이지 않은 행동은 최소의 선택 확률 $\frac{\varepsilon}{| A(s) |}$ 을 가지며, 탐욕 행동은 $1-\varepsilon + \frac{\varepsilon}{| A(s) |}$ 의 확률을 가진다.&lt;/li&gt;
              &lt;li&gt;(집합을 절대값 기호로 묶은 표기는 해당 집합의 원소 수를 나타낸다.)&lt;/li&gt;
              &lt;li&gt;$\varepsilon - greedy$ 정책은 $\varepsilon - soft$ 정책의 한 예이다.
                &lt;ul&gt;
                  &lt;li&gt;모든 상태와 행동에 대해, $\pi (a | s) \ge \frac{\varepsilon}{ | A(s) |}$ 이며, $\varepsilon &amp;gt; 0$&lt;/li&gt;
                  &lt;li&gt;$\varepsilon - greedy$ 정책은 $\varepsilon - soft$ 정책 중에서도 탐욕 정책에 가장 가까운 정책이다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;On-policy 몬테카를로 제어의 전반적인 아이디어는 여전히 GPI 의 아이디어이다.&lt;/li&gt;
          &lt;li&gt;몬테카를로 ES 와 마찬가지로 이번에도 first-visit MC 방식 으로 현 정책의 행동가치함수를 추정한다.
            &lt;ul&gt;
              &lt;li&gt;그러나 탐색 시작의 가정 없이 현재 가치함수를 탐욕정책으로 개선할 수 없다. (탐욕스럽지 않은 행동의 추가 탐색을 방지할 수 있음.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;GPI 는 정책이 탐욕정책으로 완전히 전환되는 것을 요구하지 않고, 탐욕정책의 바향으로 이동하기만 하면 됨.
            &lt;ul&gt;
              &lt;li&gt;On-policy 방식에서는 $\varepsilon - greedy$ 정책으로 이동할 것임.&lt;/li&gt;
              &lt;li&gt;$q_\pi$ 에 대한 $\varepsilon - soft$ 정책, $\varepsilon - greedy$ 정책 은 $\pi$ 와 같거나 더 나음을 보장한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;아래는 전체 알고리즘에 대한 내용이다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_4_1_on_policy_first_visit_mc_control.png&quot; alt=&quot;5_4_1_on_policy_first_visit_mc_control&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$q_\pi$ 에 기반한 어떠한 $\varepsilon - greedy$ 정책도 $\varepsilon - soft$ 정책보다 개선된다는 점은 policy improvement theorem 에 의해 보장된다.&lt;/li&gt;
          &lt;li&gt;$\pi’$ 를 $\varepsilon - greedy$ 정책이라 가정하면 아래의 수식이 성립된다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_4_2_e_greedy_e_soft_policy_improvement_theorem.png&quot; alt=&quot;5_4_2_e_greedy_e_soft_policy_improvement_theorem&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;policy improvement theorem 에 따라, $\pi’ \geq \pi$ (i.e., $v_{\pi’} (s) \geq v_\pi (s)$, for all $s \in S$).
            &lt;ul&gt;
              &lt;li&gt;이때 등식은 $\pi’$ 와 $\pi$ 모두 $\varepsilon - soft$ 정책 내에서 최적이며, 동시에 모든 타 $\varepsilon - soft$ 정책보다 좋거나 같을 때 성립한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$\varepsilon - soft$ 정책이 환경 내부로 이동한, 원래 환경과 동일한 새 환경을 고려한다.
            &lt;ul&gt;
              &lt;li&gt;새 환경은 원 환경과 동일한 행동, 동일한 상태 설정을 가진다.&lt;/li&gt;
              &lt;li&gt;만약 상태 s 에서 행동 a 를 취하는 경우, 확률 $1 - \varepsilon$ 으로 새 환경은 이전 환경과 동일하게 작동한다.&lt;/li&gt;
              &lt;li&gt;$\varepsilon$ 의 확률로, 같은 확률분포로 임의의 행동을 재선택하면, 새로운 행동을 선택한 이전의 환경처럼 동작한다.&lt;/li&gt;
              &lt;li&gt;보통의 정책으로 이 새로운 환경에서 할 수 있는 최선은, $\varepsilon - soft$ 정책으로 원래 환경에서 할 수 있는 최선과 동일함.&lt;/li&gt;
              &lt;li&gt;$\tilde{v_*}$ 는 새로운 환경에 대한 최적 가치함수를 나타냄.
                &lt;ul&gt;
                  &lt;li&gt;$v_\pi = \tilde{v_*}$ 인 경우에만 정책 $\pi$ 가 $\varepsilon - soft$ 정책 중에서 최적임.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_4_3_e_soft_policy_tilde_optimal_state_value_function.png&quot; alt=&quot;5_4_3_e_soft_policy_tilde_optimal_state_value_function&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;이 방정식은 $\tilde{v_*}$ 를 $v_\pi$ 로 치환한 것을 제외하고 이전 것과 동일하다.
            &lt;ul&gt;
              &lt;li&gt;$\tilde{v_*}$ 는 고유 솔루션이다.&lt;/li&gt;
              &lt;li&gt;따라서 $v_\pi = \tilde{v_*}$ 여야만 한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;지금까지 $\varepsilon - soft$ 정책에도 정책 반복이 작동한 다는 것을 살펴봄.
            &lt;ul&gt;
              &lt;li&gt;탐욕 정책의 자연스러운 개념을 사용하면, $\varepsilon - soft$ 정책 중에서 최상의 정책이 발견된 경우를 제외하고 모든 단계에서 개선이 보장됨.&lt;/li&gt;
              &lt;li&gt;이 분석은 각 단게에서 행동가치함수가 결정되는 방식과 별개이나, 정확하게 계산된다고 가정하였다.&lt;/li&gt;
              &lt;li&gt;한편으로 이는 탐색 시작이라는 가정을 없앤 것임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Off-policy Prediction via Importance Sampling&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Off-policy learning 의 정의
        &lt;ul&gt;
          &lt;li&gt;모든 제어(Control) 에 대한 학습방법은 딜레마에 직면함.
            &lt;ul&gt;
              &lt;li&gt;그들은 후속 최적 행동에 대해 조건부로 행동값을 학습하려고 함.&lt;/li&gt;
              &lt;li&gt;모든 행동을 탐색하기 위해 최적이 아닌 행동을 해야 함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;탐색적 정책에 따라 행동하는데 최적의 정책에 알수 있는 방법
            &lt;ul&gt;
              &lt;li&gt;On-policy 방식 : 절충안
                &lt;ul&gt;
                  &lt;li&gt;최적의 정책이 아니라 여전히 탐색하는 최적에 가까운 정책에 대한 작업 값을 학습함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Off-policy 방식 : 보다 직접적인 접근방식
                &lt;ul&gt;
                  &lt;li&gt;학습되어 최적의 정책이 되는 정책 (대상정책 : target policy) 과 탐험을 하며 행동을 생성하는 정책 (행동정책 : behavior policy)&lt;/li&gt;
                  &lt;li&gt;학습이 대상 정책에서 떨어진 (off) 데이터에서 발생한다 : Off-policy learning&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;On-policy 와 Off-policy
        &lt;ul&gt;
          &lt;li&gt;이 책의 나머지 부분에서는 On-policy 방식과 Off-policy 방식 모두를 고려함.&lt;/li&gt;
          &lt;li&gt;On-policy 방식
            &lt;ul&gt;
              &lt;li&gt;일반적으로 더 간단&lt;/li&gt;
              &lt;li&gt;먼저 고려되는 방식&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Off-policy 방식
            &lt;ul&gt;
              &lt;li&gt;추가 개념과 표기법이 필요하며, 데이터가 다른 정책의 것이기 때문에 분산이 더 크고, 수렴 속도가 느림.&lt;/li&gt;
              &lt;li&gt;더 강력하고 일반적임.&lt;/li&gt;
              &lt;li&gt;특이 케이스로 대상 정책 (target policy) 과 행동 정책 (behavior policy) 이 같으면 On-policy 방식이 된다. (포함)&lt;/li&gt;
              &lt;li&gt;응용 프로그램에서 더 많은 쓰임새가 있음.
                &lt;ul&gt;
                  &lt;li&gt;기존의 비학습 컨트롤러 또는 인간 전문가가 생성한 데이터를 학습할 수도 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;일부 사람들에게는 세상의 동적 모델을 학습하는 다단계 예측 모델을 배우는 데 중요한 역할을 하는 것으로 간주됨.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Off-policy 방식
        &lt;ul&gt;
          &lt;li&gt;대상정책 (target policy) 과 행동정책 (behavior policy) 이 모두 고정된 예측 문제를 가정
            &lt;ul&gt;
              &lt;li&gt;즉, $v_\pi$ 또는 $q_\pi$ 를 추정하려 하나, 우리가 가진 데이터는 정책 $b$ 를 따르는 에피소드 뿐이라고 가정. ($b \ne \pi$)&lt;/li&gt;
              &lt;li&gt;이 경우, $\pi$ 는 대상정책, $b$ 는 행동정책이며, 두 정책 모두 고정 및 주어진 것으로 간주한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;커버리지 가정 (assumption of coverage)
            &lt;ul&gt;
              &lt;li&gt;$b$ 의 에피소드를 사용하여 $\pi$ 의 값을 추정하려면, $\pi$ 에서 취한 모든 작업이 $b$ 에서도 적어도 가끔씩 수행되어야 함.&lt;/li&gt;
              &lt;li&gt;즉, $\pi (a | s) &amp;gt; 0$ 이면 $b (a | s)$ 를 요구한다.&lt;/li&gt;
              &lt;li&gt;커버리지 가정의 적용 범위 내에서 $b$ 는 $\pi$ 와 동일하지 않은 상태에서 확률론적이어야 함.&lt;/li&gt;
              &lt;li&gt;반면 목표정책 $\pi$ 는 결정론적일 수 있으며, 제어 응용프로그램에서 특히 중요한 사항임.&lt;/li&gt;
              &lt;li&gt;제어 측면에서, 대상정책은 일반적으로 행동가치함수의 현재 추정치의 결정론적 탐욕 정책임.&lt;/li&gt;
              &lt;li&gt;대상정책이 결정론적 최적 정책이 되는 반면, 행동정책은 확률론적이고 탐색적임. (예 : $\varepsilon - greedy$ 정책)&lt;/li&gt;
              &lt;li&gt;이 섹션에서는, $\pi$ 가 변하지 않고, 주어진 예측 문제를 고려함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;중요도 샘플링 (importance sampling)
        &lt;ul&gt;
          &lt;li&gt;거의 모든 Off-policy 방식은 중요도 샘플링 (importance sampling) 을 활용함.&lt;/li&gt;
          &lt;li&gt;중요도 샘플링은 다른 분포에서의 샘플을 통해 하나의 분포에서의 기댓값을 추정하는 일반적인 기법임.&lt;/li&gt;
          &lt;li&gt;중요도 샘플링 비율 (imporatnce-sampling ratio)
            &lt;ul&gt;
              &lt;li&gt;대상정책 (target policy) 과 행동정책 (behavior policy) 에 따른 경로 발생의 상대적 확률에 따라 리턴 값에 부여하는 가중치&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$S_t$ 에서 시작하여, 정책 $\pi$ 하에 후속 상태-행동 궤적 ($A_t, S_{t+1}, A_{t+1}, \cdots , S_T$) 의 확률은 아래와 같다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_5_1_importance_sampling_trajectory.png&quot; alt=&quot;5_5_1_importance_sampling_trajectory&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;여기에서 $p$ 는 상태전이확률 함수이다.&lt;/li&gt;
          &lt;li&gt;따라서, 대상정책과 행동정책 간 궤적의 상대적 확률 (importance-sampling ratio) 은 아래와 같다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_5_2_importance_sampling_ratio.png&quot; alt=&quot;5_5_2_importance_sampling_ratio&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;궤적 확률은 일반적으로 알려지지 않은 MDP의 전이확률에 의존하지만, 분자와 분모 모두에 동일하게 나타나 상쇄된다.&lt;/li&gt;
          &lt;li&gt;중요도 샘플링 비율은 MDP 가 아닌, 두 정책과 시퀀스에만 의존하게 된다.&lt;/li&gt;
          &lt;li&gt;우리가 원하는 추정값은 대상정책(traget policy) 의 기대되는 리턴값이다.
            &lt;ul&gt;
              &lt;li&gt;실제로 가진 값은 행동정책(behavior policy) 의 리턴값 $G_t$ 이다.&lt;/li&gt;
              &lt;li&gt;즉, $\mathbb{E}[G_t | S_t = s] = v_b(s)$ 이며, 이를 이용해 평균값을 구하여 $v_\pi$ 를 얻는 것은 불가능하다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;이 부분에서 중요도 샘플링 (importance sampling) 개념이 필요하다.
            &lt;ul&gt;
              &lt;li&gt;즉, ratio $p_{t:T-1}$ 값이 리턴값을 변환시켜, 올바른 예측값을 구하게 된다.&lt;/li&gt;
              &lt;li&gt;$\mathbb{E}[p_{t:T-1}G_t | S_t = s] = v_\pi (s)$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;중요도 샘플링 (importance sampling) 을 활용한 가치함수추정
        &lt;ul&gt;
          &lt;li&gt;이제 우리는 $v_\pi(s)$ 를 추정하기 위해 정책 $b$ 의 관찰된 에피소드 배치의 평균 리턴값을 계산하는 몬테카를로 알고리즘이 준비되었다.&lt;/li&gt;
          &lt;li&gt;여기에서 에피소드의 경계를 넘어 증가하는 방식으로 타임스텝 (time step) 번호를 지정하는 것이 편리하다.
            &lt;ul&gt;
              &lt;li&gt;즉, 배치의 첫 에피소드가 time step = 100 에서 종료했다면, 다음 에피소드는 t = 101 에서 시작됨.&lt;/li&gt;
              &lt;li&gt;이를 통해 특정 에피소드의 특정 타입스텝을 참조하기 위해 time-step number 를 사용할 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;수식 내 문자의 의미
            &lt;ul&gt;
              &lt;li&gt;$\mathcal{T} (s)$ : 상태 s 를 방문하는 모든 time-step number 집합
                &lt;ul&gt;
                  &lt;li&gt;이것은 every-visit 방식에 해당하는 의미이며, first-visit 방식의 경우 $\mathcal{T} (s)$ 는 에피소드 내 s를 처음 방문한 time-step number 만 포함한다.&lt;/li&gt;
                  &lt;li&gt;$T(t)$ : 시간 t 이후 첫 번째 종료를 의미&lt;/li&gt;
                  &lt;li&gt;$G_t$ : t 이후 $T(t)$ 까지의 리턴값&lt;/li&gt;
                  &lt;li&gt;${G_t}_{t \in \mathcal{T}(s)}$ : 상태 s 에 속하는 리턴값&lt;/li&gt;
                  &lt;li&gt;${p_{t:T(t)-1}}_{t \in \mathcal{T}(s)}$ : 위에 상응하는 중요도 샘플링 비율&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Ordinary Importance Sampling : $v_\pi (s)$ 를 추정하기 위해, 간단히 리턴 값을 비율로 조정하고 평균값을 구하는 수식은 아래와 같다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_5_3_ordinary_importance_sampling.png&quot; alt=&quot;5_5_3_ordinary_importance_sampling&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Weighted Importance Sampling : 0이 아닌 평균의 가중치 (weighted average) 를 사용한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/5_5_4_weighted_importance_sampling.png&quot; alt=&quot;5_5_4_weighted_importance_sampling&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Ordinary Importance Sampling 과 Weighted Importance Sampling
            &lt;ul&gt;
              &lt;li&gt;두 가지 종류의 중요도 샘플링을 이해하기 위해, 한 번의 리턴을 관찰한 후 first visit 방법 의 추정치를 고려해보자.&lt;/li&gt;
              &lt;li&gt;평균의 가중치 추정 (weighted-average estimate) (Weighted Importance Sampling의 경우) 에서는 단일 리턴에 대한 비율 $p_{t:T(t)-1}$ 이 분자와 분모에서 상쇄되어, 비율과 관계없이 추정치는 관찰된 리턴과 동일하게 됨. (비율이 0이 아닌 경우를 가정)&lt;/li&gt;
              &lt;li&gt;이 관찰된 리턴이 유일하게 관찰된 경우에는 합리적인 추정치이지만, 기대값은 $v_b(s)$ 가 아닌 $v_\pi (s)$ 이며, 이 통계적 의미에서 편향된 추정치이다.&lt;/li&gt;
              &lt;li&gt;반면 일반 중요도 샘플링 (Ordinary Importance Sampling) 추정의 first visit 버전은 항상 기대값이 $v_\pi (s)$ 이다. (편향이 없는 추정치)
                &lt;ul&gt;
                  &lt;li&gt;하지만 이는 극단적일 수 있음.&lt;/li&gt;
                  &lt;li&gt;예를 들어 가중치 비율이 10이라고 가정해보면, 이는 관측된 경로가 행동정책에 비해 대상정책에서 10배 더 가능성이 높다는 의미이다.&lt;/li&gt;
                  &lt;li&gt;이 경우 일반 중요도 샘플링 (Ordinary Importance Sampling) 의 추정치는 관측된 리턴의 10배가 된다.&lt;/li&gt;
                  &lt;li&gt;즉, 관측된 보상과는 상당히 멀어질 것이며, 이는 해당 에피소드의 경로가 대상정책을 매우 잘 대표한다고 생각되더라도 그렇다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;형식적으로, 두 종류의 중요도 샘플링의 첫 방문 방법의 차이는 편향과 분산으로 표현된다.
                &lt;ul&gt;
                  &lt;li&gt;일반 중요도 샘플링은 편향이 없지만, 가중 중요도 샘플링은 편향이 존재한다. (편향은 점진적으로 0으로 수렴함.)&lt;/li&gt;
                  &lt;li&gt;일반 중요도 샘플링의 분산은 일반적으로 무한대일 수 있다.
                    &lt;ul&gt;
                      &lt;li&gt;비율의 분산이 무한대일 수 있기 때문&lt;/li&gt;
                      &lt;li&gt;반면 가중 샘플링 방식의 추정치에서는 단일 리턴값의 가장 큰 가중치는 1이다.&lt;/li&gt;
                      &lt;li&gt;실제로 유한한 리턴값을 가정하면, 가중 중요도 샘플링 추정치의 분산은 비율의 분산 자체가 무한대일지라도 0으로 수렴함 (Precup, Sutton, and Dasgupta 2001)&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;가중 추정치의 분산은 일반적으로 매우 낮고 강력하게 선호된다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;함수 근사를 사용한 근사 방법을 더 쉽게 확장할 수 있기 때문에, 일반 중요도 샘플링 또한 학습할 것이다.&lt;/li&gt;
              &lt;li&gt;일반 중요도 샘플링과 가중 중요도 샘플링의 every visit 방식은 모두 편향이 있으며, 샘플의 수가 증가함에 따라 편향은 0으로 수렴한다.&lt;/li&gt;
              &lt;li&gt;every visit 방식은 방문한 상태를 추적할 필요가 없으며, 근사화에 쉽게 확장할 수 있기 때문에 선호되는 편이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;부연설명 (가치함수 V(s)를 구하는 Ordinary Importance Sampling과 Weighted Importance Sampling의 수식)
        &lt;ul&gt;
          &lt;li&gt;Ordinary Importance Sampling
            &lt;ul&gt;
              &lt;li&gt;$V(s) = \frac{1}{N} * \sum [ W(i) * R(i) ]$
                &lt;ul&gt;
                  &lt;li&gt;N : 수집된 샘플의 개수&lt;/li&gt;
                  &lt;li&gt;$W(i)$ : i 번째 샘플의 가중치&lt;/li&gt;
                  &lt;li&gt;$R(i)$ : i 번째 샘플의 보상&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Weighted Importance Sampling
            &lt;ul&gt;
              &lt;li&gt;$V(s) = \sum [ \frac{(W(i) * R(i))}{\sum W(i)} ]$
                &lt;ul&gt;
                  &lt;li&gt;$W(i)$ : i 번째 샘플의 가중치&lt;/li&gt;
                  &lt;li&gt;$R(i)$ : i 번째 샘플의 보상&lt;/li&gt;
                  &lt;li&gt;$\sum W(i)$ : 가중치의 합&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Ordinary Importance Sampling 은 모든 샘플이 동일한 중요도 ($\frac{1}{N}$) 를 가짐 (평균을 구함)&lt;/li&gt;
              &lt;li&gt;Weighted Importance Sampling 은 각 샘플마다 개별적인 중요도를 가짐. ($\frac{W(i)}{\sum W(i)}$)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-monte-carlo-methods&quot;&gt;Introduction to Monte Carlo Methods&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;What is Monte Carlo?
    &lt;ul&gt;
      &lt;li&gt;소개
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 라는 용어는 반복적으로 무작위 샘플링에 의지하는 추측방식을 광범위하게 일컷는 용어로 많이 쓰임.&lt;/li&gt;
          &lt;li&gt;강화학습에서는 상태의 시퀀스, 행동, 보상등의 경험으로부터 직접적으로 추정값에 접근하는 방식임.
            &lt;ul&gt;
              &lt;li&gt;경험으로부터 직접 학습한다는 것은 강력한 이점인데, 환경역학에 대한 사전지식 없이 정확한 가치함수를 추정할 수 있기 때문&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 방식이 샘플링된 상호작용을 통한 가치함수 예측에 어떻게 쓰이는지 이해하기&lt;/li&gt;
          &lt;li&gt;몬테카를로 방식을 통해 풀 수 있는 문제 식별하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습에서 DP 학습의 한계점
        &lt;ul&gt;
          &lt;li&gt;에이전트는 환경의 상태변이확률을 알고 있어야 함.
            &lt;ul&gt;
              &lt;li&gt;예를 들어, 기상학자가 미래의 기상 예측을 할 때, 환경의 상태변이확률을 알 수가 없음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;상태변이확률을 계산하는 것도 어렵고 에러 발생이 큰 지루한 작업임.
            &lt;ul&gt;
              &lt;li&gt;12개의 주사위를 던지는 문제일 때, DP의 계산은 코딩 혹은 계산 정확성에서 지루하고 오류가 발생하기 쉬운 작업임.
                &lt;ul&gt;
                  &lt;li&gt;합계 12가 나올 확률, … , 합계 72가 나올 확률을 계산하는 것&lt;/li&gt;
                  &lt;li&gt;몬테카를로 방식에서는 많은 무작위 샘플을 이용해 평균을 구하고, 값을 추정함.&lt;/li&gt;
                &lt;/ul&gt;

                &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_12_dices.png&quot; alt=&quot;monte_carlo_12_dices&quot; /&gt;&lt;/p&gt;

                &lt;ul&gt;
                  &lt;li&gt;12개의 주사위의 기대 합 42와 근접한 수치의 추측&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;몬테카를로 방식&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;몬테카를로 방식을 이용한 가치함수 추정&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_for_policy_evaluation.png&quot; alt=&quot;monte_carlo_for_policy_evaluation&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;동일한 상태에서 여러 리턴값을 관찰&lt;/li&gt;
          &lt;li&gt;이 다수의 리턴값의 평균을 구해 해당 상태의 추정 리턴값을 구함.&lt;/li&gt;
          &lt;li&gt;샘플의 수가 많아질수록, 실제 값에 가까워짐.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;이러한 리턴 값은 에피소드가 끝나야 알 수 있음. (Episodic Tasks 라 가정)&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;몬테카를로 방식과 bandit 방식&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_and_bandits.png&quot; alt=&quot;monte_carlo_and_bandits&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;몬테카를로와 bandit 방식은 유사한데, badnit 에서도 arm 을 여러번 당겨 평균값을 구해 추정값을 구했었음.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;몬테카를로의 차이점은 arm 이 아닌 정책을 고려한다는 점이다.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;몬테카를로 방식의 리턴 값 계산&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_computing_returns_efficiently.png&quot; alt=&quot;monte_carlo_computing_returns_efficiently&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;효율적인 계산을 위해 에피소드 종료시점 부터 거꾸로 계산해야 함.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_prediction_psuedo_code.png&quot; alt=&quot;monte_carlo_prediction_psuedo_code&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;평균을 구할 때, 이전의 값들을 모두 저장하는 것이 아닌 증분 업데이트의 사용이 가능하다.
            &lt;ul&gt;
              &lt;li&gt;$NewEstimate \gets OldEstimate + StepSize [Target - OldEstimate]$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using Monte Carlo for Prediction
    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 예측을 통해 주어진 정책의 가치 함수를 예측하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;monte-carlo-for-control&quot;&gt;Monte Carlo for Control&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using Monte Carlo for Action Values&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;학습 목표
    &lt;ul&gt;
      &lt;li&gt;몬테카를로 방식을 통해 행동 가치함수 (action value function) 추정하기&lt;/li&gt;
      &lt;li&gt;몬테카를로 알고리즘에서 탐색의 유지의 중요성을 이해하기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning Action Values
    &lt;ul&gt;
      &lt;li&gt;상태의 값을 추정하는 방식과 동일 (특정 상태에서 행동을 선택했을 때의 리턴값의 평균을 구함)\&lt;/li&gt;
      &lt;li&gt;행동 가치를 학습하는 이유
        &lt;ul&gt;
          &lt;li&gt;하나의 상태에서 각 행동이 가지는 가치비교를 할 수 있음.&lt;/li&gt;
          &lt;li&gt;더 나은 행동으로의 정책 변경이 가능
            &lt;ul&gt;
              &lt;li&gt;이는 다른 행동을 하여 가치를 추정했을 때 가능함.&lt;/li&gt;
              &lt;li&gt;이 부분이 어려운 부분인데, 결정론적인 정책을 따를 경우 다른 행동을 탐색하지 않는다. (정책을 따름)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;탐색의 유지 문제
        &lt;ul&gt;
          &lt;li&gt;에이전트는 값들을 학습을 위해 각 상태의 모든 행동을 시도해봐야 한다.&lt;/li&gt;
          &lt;li&gt;exploring starts (탐색 시작)
            &lt;ul&gt;
              &lt;li&gt;모든 상태-행동 쌍에서 첫 시작을 해보는 것을 보증해야한다.&lt;/li&gt;
              &lt;li&gt;그 뒤로 에이전트는 정책을 따라 움직인다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Monte Carlo methods for generalized policy iteration&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;몬테카를로 방식을 사용하여 GPI (Generalized Policy Iteration) 알고리즘 구현하는 방법 이해하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Monte Carlo Generalized Policy Iteration&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_generalized_policy_iteration.png&quot; alt=&quot;monte_carlo_generalized_policy_iteration&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;정책 평가를 Monte Carlo Prediction 으로 진행 (샘플링된 에피소드의 평균을 통한 행동가치함수의 추정)
            &lt;ul&gt;
              &lt;li&gt;탐색 유지를 위한 여러 방법 중 하나를 사용 (여기에서는 탐색시작 방법을 사용한다.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;정책 개선을 $q_{\pi_k}$ 의 $\arg\max$ 함수를 이용해 탐욕적으로 개선&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_es_psuedo_code.png&quot; alt=&quot;monte_carlo_es_psuedo_code&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Solving the Blackjack Example&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;MDP 해결을 위한 탐색 시작과 함께 몬테 카를로의 적용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;몬테카를로 방식의 블랙잭 적용 예시
        &lt;ul&gt;
          &lt;li&gt;문제의 정의
            &lt;ul&gt;
              &lt;li&gt;각 블랙잭 게임을 하나의 에피소드로 보고, 할인되지않은 MDP 를 적용한다.&lt;/li&gt;
              &lt;li&gt;보상 : 패배시 -1, 비길 시 0, 이길 시 1&lt;/li&gt;
              &lt;li&gt;행동 : 힛, 스틱&lt;/li&gt;
              &lt;li&gt;상태 : 에이스가 있는지 여부, 플레이어 카드의 총합, 딜러의 카드 1장
                &lt;ul&gt;
                  &lt;li&gt;위의 경우 총 200개의 상태가 존재한다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;덱의 모든 카드를 교체나 버리는 것 없이 사용한다고 가정한다. (카드카운팅 불가 : 현 상태로 마르코브 속성을 가짐.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;몬테카를로 적용
            &lt;ul&gt;
              &lt;li&gt;에이전트는 하나의 에피소드가 끝나고 학습이 가능&lt;/li&gt;
              &lt;li&gt;Discount factor 가 1이기 때문에, 승리했을 경우 해당 에피소드의 각 상태에서의 보상값은 1이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;몬테카를로의 장점
            &lt;ul&gt;
              &lt;li&gt;환경의 거대한 모델을 저장할 필요가 없음&lt;/li&gt;
              &lt;li&gt;특정 상태에 대한 값을 개별적으로 측정할 수 있다. (타 상태의 값과 관계없이)&lt;/li&gt;
              &lt;li&gt;값의 업데이트 계산에 MDP의 상태집합의 크기가 영향을 주지 않는다. (에파소드의 길이에 영향을 받음)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;블랙잭의 조건
        &lt;ul&gt;
          &lt;li&gt;Exploring Starts 를 사용하기 좋은 조건 (에피소드 시작을 무작위 상태에서 무작위 행동을 하는 것으로 시작한다.)
            &lt;ul&gt;
              &lt;li&gt;현재의 블랙잭 게임 조건은 자연스럽게 랜덤한 상태에서 시작하게 된다. (플레이어 손패 카드 2장 (에이스 유무와 합계), 딜러 패 2장중 1장 오픈)&lt;/li&gt;
              &lt;li&gt;즉, 첫 행동을 무작위로 선택하면 되는데, 정책을 따르는 것은 그 이후에 하게 됨. (합이 20이 넘으면 스틱)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;exploration-methods-for-monte-carlo&quot;&gt;Exploration Methods for Monte Carlo&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Epsilon-soft Policy&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;왜 탐색 시작이 실 문제에서 적용이 어려운지 이해하기&lt;/li&gt;
          &lt;li&gt;몬테카를로 제어를 위한 대안적 탐색유지 방법의 설명&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;탐색 시작을 사용할 수 없는 경우
        &lt;ul&gt;
          &lt;li&gt;탐색 시작 알고리즘은 모든 가능한 상태와 행동 쌍에서 시작할 수 있어야 한다.
            &lt;ul&gt;
              &lt;li&gt;그렇지 않은 경우 충분한 탐색을 하지 못하므로, 최적 정책이 아닌 차선의 다른 정책에 수렴하게 될 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;대부분의 문제는 시작 상태-행동 쌍으로 무작위 샘플을 구할 수 없다.
            &lt;ul&gt;
              &lt;li&gt;예를 들어 자율주행 문제의 경우 초기 상태-행동 쌍을 무작위 샘플링할 수 없다. (닥칠 수 있는 모든 상황-행동으로 초기화 시작)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;e-Greedy 탐색&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/e_greedy_exploration.png&quot; alt=&quot;e_greedy_exploration&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\varepsilon-Greedy$ 정책
            &lt;ul&gt;
              &lt;li&gt;Bandits 에서 사용했던 $\varepsilon-Greedy$ 을 몬테카를로에 적용&lt;/li&gt;
              &lt;li&gt;$\varepsilon-Greedy$ 정책은 확률론적인 정책임
                &lt;ul&gt;
                  &lt;li&gt;보통 탐욕적인 행동을 취하나 때로는 무작위 행동을 선택함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$\varepsilon-Soft$ 정책
            &lt;ul&gt;
              &lt;li&gt;$\varepsilon-Greedy$ 정책 을 포괄하는 정책&lt;/li&gt;
              &lt;li&gt;각 행동에 $\frac{\epsilon}{ | \mathcal{A} |}$ (nonzero) 이상의 확률을 부여
                &lt;ul&gt;
                  &lt;li&gt;필연적으로 모든 행동을 시도하게&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;$\varepsilon-Soft$ 정책은 에이전트로 하여금 계속 탐색하도록 강제한다.
                &lt;ul&gt;
                  &lt;li&gt;즉, 탐색 시작의 요구사항을 제거 (탐색 시작을 대체) 할 수 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\varepsilon-Soft$ 정책의 한계
  &lt;img src=&quot;/assets/images/posts/e_soft_policies_not_be_optimal.png&quot; alt=&quot;e_soft_policies_not_be_optimal&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;결정론적 최적 정책에 수렴할 수 없다.
            &lt;ul&gt;
              &lt;li&gt;탐색 시작의 방식은 최적 정책에 도달함과 다르게, 최적 $\varepsilon-Soft$ 정책 에만 도달할 수 있음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/e_soft_policy_psuedo_code.png&quot; alt=&quot;e_soft_policy_psuedo_code&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위 코드는 몬테카를로 $\varepsilon-Soft$ 정책에 대한 내용으로, 탐색시작과 차이가 생기는 부분을 표시한 것이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;off-policy-learning-for-prediction&quot;&gt;Off-policy Learning for Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Why does off-policy learning matter?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;off-policy learning 이 탐색 문제를 해결하는 데 어떻게 도움이 되는지 이해하기&lt;/li&gt;
          &lt;li&gt;목표 정책의 예시 및 행동 정책의 예시를 생성&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\varepsilon - soft$ 정책의 한계점&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/epsilon_soft_policy_limitation.png&quot; alt=&quot;epsilon_soft_policy_limitation&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;탐색에도, 행동에도 최적 정책이 아니다.&lt;/li&gt;
          &lt;li&gt;학습을 통해 차선의 최적 정책으로 수렴함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;On-Policy 와 Off-Policy
        &lt;ul&gt;
          &lt;li&gt;On-Policy Learning: 에이전트가 학습할 데이터의 생성에 기여한 정책을 발전시킴.&lt;/li&gt;
          &lt;li&gt;Off-Policy Learning: 에이전트가 다른 정책으로부터 학습할 데이터를 생성하여 이와 별개의 정책을 학습
            &lt;ul&gt;
              &lt;li&gt;예를들어 uniform random 정책으로 생성한 데이터로 최적 정책을 학습함.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/off_policy_target_policy.png&quot; alt=&quot;off_policy_target_policy&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;대상정책(target policy) : 에이전트가 개선해나아가며 최종적으로 획득하길 원하는 정책
    + 에이전트가 학습할 가치 함수는 대상정책(target policy) 를 기반으로 함.&lt;/p&gt;

                &lt;p&gt;&lt;img src=&quot;/assets/images/posts/off_policy_behavior_policy.png&quot; alt=&quot;off_policy_behavior_policy&quot; /&gt;&lt;/p&gt;

                &lt;ul&gt;
                  &lt;li&gt;행동정책(behavior policy) : 에이전트가 행동을 선택하는 기준이 되는 정책&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Off-Policy 를 사용하는 이유
        &lt;ul&gt;
          &lt;li&gt;지속 탐색의 또다른 전략을 제공하기 때문
            &lt;ul&gt;
              &lt;li&gt;대상정책을 따라 학습하게 되면, 전체 상태가 아닌 소수의 상태만을 탐색하게 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Off-policy 에 대한 몇가지 유효한 어플리케이션이 존재함 (참고용 자료)
            &lt;ul&gt;
              &lt;li&gt;learning from demonstartion
                &lt;ul&gt;
                  &lt;li&gt;따라하기 학습&lt;/li&gt;
                  &lt;li&gt;전문가의 행동을 따라하거나 전문가가 제공한 상태-행동 쌍 데이터를 이용하여 학습을 진행&lt;/li&gt;
                  &lt;li&gt;초기에 정확한 행동을 배우기 위해 유용함&lt;/li&gt;
                  &lt;li&gt;전문ㅏ의 경험을 통해 학습 과정을 가속화, 좋은 품질의 행동을 습득&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;parallel learning
                &lt;ul&gt;
                  &lt;li&gt;여러 에이전트가 동시에 병렬로 학습을 수행하는 방법 (여러 에이전트가 동시에 다른 환경 또는 다른 부분 문제를 처리)&lt;/li&gt;
                  &lt;li&gt;학습 속도를 높이고 효율성을 향상시키는 데 도움이 됨&lt;/li&gt;
                  &lt;li&gt;다양한 경험을 공유하고 전체 학습 성능을 향상&lt;/li&gt;
                  &lt;li&gt;분산 시스템에서 사용되는 경우가 많으며, 학습 속도와 성능 향상을 위해 활용될 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Off-Policy 의 중요 요건
        &lt;ul&gt;
          &lt;li&gt;행동정책 (behavior policy) 이 대상정책 (target policy) 을 포괄해야 함.
            &lt;ul&gt;
              &lt;li&gt;즉, $\pi (a | s) &amp;gt; 0$ 이면 $b(a | s ) &amp;gt; 0$ 이어야 한다.
                &lt;ul&gt;
                  &lt;li&gt;수학적인 이유는 다음 섹션에서 살펴봄&lt;/li&gt;
                  &lt;li&gt;직관적인 이유는 아래의 그림과 같을 경우, 우측으로 가는 행동에 대한 행동 가치를 알 수가 없기 때문&lt;/li&gt;
                &lt;/ul&gt;

                &lt;p&gt;&lt;img src=&quot;/assets/images/posts/off_policy_behavior_policy_must_contain_target_policy.png&quot; alt=&quot;off_policy_behavior_policy_must_contain_target_policy&quot; /&gt;&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Importance Sampling&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;중요도 샘플링을 이용하여 다른 분포의 샘플을 통한 대상 분포도에 대한 예상 값을 추정한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Importance Sampling 유도
        &lt;ul&gt;
          &lt;li&gt;개요
            &lt;ul&gt;
              &lt;li&gt;Sample : $x \sim b$
                &lt;ul&gt;
                  &lt;li&gt;~ : 분포에 의해 생성되는 것을 의미함.&lt;/li&gt;
                  &lt;li&gt;x : 생성된 데이터&lt;/li&gt;
                  &lt;li&gt;b : 정책&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Estimate : $\mathbb{E}_\pi [X]$
                &lt;ul&gt;
                  &lt;li&gt;target 정책에 의해 추정되어지는 예측값&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;$b$ 에서 유도된 데이터 $x$ 이므로, 이 값을 간단히 평균내어 $\pi$ 에 대한 예측치를 구할 수 없다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;기대값에 대한 정의&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/importance_sampling_derivation.png&quot; alt=&quot;importance_sampling_derivation&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;$E_\pi [X] \doteq \sum_{x \in X} x \pi (x)$
                &lt;ul&gt;
                  &lt;li&gt;모든 가능한 출력값 $x$ 에 대해 $\pi$ 에 따른 확률값을 곱해 합계를 구함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;$=\sum_{x \in X} x \pi (x) \frac{b(x)}{b(x)}$
                &lt;ul&gt;
                  &lt;li&gt;뒤의 $\frac{b(x)}{b(x)}$ 는 값이 1이므로 수식에 곱할 수 있다.&lt;/li&gt;
                  &lt;li&gt;$b(x)$ : b 정책 하의 관측된 결과값 $x$ 에 대한 확률값&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;$=\sum_{x \in X} x \frac{\pi (x)}{b(x)} b(x)$
                &lt;ul&gt;
                  &lt;li&gt;여기에서 $\frac{\pi (x)}{b(x)}$ 를 Importance Sampling Ratio 라고 한다.&lt;/li&gt;
                  &lt;li&gt;우리는 보통 Importance Sampling Ratio 를 $\rho (x)$ 로 표기한다.&lt;/li&gt;
                  &lt;li&gt;즉, $\sum_{x \in X} x \rho (x) b(x)$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;기대값을 $b$ 정책에 대한 식으로 치환&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/importance_sampling_derivation_to_b_1.png&quot; alt=&quot;importance_sampling_derivation_to_b_1&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 공식에서 유도된 $E_\pi [X] = \sum_{x \in X} x \rho (x) b(x)$&lt;/li&gt;
              &lt;li&gt;$= E_b [X \rho (X)]$ : $\pi$ 에 대한 식을 $b$ 에 대한 식으로 치환함.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/importance_sampling_derivation_to_b_2.png&quot; alt=&quot;importance_sampling_derivation_to_b_2&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;$\mathbb{E} [X] \approx \frac{1}{n} \sum_{i=1}^n x_i$
                &lt;ul&gt;
                  &lt;li&gt;$\approx$ : 거의 동일&lt;/li&gt;
                  &lt;li&gt;위 식은 몬테카를로에서 기대값을 구하는 방식으로 평균을 취한 값임.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;$\approx \frac{1}{n} \sum_{i=1}^n x_i \rho (x_i)$
                &lt;ul&gt;
                  &lt;li&gt;즉, 위 식을 풀기 위해 weighted sample average 를 구하면 되는데, 이는 importance sampling ratio 를 weightings 로 사용한 수식이다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/importance_sampling_derivation_to_b_3.png&quot; alt=&quot;importance_sampling_derivation_to_b_3&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;우리는 식에서 $\pi$ 가 아닌 $b$ 의 분포를 이용하여 계산하고, 실제로 이 값은 sample average 를 이용해 $\pi$ 의 분포 아래 값을 구하는 식이 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;샘플링을 이용한 예측치를 구하는 예시&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/importance_sampling_example.png&quot; alt=&quot;importance_sampling_example&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위의 예시는 $b$ 의 확률분포 내에서 몬테카를로 방식으로 진행했을 때 보상값이 1, 3, 1 이 나왔을 시 importance sampling ratio 를 이용해 $\pi$ 의 분포도로 치환하여 계산한 식을 의미하며 $\pi$ 의 실제 예측값인 2.2 에 가까운 근사값이 나오는 것을 볼 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Off-policy Monte Carlo Prediction
    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;중요도 샘플링을 이용하여 리턴값을 수정하는 방법을 이해하기&lt;/li&gt;
          &lt;li&gt;몬테카를로 예측 알고리즘을 변형하여 off-policy learning 에 적용하는 방법 이해하기.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;몬테카를로 방식에 대한 복기&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/monte_carlo_recall.png&quot; alt=&quot;monte_carlo_recall&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;상태 $s$ 에서 수집된 리턴값의 평균을 구하는 방식&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Off-Policy Monte Carlo&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/off_policy_montecarlo.png&quot; alt=&quot;off_policy_montecarlo&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;정책 $b$ 로부터 수집된 리턴값에 importance sampling ratio weight ($\rho$) 값을 곱한 값으로 평균을 구함.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;$\rho = \frac{\mathbb{P} ( \texttt{trajectory under } \pi )}{\mathbb{P} ( \texttt{trajectory under } b )}$&lt;/li&gt;
          &lt;li&gt;여기에서 $\rho$ 는 전체 궤적의 분포도를 수정하여 리턴값의 분포를 수정한다.&lt;/li&gt;
          &lt;li&gt;위 수정 방식을 이용해 $\pi$ 의 기대 리턴값을 구한다.&lt;/li&gt;
          &lt;li&gt;$V_\pi (s) = \mathbb{E}_b [ \rho G_t | S_t = s ]$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/off_policy_trajectories.png&quot; alt=&quot;off_policy_trajectories&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;궤적의 확률 값은 시점 $t$ 에서 종료상태 $T$ 까지의 정책 상 행동 $a$ 의 선택 확률과 상태 $s$ 로의 전이확률 (환경역학) 값의 연속이다.
            &lt;ul&gt;
              &lt;li&gt;$b(A_t | S_t) p (S_{t+1} | S_t, A_t) b(A_{t+1} | S_{t+1}) p (S_{t+2} | S_{t+1}, A_{t+1}) \cdots p (S_T | S_{T-1}, A_{T-1} )$&lt;/li&gt;
              &lt;li&gt;위의 식을 $\Pi_{k=t}^{T-1} b(A_k | S_k) p (S_{k+1} | S_k, A_k)$ 로 표기할 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위의 식에서 환경역학 $p$ 는 동일한 값이기에 상쇄 (약분) 된다.&lt;/li&gt;
          &lt;li&gt;위의 식에서 남은 앞 부분은 importance sampling ratio 인 $\rho$ 이다.
            &lt;ul&gt;
              &lt;li&gt;$\rho_{t:T-1} \doteq \Pi_{k=t}^{T-1} \frac {\pi (A_k | S_k)} {b (A_k | S_k)}$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Off-Policy Value : $E_b [ \rho_{t:T-1} G_t | S_t = s] = v_\pi (s)$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Off-Policy Monte Carlo 의 구현&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Off-policy every-visit MC&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/off_policy_every_visit_MC_psuedo_code.png&quot; alt=&quot;off_policy_every_visit_MC_psuedo_code&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;에피소드를 생성할 때 $\pi$ 가 아닌 $b$ 를 통해 생성한다.&lt;/li&gt;
              &lt;li&gt;에피소드의 리턴 값이 $G \gets \gamma G + R_{t+1}$ 이 아닌 $G \gets \gamma W G + R_{t+1}$ 로 계산된다.&lt;/li&gt;
              &lt;li&gt;$W \gets W \frac {\pi (A_t | S_t)}{b (A_t | S_t)}$ 로 매 스텝마다 누적곱이 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$\rho_{t:T-1}$ 의 증분계산&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/importance_sampling_calculate.png&quot; alt=&quot;importance_sampling_calculate&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;몬테카를로 알고리즘 루프가 마지막 타임스텝에서부터 거꾸로 계산을 누적하는 방식이므로, $\rho$ 의 계산 또한 동일하다.&lt;/li&gt;
              &lt;li&gt;위의 식과 같이 이전 단계의 값의 누적 곱을 구하는 방식이기에, 이전 단계의 모든 $\rho$ 값을 따로 저장할 필요가 없다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Emma Brunskill - Batch Reinforcement Learning
    &lt;ul&gt;
      &lt;li&gt;배치 강화학습의 개요
        &lt;ul&gt;
          &lt;li&gt;현재 강화학습의 놀라운 성공에 대해 생각해보면 로봇 공학이나 게임플에이와 같은 영역이며, 시뮬레이터에서 액세스 할 수 있는 케이스임&lt;/li&gt;
          &lt;li&gt;에이전트가 오랫동안 학습에 실패하더라도 결국 학습하게 됨.&lt;/li&gt;
          &lt;li&gt;반대로 사람과 상호작용하는 영역의 경우 사람들이 학습하거나 행동하는 방식에 대한 훌륭한 시뮬레이터를 얻을 수 없음
            &lt;ul&gt;
              &lt;li&gt;실제 데이터에 의존해야 함 (실제 인간과 상호작용 하는 것과 관련이 있기에 어려운 일임)&lt;/li&gt;
              &lt;li&gt;에이전트가 학습해야 하는 데이터의 양을 최소화하기 위한 기술과 학습량의 근본적인 한계는 무엇일까?&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;실 생활 데이터의 활용 어려움
        &lt;ul&gt;
          &lt;li&gt;실제 행하지 않은 행동의 결과 추론 문제&lt;/li&gt;
          &lt;li&gt;과거에 행했던 하나의 행동이 아닌 일련의 행동에 대한 조합 가능성&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;실 생활 데이터의 활용 가능성
        &lt;ul&gt;
          &lt;li&gt;특정한 행동순서로 진행하는 것의 결과값을 알고 싶을 때, 데이터가 10만건이라면 이 중 해당 행동순서를 가지는 경우는 100건이 될 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;중요도 샘플링의 문제점
        &lt;ul&gt;
          &lt;li&gt;중요도 샘플링은 편향되지 않은 추정량을 제공하지만 일반적으로 분산이 매우 높을 수 있음
            &lt;ul&gt;
              &lt;li&gt;즉, 데이터가 많지 않거나 에피소드가 길면 열악한 근사값을 구할 확률이 큼&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;중요도 샘플링의 대안 : MDP 의 Parametric Models
        &lt;ul&gt;
          &lt;li&gt;상태 전이확률과 보상 함수를 파라미터화한 모델
            &lt;ul&gt;
              &lt;li&gt;파라미터를 사용하여 상태 전이 확률과 보상함수를 추정하거나 근사하는 방식으로 동작&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;실제 환경의 동작을 정확히 모델링하지 않고도 예측과 학습이 수행 가능, 복잡한 환경에서도 효율적인 학습을 할 수 있음.&lt;/li&gt;
          &lt;li&gt;하지만 실제 환경과 다름 - 편향을 가질 수 있음&lt;/li&gt;
          &lt;li&gt;그러나 약간의 편향에 대한 대가로 낮은 분산을 가질 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Doubly Robust Estimator 방법
        &lt;ul&gt;
          &lt;li&gt;이 방법은 오프-폴리시(Off-Policy) 학습에서 사용되며, 동일한 데이터를 사용하여 가치 함수를 추정하는 데에 효과적&lt;/li&gt;
          &lt;li&gt;Doubly Robust Estimators의 핵심 개념은 두 가지 보정 요소를 조합하는 것
            &lt;ul&gt;
              &lt;li&gt;가치 함수 추정에 사용되는 모델 또는 추정기(estimator) : Parametric Models&lt;/li&gt;
              &lt;li&gt;가치 함수 추정에서 발생하는 편향을 보정하는 가중치 : Importance Sampling&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;첫 번째 방법은 모델 또는 추정기를 사용하여 행동 가치 함수 또는 상태 가치 함수를 추정하는 것. 이 추정은 일부 편향을 가질 수 있지만, 적은 데이터에서도 추정이 가능.&lt;/li&gt;
          &lt;li&gt;두 번째 방법은 경험 데이터를 사용하여 행동 가치 함수 또는 상태 가치 함수를 보정하는 것. 이 보정은 가중치를 사용하여 편향을 보정하는 것으로, 정책 평가의 불일치 문제를 해결하는 데 도움이 됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;몬테카를로 방식의 장점
    &lt;ul&gt;
      &lt;li&gt;본 장의 몬테카를로 방식 : 샘플 에피소드의 형태에서의 경험으로 가치함수와 최적의 정책을 학습&lt;/li&gt;
      &lt;li&gt;환경 역학의 모델 없이, 환경과의 상호작용에서 직접 최적의 정책을 학습함.&lt;/li&gt;
      &lt;li&gt;시뮬레이션 또는 샘플 모델과 함께 사용할 수 있음.
        &lt;ul&gt;
          &lt;li&gt;많은 응용프로그램의 경우 DP 방식에서 요구하는 전환 확률의 명시적 모델을 구성하는 것은 어렵지만, 샘플 에피소드를 시뮬레이션 하는 것은 쉬움.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;몬테카를로 방식을 상태 중 작은 하위 집합에 집중시킬 수 있다.
        &lt;ul&gt;
          &lt;li&gt;나머지 상태 집합을 정확하게 평가하는 데 비용을 들이지 않고 특별한 관심 영역을 정확히 평가 가능 (8장에서 살펴볼 내용)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Markov 속성 위반으로 인한 피해가 적음 (이후에 다룰 내용)
        &lt;ul&gt;
          &lt;li&gt;이는 후속 상태의 가치 추정치를 기반으로 가치 추정치를 업데이트 하지 않기 때문 (부트스트랩을 하지 않기 때문)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;몬테카를로의 제어 방법 설계
    &lt;ul&gt;
      &lt;li&gt;이 장에서는 일반화된 정책 반복 (Generalized Policy Iteration, GPI) 의 전체 스키마를 따랐음.
        &lt;ul&gt;
          &lt;li&gt;GPI는 정책 평가 및 정책 개선의 상호 작용 프로세스를 포함함.&lt;/li&gt;
          &lt;li&gt;몬테카를로 방식은 정책 평가 프로세스를 대체함
            &lt;ul&gt;
              &lt;li&gt;모델을 사용하여 각 상태의 가치를 계산하는 대신, 상태에서 시작되는 많은 샘플의 리턴값 평균을 구함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;제어 방법에서 행동가치함수를 근사화 하는데 집중함. (환경의 전환 역학 모델을 요구하지 않고 정책을 개선하는데 사용 가능)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;충분한 탐색을 유지하기 위한 몬테카를로 제어 방법의 문제
    &lt;ul&gt;
      &lt;li&gt;현재 최선이라고 추정되는 행동만을 선택하는 것으로는 다른 행동을 선택했을 때의 리턴값을 알 수 없고 실제로 더 나은 조치를 학습할 수 없게 됨.&lt;/li&gt;
      &lt;li&gt;한가지 접근 방식은 에피소드가 모든 가능성을 다루기 위해 무작위로 선택된 상태-행동 쌍으로 시작한다고 가정하여 이 문제를 무시하는 것
        &lt;ul&gt;
          &lt;li&gt;탐색 시작은 시뮬레이션된 에피소드가 있는 응용프로그램에서 사용될 수 있지만, 실제 경험에서 학습할 가능성은 낮음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;On-Policy 방식
        &lt;ul&gt;
          &lt;li&gt;에이전트는 항상 탐색하고, 탐색이 포함된 최상의 정책을 찾으려고 노력함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Off-Policy 방식
        &lt;ul&gt;
          &lt;li&gt;에이전트는 탐색하지만 탐색 정책과 관련이 없을 수 있는 결정론적 최적 정책을 학습함.
            &lt;ul&gt;
              &lt;li&gt;중요도 샘플링의 일부 형태를 기반으로 함.&lt;/li&gt;
              &lt;li&gt;두 정책에서 관찰된 행동을 취할 확률의 비율로 구해, 리턴값에 가중치를 부여하여 행동정책에서 대상정책으로 기대치를 변환함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;일반 중요도 샘플링 : 가중 수익율의 단순 평균을 사용
            &lt;ul&gt;
              &lt;li&gt;편향되지 않은 추정치를 생성하지만 분산이 더 크고 무한할 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;가중 중요도 샘플링 : 가중 평균을 사용
            &lt;ul&gt;
              &lt;li&gt;유한한 분산을 가짐&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;개념적 단순성에도 불구하고 예측 및 제어 모두에 대한 정책 외 몬테카를로 방법은 여전히 불안정하며 지속적인 연구 대상임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;몬테카를로 방식과 DP 방식의 차이점
    &lt;ul&gt;
      &lt;li&gt;샘플 경험을 기반으로 작동하므로 모델 없이 직접 학습에 사용할 수 있음&lt;/li&gt;
      &lt;li&gt;부트스트랩을 수행하지 않음 (다른 가치 추정치를 기반으로 가치 추정치를 업데이트하지 않음)&lt;/li&gt;
      &lt;li&gt;다음 장에서는 몬테카를로 방법과 같이 경험에서 배우는 방법과 DP 방법과 같은 부트스트랩 방법을 고려함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Monte Carlo Methods" /><summary type="html">관련 자료 (RLbook2018 Pages 91-104)</summary></entry><entry><title type="html">Sample-based Learning Methods - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Sample_based_Learning_Methods_00_About-this-course/" rel="alternate" type="text/html" title="Sample-based Learning Methods - 00. 강좌소개" /><published>2023-06-09T14:00:00+09:00</published><updated>2023-06-09T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Sample_based_Learning_Methods_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Sample_based_Learning_Methods_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_sampleBasedLearningMethods.png.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;환경과의 trial and error 상호작용을 통한 최적정책&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 04. Week 4. Dynamic Programming</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_04_Week4/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 04. Week 4. Dynamic Programming" /><published>2023-04-20T15:00:00+09:00</published><updated>2023-04-20T15:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_04_Week4</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_04_Week4/">&lt;h2 id=&quot;관련-자료-rlbook2018-pages-73-88&quot;&gt;관련 자료 (RLbook2018 Pages 73-88)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic Programming
    &lt;ul&gt;
      &lt;li&gt;동적 프로그래밍이란?
        &lt;ul&gt;
          &lt;li&gt;Markov decision process (MDP) 형태의 완벽한 환경 모델이 제공될 때 최적 정책을 계산하기 위한 알고리즘의 집합&lt;/li&gt;
          &lt;li&gt;완벽한 모델이라는 가정과 비싼 컴퓨팅 비용 때문에 전통적 동적 프로그램 (DP) 은 강화학습에서 활용성이 떨어짐&lt;/li&gt;
          &lt;li&gt;하지만 이론적으로 여전히 중요함 (필수적인 기초 지식)
            &lt;ul&gt;
              &lt;li&gt;모든 방식들이 위의 2가지 제약을 벗어나 DP (Dynamic Programming) 와 동일한 효과를 내는 것을 시도하는 것이라 볼 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;주로 사용하는 동적 프로그래밍의 환경 조건
        &lt;ul&gt;
          &lt;li&gt;a finite MDP (유한 MDP)
            &lt;ul&gt;
              &lt;li&gt;state sets $S$, action sets $A$, reward sets $R$ 은 유한함&lt;/li&gt;
              &lt;li&gt;환경의 역학 (dynamics) 은 확률의 집합 $p(s’,r|s,a)$ 로 제공됨 ($s \in S, a \in A(s), r \in R, s’ \in S^+$)
                &lt;ul&gt;
                  &lt;li&gt;$S^+$ 는 $S$ 에 terminal state 를 포함한 것 (episodic task 일 경우)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;비록 DP 가 continuous state, action spaces 에서도 적용할 수 있다 해도 그것은 특이 케이스인 경우 뿐임.
            &lt;ul&gt;
              &lt;li&gt;continuous state 에서 DP를 적용하는 통상의 방법은 state 와 action을 quantize (근사) 하여 유한 상태의 DP 방식을 적용하는 것이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;DP, 그리고 일반적인 강화학습의 핵심 아이디어는 가치 함수를 사용하여 좋은 정책을 찾기 위한 구조화를 하는 것이다.
        &lt;ul&gt;
          &lt;li&gt;우리는 최적 정책을 벨만 최적 방정식을 만족하는 최적 가치함수를 구함으로서 쉽게 찾을 수 있다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_0_1_bellman_optimality_equations.png&quot; alt=&quot;4_0_1_bellman_optimality_equations&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;for all $s \in S$, $a \in A(s)$, $s’ \in S^+$&lt;/li&gt;
          &lt;li&gt;DP 알고리즘은 벨만 방정식을 원하는 가치함수 (최적가치함수) 의 근사값을 개선하는 업데이트 규칙으로 전환한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Policy Evaluation (Prediction)
    &lt;ul&gt;
      &lt;li&gt;첫번째로 임의의 정책 $\pi$ 의 상태 가치 함수 $v_{\pi}$ 를 계산하는 방법을 고려한다.
        &lt;ul&gt;
          &lt;li&gt;이것을 DP (Dynamic Programming) 용어로 정책 평가 (Policy evaluation) 이라 한다.
  &lt;img src=&quot;/assets/images/posts/4_1_1_state_value_bellman_equation.png&quot; alt=&quot;4_1_1_state_value_bellman_equation&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;$v_\pi$ 의 고유성은 $\gamma &amp;lt; 1$ 혹은 정착 $\pi$ 아래 모든 상태에서 필연적인 종료 (termination) 가 보장될 경우 보장된다.&lt;/li&gt;
          &lt;li&gt;만일 환경의 역학을 완전히 알 경우 (4.4) 는 $|S|$ 개의 미지수를 가진 선형 방정식으로 풀릴 수 있다.
            &lt;ul&gt;
              &lt;li&gt;이는 직관적이지만 지루한 연산 과정이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;우리의 목적을 달성하기 위해 반복 솔루션 방법이 가장 적합하다.
        &lt;ul&gt;
          &lt;li&gt;$S^+$ 와 $R$ (실제 숫자 값) 을 매핑하는 근사 가치함수 $v_0, v_1, v_2, …$ 를 가정한다.&lt;/li&gt;
          &lt;li&gt;최초의 근사, $v_0$ 는 임의로 선택 (단, 최종 상태가 있는 경우 0을 지정)&lt;/li&gt;
          &lt;li&gt;각 연속적인 근사치는 벨만 방정식의 업데이트 규칙을 사용하여 얻음.
  &lt;img src=&quot;/assets/images/posts/4_1_2_bellman_equation_update_rule.png&quot; alt=&quot;4_1_2_bellman_equation_update_rule&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;$v_\pi$ 에 대한 벨만 방정식이 등치를 보장하기 때문에, 모든 $s \in S$ 에 대해 $v_k = v_\pi$ 는 위 업데이트 규칙의 고정점이다.&lt;/li&gt;
          &lt;li&gt;사실 시퀀스 $\lbrace v_k \rbrace$ 는 일반적으로 $k \to \infty$ 에 따라 $v_\pi$ 로 수렴한다.&lt;/li&gt;
          &lt;li&gt;위 알고리즘을 반복 정책 평가 (iterative policy evaluation) 라 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;iterative policy evaluation
        &lt;ul&gt;
          &lt;li&gt;expected update
            &lt;ul&gt;
              &lt;li&gt;$v_k$ 로부터 $v_{k+1}$ 연속 근사를 생성하기 위해, iterative policy evaluation 은 각 $s$ 상태에 같은 연산을 적용한다.&lt;/li&gt;
              &lt;li&gt;평가 중인 정책의 가능한 모든 한 단계의 전환에 대하여…&lt;/li&gt;
              &lt;li&gt;이전 값 $s$ 를 새로운 값으로 교체하기 위해 $s$ 의 후속상태의 이전 값과 즉각적인 보상을 이용&lt;/li&gt;
              &lt;li&gt;우리는 이러한 연산을 expected update 라 한다.&lt;/li&gt;
              &lt;li&gt;위의 각 반복은 모든 상태의 값을 한번 업데이트 하여 $v_k$ 에서 $v_{k+1}$ 을 생성한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;several different kinds of expected updates
            &lt;ul&gt;
              &lt;li&gt;state 를 업데이트 하거나, state-action pair 를 업데이트 할 수 있음&lt;/li&gt;
              &lt;li&gt;후속 상태의 추정 값이 결합되는 방법에 따라 달라짐&lt;/li&gt;
              &lt;li&gt;DP 에서 사용하는 모든 업데이트 방식을 expected update 라 한다.
                &lt;ul&gt;
                  &lt;li&gt;모든 가능한 다음 상태의 추정값에 기반하기 때문 (다음 상태를 샘플링 하는 것이 아닌)&lt;/li&gt;
                  &lt;li&gt;방정식이나 백업 다이어그램 등으로 표현이 가능함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;순차 컴퓨터 프로그램으로 iterative policy evaluation 을 구현하는 방법
            &lt;ul&gt;
              &lt;li&gt;two-array version
                &lt;ul&gt;
                  &lt;li&gt;두 개의 array 를 사용 (old values $v_k (s)$, new values $v_{k+1} (s)$)&lt;/li&gt;
                  &lt;li&gt;두 개의 array 를 사용함으로써, 이전 값의 변경 없이 새로운 값을 계산할 수 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;in-place algorithm
                &lt;ul&gt;
                  &lt;li&gt;하나의 array 를 사용하여 제자리에서 값을 업데이트 한다.&lt;/li&gt;
                  &lt;li&gt;새로운 값이 즉각적으로 이전 값을 덮어 쓴다.&lt;/li&gt;
                  &lt;li&gt;각 상태의 업데이트 순서에 따라, 이전 값이 아닌 새로운 값을 업데이트에 쓰기도 한다.&lt;/li&gt;
                  &lt;li&gt;이 알고리즘 또한 $v_\pi$ 로 수렴한다.
                    &lt;ul&gt;
                      &lt;li&gt;two-array version 보다 더 빠르게 수렴한다.&lt;/li&gt;
                      &lt;li&gt;사용 가능한 새로운 데이터를 즉각 사용하기 때문&lt;/li&gt;
                      &lt;li&gt;업데이트 되는 상태의 순서가 수렴율에 큰 영향을 끼친다.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;DP 알고리즘에서의 알고리즘은 주로 in-place version 이라 생각하면 된다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;in-place version of iterative policy evaluation (pseudocode)
  &lt;img src=&quot;/assets/images/posts/4_1_3_iterative_policy_evaluation.png&quot; alt=&quot;4_1_3_iterative_policy_evaluation&quot; /&gt;
            &lt;ul&gt;
              &lt;li&gt;종료 상태를 관리함 (수렴은 극한 값에서 이루어지지만, 이보다 짧아야 함.)&lt;/li&gt;
              &lt;li&gt;의사코드는 $\max_{s \in S} | v_{k+1} (s) - v_k (s) |$ 의 값을 매 sweep 마다 체크하고, 값이 충분히 작아지면 중지한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;예제 4.1 : 4 x 4 gridworld
  &lt;img src=&quot;/assets/images/posts/4_1_4_example_4_1_gridworld.png&quot; alt=&quot;4_1_4_example_4_1_gridworld&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;종료상태 미포함 상태 $S= { 1,2,…,14 }$&lt;/li&gt;
          &lt;li&gt;각 상태에서 가능한 행동 $A = { up, down, right, left }$&lt;/li&gt;
          &lt;li&gt;행동에 따라 결정론적으로 상태의 전이가 일어남 (그리드를 벗어나는 행동에 대해서는 상태가 변하지 않음)
            &lt;ul&gt;
              &lt;li&gt;$p(6,-1 | 5,right) = 1$&lt;/li&gt;
              &lt;li&gt;$p(7,-1 | 7,right) = 1$&lt;/li&gt;
              &lt;li&gt;$p(10,r | 5,right) = 0$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;이것은 할인이 없는, episodic task 이다.&lt;/li&gt;
          &lt;li&gt;모든 전이에 대한 보상은 -1 이며, terminal state 에 닿을 때까지 지속된다.&lt;/li&gt;
          &lt;li&gt;terminal state 는 해당 그림에서 음영으로 표시된 상태이다.&lt;/li&gt;
          &lt;li&gt;모든 상태 $s, s’$ 와 행동 $a$ 에 대한 기대 보상 함수는 $r(s,a,s’) = -1$ 이다.&lt;/li&gt;
          &lt;li&gt;에이전트가 equprobable random policy (모든 action 이 동등 확률) 을 따른다고 가정한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy Improvement&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;예제 4.1 gridworld 의 iterative policy evaluation 의 수렴
  &lt;img src=&quot;/assets/images/posts/4_2_1_example_4_1_gridworld_convergence.png&quot; alt=&quot;4_2_1_example_4_1_gridworld_convergence&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;정책의 변경
        &lt;ul&gt;
          &lt;li&gt;임의의 결정론적 정책 $\pi$ 에 대한 가치함수 $v_\pi$ 를 결정했다고 가정할 경우,&lt;/li&gt;
          &lt;li&gt;일부 상태의 경우 결정론적으로 행동 $a \neq \pi (s)$ 인 a 를 선택하도록 정책을 변경해야 하는지 여부를 알고 싶음&lt;/li&gt;
          &lt;li&gt;우리는 $s$ 상태에서 현 정책을 따르는 것이 얼마나 좋은지 ($v_\pi (s)$) 를 알고 있지만, 새 정책으로 변경하는 것이 더 나은지 알고 싶음&lt;/li&gt;
          &lt;li&gt;위 질문에 대한 하나의 해법은 $s$ 상태에서 $a$ 를 선택하고, 그 뒤 기존 정책 $\pi$ 를 따라보는 것이다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_2_2_q_pi_s_a.png&quot; alt=&quot;4_2_2_q_pi_s_a&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;그 값은 위와 같다.&lt;/li&gt;
          &lt;li&gt;위의 값이 $v_\pi (s)$ 보다 큰지 작은지가 중요하다.
            &lt;ul&gt;
              &lt;li&gt;클 경우, $s$ 에서 $a$ 를 선택한 뒤 $\pi$ 정책을 따르는 것이 $\pi$ 를 계속 따르는 것보다 낫다는 뜻임.&lt;/li&gt;
              &lt;li&gt;즉, $s$ 에서 $a$ 를 선택하는 정책이 $\pi$ 정책보다 더 나은 정책임.&lt;/li&gt;
              &lt;li&gt;이것을 정책 개선 정리 (policy improvement theorem) 라 한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$\pi$ 와 $\pi’$ 를 결정론적 정책이라 가정, $\textrm{all s} \in S$ 인 경우&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_2_3_policy_improvement_q_pi.png&quot; alt=&quot;4_2_3_policy_improvement_q_pi&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;정책 $\pi’$ 는 $\pi$ 와 같거나 보다 좋은 정책이다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_2_4_policy_improvement_v_pi.png&quot; alt=&quot;4_2_4_policy_improvement_v_pi&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;모든 상태에서 위 4.7이 성립할 경우 4.8 또한 성립한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_2_5_policy_improvement_reapplying.png&quot; alt=&quot;4_2_5_policy_improvement_reapplying&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;위와 같이 하나의 상태에서 특정 행동을 변경하는 정책을 어떻게 평가하는지 알아봤음.&lt;/li&gt;
          &lt;li&gt;이 방식을 모든 상태에 모든 선택가능한 행동에 적용하는 것으로 확장하는 것은 자연스러운 방법이다.&lt;/li&gt;
          &lt;li&gt;즉, 새로운 탐욕 정책 $\pi’$ 를 고려해본다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Policy Improvement&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_2_6_new_greedy_policy_pi_quote.png&quot; alt=&quot;4_2_6_new_greedy_policy_pi_quote&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;$\arg\max_a$ 는 뒤의 식이 최대값이 되는 $a$ 를 뜻한다. (동률일 경우 임의로 선택)&lt;/li&gt;
          &lt;li&gt;greedy policy 는 $v_\pi$ 하의 단기적으로 가장 좋아보이는 행동을 선택한다. (한 번의 스텝 진행만을 고려)&lt;/li&gt;
          &lt;li&gt;greedy policy 는 policy improvement theorm (4.7) 의 조건을 만족하므로, 기존 정책과 같거나 더 좋은 정책이라는 것을 알 수 있다.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;기존 정책의 가치 함수를 사용하여 기존의 정책보다 더 나은 정책을 만드는 것을 policy improvement 라 한다.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;new greedy policy $\pi’$ 가 old policy $\pi$ 만큼 좋지만, 더 좋지는 않은 상태를 가정한다.
            &lt;ul&gt;
              &lt;li&gt;for $\textrm{all s} \in S$&lt;/li&gt;
              &lt;li&gt;$v_\pi = v_{\pi’}$&lt;/li&gt;
              &lt;li&gt;4.9 의 식을 따름
  &lt;img src=&quot;/assets/images/posts/4_2_7_bellman_optimality_equation.png&quot; alt=&quot;4_2_7_bellman_optimality_equation&quot; /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;결국 위의 식은 Bellman optimality equation 이 된다.&lt;/li&gt;
          &lt;li&gt;따라서 $v_{\pi’}$ 는 $v_*$ 와 같고 $\pi$ 와 $\pi’$ 는 최적 정책 (optimal policy) 이 된다.&lt;/li&gt;
          &lt;li&gt;정책 개선 (Policy Improvement) 는 기존 정책이 이미 최적이 아닌 경우, 엄격하게 더 나은 정책을 제공한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;확률론적 정책 (stochastic policy) 의 경우
        &lt;ul&gt;
          &lt;li&gt;지금까지 결정론적 정책 (deterministic policy) 의 경우를 고려하였음.&lt;/li&gt;
          &lt;li&gt;일반적인 경우 확률론적 정책 (stochastic policy) $\pi$ 는 확률에 특화되어 있다.&lt;/li&gt;
          &lt;li&gt;$\pi (a|s)$ : $s$ 상태에서 $a$ 행동을 선택할 확률&lt;/li&gt;
          &lt;li&gt;policy improvement theorem 은 사실 확률론적 정책으로 쉽게 확장이 가능하다.
            &lt;ul&gt;
              &lt;li&gt;최선의 행동이 복수개일 경우 특정 확률의 배분을 차지할 수 있다.&lt;/li&gt;
              &lt;li&gt;차선의 행동은 확률이 0 가 된다.&lt;/li&gt;
              &lt;li&gt;(예제 4.1 gridworld 의 iterative policy evaluation 의 수렴 참조)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy Iteration&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Policy Iteration
        &lt;ul&gt;
          &lt;li&gt;정책 $\pi$ 에서 $v_\pi$ 를 이용해 더 나은 정책 $\pi’$ 를 도출했다면..&lt;/li&gt;
          &lt;li&gt;$v_{\pi’}$ 를 계산하여 더 나은 정책 $\pi’’$ 를 도출할 수도 있다.&lt;/li&gt;
          &lt;li&gt;따라서, 우리는 단조롭게 개선되는 일련의 정책과 가치 함수를 얻을 수 있다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_3_1_policy_iteration.png&quot; alt=&quot;4_3_1_policy_iteration&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\overset{E}{\rightarrow}$ 는 policy evaluation 을,&lt;/li&gt;
          &lt;li&gt;$\overset{I}{\rightarrow}$ 는 policy improvement 를 뜻한다.&lt;/li&gt;
          &lt;li&gt;각 정책은 이전의 정책보다 엄격히 개선되었다는 것을 보장한다. (이미 최적이 아닌 경우)&lt;/li&gt;
          &lt;li&gt;유한 MDP 는 유한한 수의 정책을 가지고 있기 때문에, 이 프로세스는 반드시 최적 정책과 최적 가치함수에 유한한 반복 진행으로 수렴한다.&lt;/li&gt;
          &lt;li&gt;위의 방법으로 최적 정책을 찾는 방법을 policy iteration 이라 한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_3_2_policy_iteration_algorithm.png&quot; alt=&quot;4_3_2_policy_iteration_algorithm&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;반복 계산인 각 정책 평가 (policy evaluation) 은 이전 정책에 대한 가치 함수로 시작된다.&lt;/li&gt;
          &lt;li&gt;이것은 수렴의 속도를 크게 향상시키는데, 이전 정책에서 다음 정책으로 변화 할때, 가치 함수가 크게 바뀌지 않기 때문으로 추정된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;예제 4.2 : Jack’s Car Rental
    &lt;ul&gt;
      &lt;li&gt;문제의 설정
        &lt;ul&gt;
          &lt;li&gt;Jack 은 렌터카 회사의 두 지점을 관리한다.&lt;/li&gt;
          &lt;li&gt;매일 일정 수의 고객이 자동차를 렌트하기 위해 각 지점에 도착한다.
            &lt;ul&gt;
              &lt;li&gt;사용할 수 있는 자동차가 있으면 그것을 임대하고 $10 을 정립한다.&lt;/li&gt;
              &lt;li&gt;지점에 차가 없으면 사업을 잃게 됨&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;차량은 반납한 다음날부터 대여가 가능함&lt;/li&gt;
          &lt;li&gt;차량이 필요한 곳에서 사용될 수 있도록, 두 지점간 차량을 밤 사이에 이동시킬 수 있고, 차량 당 $2 가 든다.
            &lt;ul&gt;
              &lt;li&gt;한 위치에서 차량을 최대 5대 까지 이동할 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;각 위치에서 요청되고 반환되는 자동차의 수는 푸아송 확률 변수라고 가정한다.
            &lt;ul&gt;
              &lt;li&gt;즉, 수량이 $n$ 일 확률은 $\frac{n!}{\lambda^n} e^{-\lambda}$ 이며, $\lambda$ 는 예상치이다.&lt;/li&gt;
              &lt;li&gt;$\lambda$ 를, 첫 번째 위치와 두 번째 위치 각각, 3과 4 (렌탈 수량), 3과 2 (반납수량) 으로 가정한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;각 지점에 차량이 최대 20대까지 있을 수 있음 (추가되는 차량은 사라짐)&lt;/li&gt;
          &lt;li&gt;할인율 $\gamma = 0.9$
            &lt;ul&gt;
              &lt;li&gt;할인율을 이용해 연속적인 유한 MDP 로 가정&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;하나의 step 기준은 하루&lt;/li&gt;
          &lt;li&gt;States 는 하루가 끝날 때 각 위치의 자동차 수&lt;/li&gt;
          &lt;li&gt;Actions 는 숫자 (밤 새 두 장소 사이를 오가는 차량의 수)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;하기 이미지는 차를 전혀 움직이지 않는 정책부터 시작하여, 정책 반복으로 찾은 정책의 순서를 보여준다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_3_3_Example_4_2_Jacks_Car_rental.png&quot; alt=&quot;4_3_3_Example_4_2_Jacks_Car_rental&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Jack 의 자동차 임대 문제에 대한 정책 반복으로 찾은 정책의 순서와 최종 상태가치 함수&lt;/li&gt;
      &lt;li&gt;각 상태에 대해 몇 대의 차를 보내야 하는지를 보여줌 (음수는 두번째 위치에서 첫 번째 위치로의 이동을 나타냄)&lt;/li&gt;
      &lt;li&gt;각 정책은 이전 정책에서의 엄격한 개선을 한 것이며, 마지막 정책이 최적 정책임.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Value Iteration
    &lt;ul&gt;
      &lt;li&gt;정책 반복의 문제점
        &lt;ul&gt;
          &lt;li&gt;정책 반복의 한 가지 단점은, 각 반복에 정책 평가가 포함된다는 점이다.&lt;/li&gt;
          &lt;li&gt;정책 평가는 상태 세트를 여러 번 스윕하는 장 기간의 반복 계산일 수 있다.&lt;/li&gt;
          &lt;li&gt;반복적인 정책 평가를 하면 $v_\pi$ 로의 수렴은 극한에서만 일어난다.&lt;/li&gt;
          &lt;li&gt;그림 4.1 의 예는 정책 평가를 생략하는 것이 가능할 수 있음을 시사한다. (3번의 반복 이후에 greedy policy 에 영향을 주지 않음.)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;가치 반복 (Value Iteration)
        &lt;ul&gt;
          &lt;li&gt;정책 반복 (Policy iteration) 의 정책 평가 (Policy evaluation) 단계는 정책 반복의 수렴 보장을 유지한 채 여러 가지 방법으로 축소될 수 있음.&lt;/li&gt;
          &lt;li&gt;한 가지 중요한 특수 사례는 정책 평가 (Policy evaluation) 가 단 한번의 스윕 (각 상태의 업데이트 1회) 후에 중지되는 경우임.&lt;/li&gt;
          &lt;li&gt;위와 같은 사례 (알고리즘) 를 Value Iteration 이라 한다.&lt;/li&gt;
          &lt;li&gt;정책 개선 (Policy improvement) 과 축소된 정책 평가 (Policy evaluation) 는 단순한 업데이트 연산으로 결합할 수 있다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_4_1_value_iteration_1.png&quot; alt=&quot;4_4_1_value_iteration_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;정책 평가와 같이, Value Iteration 도 정확한 $v_*$ 로의 수렴을 위해 무한히 반복을 해야 한다.&lt;/li&gt;
          &lt;li&gt;실제로는, 가치 함수가 한 번의 스윕에서 매우 적은 양만 변화할 경우 중지한다.&lt;/li&gt;
          &lt;li&gt;아래의 박스는 완성된 알고리즘 (중지 상태를 포함) 을 보여준다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_4_2_value_iteration_2.png&quot; alt=&quot;4_4_2_value_iteration_2&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;가치 반복 (Value Iteration) 은 각 스윕에서 한 번의 정책 평가 스윕과 한 번의 정책 개선 스윕을 효과적으로 결합한다.&lt;/li&gt;
          &lt;li&gt;종종 각 정책 개선 (Policy Improvement) 스윕 사이에 여러 정책 평가 (Policy evaluation) 스윕을 삽입하여 달성된다.&lt;/li&gt;
          &lt;li&gt;일반적으로 축소된 정책 반복 알고리즘의 전체 클래스는 일부는 정책 평가 (Policy evaluation) 업데이트, 일부는 가치 반복 (Value Iteration) 업데이트 를 사용하는 스윕 시퀀스로 생각할 수 있다.
            &lt;ul&gt;
              &lt;li&gt;4.10 수식의 최대 (max) 연산이 유일한 차이점이기 때문&lt;/li&gt;
              &lt;li&gt;즉, 정책 평가의 일부 스윕에 최대 (max) 연산 작업이 추가됨을 의미함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;이러한 모든 알고리즘은 할인된 유한 MDP 에 대한 최적의 정책으로 수렴함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;예제 4.3 : Gambler’s Problem
        &lt;ul&gt;
          &lt;li&gt;문제
            &lt;ul&gt;
              &lt;li&gt;도박꾼은 일련의 동전 던지기 결과에 대해 베팅할 기회가 있다.&lt;/li&gt;
              &lt;li&gt;동전이 앞면이 나오면 그는 걸었던 만큼의 달러를 얻고, 뒷면이면 지분을 잃는다.&lt;/li&gt;
              &lt;li&gt;도박꾼이 $100 의 목표에 도달하면 이기고, 돈을 다 잃으면 끝나게 된다.&lt;/li&gt;
              &lt;li&gt;매번 도박꾼은 자신의 자본 중 어느 정도를 걸어야 할 지 정해야 한다.&lt;/li&gt;
              &lt;li&gt;이 문제는 할인되지 않은, 에피소딕한 유한한 MDP 로 공식화될 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;정의
            &lt;ul&gt;
              &lt;li&gt;상태 : 도박꾼의 자본, $s \in \lbrace 1,2,…,99 \rbrace$&lt;/li&gt;
              &lt;li&gt;행동 : 베팅 금액, $a \in \lbrace 0, 1, …, min(s, 100-s) \rbrace$&lt;/li&gt;
              &lt;li&gt;보상 : 도박꾼이 목표에 도달하였을때 +1, 그 외 모든 전환에서 0&lt;/li&gt;
              &lt;li&gt;상태가치 함수는 위의 경우 각 상태에서 승리할 확률을 제공한다.&lt;/li&gt;
              &lt;li&gt;정책은 자본과 베팅 금액 간 매핑이다.&lt;/li&gt;
              &lt;li&gt;최적의 정책은 목표에 도달할 확률을 최대화한다.&lt;/li&gt;
              &lt;li&gt;$p_h$ : 동전이 앞면이 나올 확률&lt;/li&gt;
              &lt;li&gt;$p_h$ 를 알게 되면, 전체 문제를 알 수 있고, 가치 반복 (Value Iteration) 등으로 풀 수 있다.&lt;/li&gt;
              &lt;li&gt;그림 4.3 은 가치 반복의 연속적인 스윕에 대한 가치 함수의 변화를 보여준다.&lt;/li&gt;
              &lt;li&gt;그림 4.3 은 $p_h = 0.4$ 인 경우 최적 정책을 보여준다. 이 정책은 최적이지만 고유하지는 않다.
                &lt;ul&gt;
                  &lt;li&gt;최적 가치 함수 상 값이 동률일 경우…&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_4_3_example_4_3_gamblers_problem.png&quot; alt=&quot;4_4_3_example_4_3_gamblers_problem&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Asynchronous Dynamic Programming
    &lt;ul&gt;
      &lt;li&gt;기존 DP 모델의 한계
        &lt;ul&gt;
          &lt;li&gt;MDP 의 전체 상태 집합에 대한 연산을 포함한다.&lt;/li&gt;
          &lt;li&gt;즉, 상태 집합 전체에 대한 스윕이 필요함.&lt;/li&gt;
          &lt;li&gt;상태 세트가 매우 크면 한 번의 스윕도 엄청난 비용일 수 있다.&lt;/li&gt;
          &lt;li&gt;백게먼 게임에서 상태의 수는 $10^{20}$ 개 이다.&lt;/li&gt;
          &lt;li&gt;이는 초당 백만 개의 상태의 가치 반복 업데이트를 수행할 수 있더라도, 단일 스윕을 완료하는 데 천 년 이상이 걸린다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;비동기식 DP 알고리즘
        &lt;ul&gt;
          &lt;li&gt;비동기식 DP 알고리즘은 상태 세트의 체계적 스윕이 구성되지 않은 내부 반복 DP 알고리즘이다.&lt;/li&gt;
          &lt;li&gt;이 알고리즘은 사용 가능한 다른 상태의 값을 사용하여 순서에 관계없이 상태 값을 업데이트한다.
            &lt;ul&gt;
              &lt;li&gt;일부 상태 값은 다른 상태 값이 업데이트 되기 전에 여러 번 업데이트 될 수 있다.&lt;/li&gt;
              &lt;li&gt;그러나 올바른 수렴을 위해 모든 상태의 값을 계속 업데이트 해야한다.&lt;/li&gt;
              &lt;li&gt;일정 시점 이후에는 어떤 상태도 무시할 수 없다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;비동기식 DP 알고리즘의 이점
        &lt;ul&gt;
          &lt;li&gt;비동기 DP 알고리즘은 업데이트할 상태를 선택하는 데 큰 유연성을 제공한다.
            &lt;ul&gt;
              &lt;li&gt;예를 들어 가치 반복 업데이트 (4.10) 를 이용하여, 매 스탭 $k$ 에 대해 하나의 상태인 $s_k$ 에 대해서만 업데이트 하는 것이다.&lt;/li&gt;
              &lt;li&gt;만약, $0 \leq \gamma &amp;lt; 1$ 일 경우, $v_*$ 에 대한 점근적인 수렴은 모든 상태가 시퀀스 $\lbrace s_k \rbrace$ 에서 무한한 횟수로 발생하는 경우에만 보장된다.
                &lt;ul&gt;
                  &lt;li&gt;시퀀스가 확률적일 수도 있음&lt;/li&gt;
                  &lt;li&gt;할인되지 않는 에피소드 케이스의 경우, 수렴되지 않는 일부 업데이트 순서가 있을 수 있지만 이는 상대적으로 피하기 쉬움)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;마찬가지로 정책 평가와 가치 반복 업데이트를 혼합하여 일종의 비동기식 축소된 정책 반복을 생성할 수 있음&lt;/li&gt;
              &lt;li&gt;이것과 다른 더 특이한 DP 알고리즘에 대한 세부 사항은 이 책의 범위를 벗어남&lt;/li&gt;
              &lt;li&gt;몇 가지 다른 업데이트가 다양한 Sweepless DP 알고리즘에서 유연하게 사용할 수 있는 빌딩 블록을 형성한다는 것은 분명함&lt;/li&gt;
              &lt;li&gt;스윕을 피한다고 해서 반드시 적은 계산으로 벗어날 수 있다는 의미는 아님.
                &lt;ul&gt;
                  &lt;li&gt;이는 알고리즘이 정책을 개선하기 전에 절망적으로 긴 스윕에 갇힐 필요가 없음을 의미함&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;알고리즘의 진행률을 향상시키기 위해 업데이트를 적용할 상태를 선택하여 이러한 유연성을 활용할 수 있음.
                &lt;ul&gt;
                  &lt;li&gt;값 정보가 상태에서 상태로 효율적으로 전파되도록 업데이트 순서를 정할 수 있음&lt;/li&gt;
                  &lt;li&gt;일부 상태는 다른 상태만큼 자주 값을 업데이트할 필요가 없을 수 있음&lt;/li&gt;
                  &lt;li&gt;최적의 동작과 관련이 없는 경우 일부 상태를 완전히 업데이트 하지 않을 수도 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;비동기식 알고리즘을 사용하여 실시간 상호작용 내용을 더 쉽게 혼합하여 계산할 수 있다.
            &lt;ul&gt;
              &lt;li&gt;MDP 를 풀기 위해 에이전트가 실제로 MDP 를 경험하는 동안, 동시에 반복 DP 알고리즘을 수행할 수 있다.&lt;/li&gt;
              &lt;li&gt;에이전트의 경험은 DP 알고리즘이 업데이트를 적용할 상태를 결정하는 데 사용할 수 있다.&lt;/li&gt;
              &lt;li&gt;동시에 DP 알고리즘의 최신 값과 정책 정보를 이용해 에이전트의 의사 결정에 활용할 수 있다.
                &lt;ul&gt;
                  &lt;li&gt;예를 들어 에이전트가 특정 상태를 방문할 때 상태에 업데이트를 적용할 수 있다.&lt;/li&gt;
                  &lt;li&gt;이를 통해 에이전트와 가장 관련성 높은 상태 세트 부분의 업데이트에 집중할 수 있다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generalized Policy Iteration
    &lt;ul&gt;
      &lt;li&gt;정책 반복 (Policy Iteration) 에 대한 내용 정리
        &lt;ul&gt;
          &lt;li&gt;정책 반복은 두 개의 동시 상호 작용 프로세스로 구성됨
            &lt;ul&gt;
              &lt;li&gt;현재 정책과 일치하는 가치 함수를 만들기 (정책 평가)&lt;/li&gt;
              &lt;li&gt;현재 가치함수와 관련하여 정책을 탐욕스럽게 만들기 (정책 개선)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;정책 반복 : 이 두 프로세스는 서로 번갈아가며 다른 프로세스가 시작되기 전에 완료됨&lt;/li&gt;
          &lt;li&gt;가치 반복 : 각 정책 개선 사이에 정책 평가의 단일 반복만 수행됨&lt;/li&gt;
          &lt;li&gt;비동기식 DP 방법 : 평가 및 개선 프로세스가 훨씬 더 세밀하게 끼워넣어짐
            &lt;ul&gt;
              &lt;li&gt;예 : 다른 프로세스로 돌아가기 전에 한 프로세스에서 단일 상태가 업데이트됨&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;두 프로세스가 계속해서 모든 상태를 업데이트 하는 한, 궁극적인 결과는 일반적으로 최적 가치 함수와 최적 정책으로 수렴됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Generalized Policy Iteration (GPI) : 일반화된 정책 반복
        &lt;ul&gt;
          &lt;li&gt;우리는 일반화된 정책 반복 (GPI) 이라는 용어를 사용하여 정책 평가 및 정책 개선 과정이 서로 상호작용하는 일반 아이디어를 나타낸다.
            &lt;ul&gt;
              &lt;li&gt;두 과정의 세부 사항과 세분화와는 독립적인 개념임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;거의 모든 강화학습 방법은 GPI로 잘 설명되어 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_4_4_gpi_diagram.png&quot; alt=&quot;4_4_4_gpi_diagram&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;즉, 모든 것들은 식별 가능한 정책과 가치 함수를 가지며, 정책은 항상 가치 함수에 대해 개선되고, 가치 함수는 항삭 정책에 대한 가치 함수로 향하도록 조정된다.&lt;/li&gt;
          &lt;li&gt;만약 평가 과정과 개선 과정이 모두 안정화 되면, 즉 더 이상 변화가 발생하지 않으면, 가치 함수와 정책은 최적이어야 한다.&lt;/li&gt;
          &lt;li&gt;가치 함수는 현 정책과 일치할 때에만 안정화 되고, 정책은 현 가치함수에 대해 탐욕스러울 경우에만 안정화 된다.&lt;/li&gt;
          &lt;li&gt;따라서 두 과정이 모두 안정화되려면, 자신의 평가 함수에 대해 탐욕스러운 정책을 찾아야 한다.&lt;/li&gt;
          &lt;li&gt;이는 벨만 최적 방정식 (4.1) 이 성립하며, 정책과 가치함수가 최적임을 의미한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/4_4_5_gpi_diagram_2.png&quot; alt=&quot;4_4_5_gpi_diagram_2&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;GPI 의 평가 및 개선 프로세스는 경쟁과 협력으로 볼 수 있음
            &lt;ul&gt;
              &lt;li&gt;정책을 가치 함수에 대해 탐욕스럽게 만들면 일반적으로 가치 함수가 변경된 정책에 대해 부정확해짐&lt;/li&gt;
              &lt;li&gt;가치 함수를 정책과 일치하게 만들면 해당 정책이 더 이상 탐욕스러운 정책이 아니게 됨.&lt;/li&gt;
              &lt;li&gt;장기적으로 이 두 과정은 하나의 공통해결책인 최적의 가치 함수와 최적의 정책을 찾기 위해 상호작용함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위 다이어그램에서 제안된 대로 2차원 공간의 두 개의 선으로 생각해볼 수 있음.
            &lt;ul&gt;
              &lt;li&gt;실제 기하학은 이보다 훨씬 복잡하나, 이 다이어그램은 실제 상황이 어떤 식으로 일어나는지를 시사함.&lt;/li&gt;
              &lt;li&gt;각 과정은 가치함수, 정책을 두고 두 목표중 하나의 해결책을 나타내는 선으로 이끈다.&lt;/li&gt;
              &lt;li&gt;두 목표는 직교하지 않기 때문에 목표 간 상호작용이 발생 (한 목표로 직접적으로 나아가면 다른 목표에서 멀어지는 움직임)&lt;/li&gt;
              &lt;li&gt;그러나 결국 공동 프로세스는 최적의 목표에 가까워지게 된다.&lt;/li&gt;
              &lt;li&gt;GPI 에서 각 목표에 대해 작고 불완전한 단계를 취할 수도 있으나, 어느 경우에도 두 과정은 전반적인 최적의 목표를 달성하기 위해 함께 작동한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Efficiency of Dynamic Programming
    &lt;ul&gt;
      &lt;li&gt;MDP 문제에서 DP 방식이 가지는 효율성
        &lt;ul&gt;
          &lt;li&gt;DP(Dynamic Programming)는 매우 큰 문제에는 적합하지 않을 수 있지만, MDP(Markov Decision Processes)를 해결하기 위한 다른 방법과 비교하면 DP 방법은 실제로 꽤 효율적임.&lt;/li&gt;
          &lt;li&gt;일부 기술적인 세부 사항을 무시한다면, DP 방법이 최적 정책을 찾는 데 걸리는 (최악의 경우) 시간은 상태와 액션의 수에 다항식으로 표현됨.
            &lt;ul&gt;
              &lt;li&gt;n과 k가 상태와 액션의 수를 나타낸다면, DP 방법은 n과 k의 다항식 함수보다 적은 계산 작업을 필요로 함.&lt;/li&gt;
              &lt;li&gt;DP 방법은 전체 정책의 수가 $k^n$ 인 것과는 상관없이 다항 시간 내에 최적 정책을 찾을 것을 보장함.&lt;/li&gt;
              &lt;li&gt;(직접 탐색 (모든 경우의 수, 가능한 정책을 검사하고 평가) 은 동일한 보장을 제공하기 위해 각 정책을 철저히 검사해야 함)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;일부의 경우 선형 프로그래밍 방법 (문제를 선형 제약 조건과 목적함수를 활용해 수학적으로 리모델링) 도 MDP를 해결하는 데 사용될 수 있으며, DP 방법보다 수렴 보장이 더 좋을 수 있음.
            &lt;ul&gt;
              &lt;li&gt;예를 들어, MDP 문제에서 상태 공간이 크고 액션 수가 상대적으로 작은 경우&lt;/li&gt;
              &lt;li&gt;혹은 작은 상태 수&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;그러나 가장 큰 문제에 대해서는 DP 방법만이 실현 가능함.
            &lt;ul&gt;
              &lt;li&gt;DP(Dynamic Programming)은 종종 차원의 저주(curse of dimensionality)라는 이유로 적용 가능성이 제한적으로 생각되기도 한다. 이는 상태 변수의 수에 따라 상태의 수가 지수적으로 증가하기 때문.&lt;/li&gt;
              &lt;li&gt;큰 상태 집합은 어려움을 일으킬 수 있지만, 이는 문제의 본질적인 어려움이며 DP 자체의 해결 방법의 어려움은 아니다.&lt;/li&gt;
              &lt;li&gt;실제로 DP는 직접 탐색이나 선형 프로그래밍과 같은 경쟁적 위치의 방법들보다 큰 상태 공간을 처리하는 데 더 적합함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;비동기 DP 의 활용방식
        &lt;ul&gt;
          &lt;li&gt;실제로 DP(Dynamic Programming) 방법은 오늘날의 컴퓨터를 사용하여 수백만 개의 상태를 가진 MDP(Markov Decision Process)를 해결하는 데에 사용될 수 있다.&lt;/li&gt;
          &lt;li&gt;정책 이터레이션(policy iteration)과 가치 이터레이션(value iteration) 모두 널리 사용되며, 일반적으로 어떤 것이 더 우수한지 명확하지 않음.&lt;/li&gt;
          &lt;li&gt;이러한 방법들은 보통 좋은 초기 값 함수 또는 정책으로 시작되었을 때, 이론적인 최악의 경우 실행 시간보다 훨씬 빠르게 수렴함.&lt;/li&gt;
          &lt;li&gt;상태 공간이 큰 문제에서는 비동기적인 DP 방법이 종종 선호됨.&lt;/li&gt;
          &lt;li&gt;동기적인 방법의 경우 하나의 전체 순회를 완료하기 위해서는 모든 상태에 대한 계산과 메모리가 필요함.&lt;/li&gt;
          &lt;li&gt;일부 문제에서는 심지어 이 정도의 메모리와 계산도 실용적이지 않을 수 있지만, 최적 해결 경로에는 상대적으로 적은 수의 상태가 발생할 수 있으므로 문제는 여전히 해결이 가능함.&lt;/li&gt;
          &lt;li&gt;비동기적인 방법과 GPI의 다른 변형들은 이러한 경우에 적용될 수 있으며, 동기적인 방법보다 좋거나 최적의 정책을 훨씬 빠르게 찾을 수 있음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;policy-evaluation-prediction&quot;&gt;Policy Evaluation (Prediction)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Policy Evaluation vs. Control&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습 목표
        &lt;ul&gt;
          &lt;li&gt;정책 평가 (policy evaluation) 와 제어 (control) 의 구분&lt;/li&gt;
          &lt;li&gt;동적 프로그래밍 (Dynamic Programming) 이 적용될 수 있는 설정(환경) 과 제한을 이해&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Policy Evaluation 과 Control 의 정의
        &lt;ul&gt;
          &lt;li&gt;Policy Evaluation : 주어진 정책에 대한 stable 한 가치 함수를 구하는 것 (얼마나 좋은지 평가)
            &lt;ul&gt;
              &lt;li&gt;$\pi \to v_\pi$&lt;/li&gt;
              &lt;li&gt;$v_\pi (s) \doteq E_\pi [ G_t | S_t = s]$&lt;/li&gt;
              &lt;li&gt;$G_t \doteq \sum_{k=0}^\infty \gamma^k R_{t+k+1}$
                &lt;ul&gt;
                  &lt;li&gt;리턴 값은 미래의 보상에 대한 할인된 합계이다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Control : 가치 함수를 통해 가장 많은 보상을 얻는 정책을 찾는 것 (정책을 발전시키는 것)&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_control_1.png&quot; alt=&quot;policy_control_1&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;모든 상태에서의 가치가 같거나 더 나은 정책을 찾는 것&lt;/li&gt;
              &lt;li&gt;반복적으로 찾다 보면 최적의 정책을 찾게 됨.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Dynamic Programming Algorithms&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;벨만 방정식을 사용해 가치 평가와 제어의 반복적인 알고리즘을 정의하는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/linear_solver_vs_dynamic_programming_1.png&quot; alt=&quot;linear_solver_vs_dynamic_programming_1&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Linear equations 와 Dynamic Programming 비교
        &lt;ul&gt;
          &lt;li&gt;Linear equations
            &lt;ul&gt;
              &lt;li&gt;가치함수 $v_\pi$ 를 찾기 위해, 각 상태별로 위의 하나의 방정식을 갖게 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Dynamic Programming
            &lt;ul&gt;
              &lt;li&gt;통상 MDP 의 문제에서 DP 방식이 보다 더 적절한 방식이다.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/dynamic_programming_1.png&quot; alt=&quot;dynamic_programming_1&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;DP 에서는 다양한 형태의 벨만 방정식을 사용한다.&lt;/li&gt;
              &lt;li&gt;위의 경우 환경역학 $p$ 에 대한 지식을 기반으로 한다.&lt;/li&gt;
              &lt;li&gt;고전적인 DP 에서는 환경과의 상호작용을 포함하지 않는다. (대신 주어진 MDP 모델을 활용 / 함수 $p$ 에 접근할 수 있다는 가정.)&lt;/li&gt;
              &lt;li&gt;대부분의 강화학습 알고리즘은 모델이 없는 DP 의 근사화된 프로그래밍이라고 볼 수 있다.&lt;/li&gt;
              &lt;li&gt;(이러한 특징은 이후에 소개할 Temporal different space dynamic planning algorithm 에서 두드러진다.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Iterative Policy Evaluation&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;주어진 정책에서 상태값 평가를 위한 반복 정책 평가 (iterative policy evaluation) 알고리즘의 개요&lt;/li&gt;
          &lt;li&gt;반복 정책 평가 (iterative policy evaluation) 를 적용하여 가치함수 계산&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;벨만 방정식과 Iterative Policy Evaluation 간의 관계
        &lt;ul&gt;
          &lt;li&gt;DP 알고리즘은 벨만 방정식을 업데이트 룰로 변경함으로써 얻을 수 있다.&lt;/li&gt;
          &lt;li&gt;iterative policy evaluation 알고리즘 또한 이러한 알고리즘 중 하나이다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/iterative_policy_evaluation_update_rule_1.png&quot; alt=&quot;iterative_policy_evaluation_update_rule_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;벨만 방정식 중 $v_\pi$ 에 대핸 재귀적 표현을 활용한다.&lt;/li&gt;
          &lt;li&gt;업데이트 룰에서는 참 가치함수가 아닌 예측 값을 활용한다.
            &lt;ul&gt;
              &lt;li&gt;이 방식은 점진적으로 보다 나은 대략적인 가치 함수를 제공하게 된다.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/iterative_policy_evaluation_update_rule_2.png&quot; alt=&quot;iterative_policy_evaluation_update_rule_2&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;각각의 반복은 모든 상태 집합에 대해 업데이트를 적용하는데, 이를 스윕 (sweep) 이라고 한다.&lt;/li&gt;
              &lt;li&gt;만약 모든 상태에 대해, 가치 함수 근사값 $v_k$ 와 $v_{k+1}$ 의 값이 같을 경우 정책에 대한 참 가치 함수를 찾았다고 한다.&lt;/li&gt;
              &lt;li&gt;${v_0}$ 가 어떤 값이여도, $k$ 가 무한대에 수렴하면, $v_k$ 또한 $v_\pi$ 로 수렴하게 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;구현의 방식
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;2개의 배열 사용&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/iterative_policy_evaluation_two_array.png&quot; alt=&quot;iterative_policy_evaluation_two_array&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;모든 상태 세트에 대해 업데이트를 진행한다.&lt;/li&gt;
              &lt;li&gt;Old value V 를 이용해 New value V’ 를 갱신한다.&lt;/li&gt;
              &lt;li&gt;Old value V 는 업데이트 중에 변동이 없다.&lt;/li&gt;
              &lt;li&gt;모든 상태를 순회, 업데이트 후에 V’ 를 V 에 할당하고, V’ 에 다시 업데이트를 진행한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;1개의 배열 사용&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/iterative_policy_evaluation_one_array.png&quot; alt=&quot;iterative_policy_evaluation_one_array&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;V 배열만을 이용해 업데이트를 진행한다.&lt;/li&gt;
              &lt;li&gt;경우에 따라 특정 상태의 값을 참조할 때, Old value 가 아닌 New value 를 참조하기도 한다.&lt;/li&gt;
              &lt;li&gt;이러한 한 개의 배열 버전 또한 수렴을 보장하며, 사실 보통의 경우 최신 값을 사용하기에 더 빠르게 수렴한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;여기에서는 단순성을 위해 2개의 배열 버전에 집중한다.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Iterative Policy Evaluation 의 예시
        &lt;ul&gt;
          &lt;li&gt;4 x 4 의 grid world 를 가정&lt;/li&gt;
          &lt;li&gt;좌측 상단과 우측 하단에 Terminate State 가 있는 Episodic MDP 로 정의&lt;/li&gt;
          &lt;li&gt;모든 상태이동의 보상은 -1&lt;/li&gt;
          &lt;li&gt;할인 값은 없다고 가정 ($\gamma = 1$)&lt;/li&gt;
          &lt;li&gt;각 상태별로 4개의 방향으로 이동할 수 있음 (up, down, left, right). 각 행동은 결정론적임 (확률=100%)&lt;/li&gt;
          &lt;li&gt;그리도 밖으로의 이동은 에이전트가 해당 상태에 그대로 머물도록 함&lt;/li&gt;
          &lt;li&gt;정책은 uniform random policy (각 확률 25%).&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/iterative_policy_evaluation_example_1.png&quot; alt=&quot;iterative_policy_evaluation_example_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;스윕은 좌에서 우로, 위에서 아래로 진행된다.&lt;/li&gt;
          &lt;li&gt;첫 스윕 결과는 위의 식에 의해 Terminal State 를 제외하고 모두 -1 이 된다.&lt;/li&gt;
          &lt;li&gt;첫 풀 스윕 이후 V’ 을 V 로 카피하고, 위 과정을 반복한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/iterative_policy_evaluation_psuedo.png&quot; alt=&quot;iterative_policy_evaluation_psuedo&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위는 iterative policy evaluation 의 전체 알고리즘이다.&lt;/li&gt;
          &lt;li&gt;각 상태의 이전과 이후의 차이 ($\delta$) 가 정의한 작은 숫자 ($\theta$) 보다 작을 경우 루프를 중지한다.&lt;/li&gt;
          &lt;li&gt;$\theta$ 가 충분히 작을 경우, V 는 $v_\pi$ 에 가까운 값이라 할 수 있다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/iterative_policy_evaluation_example_2.png&quot; alt=&quot;iterative_policy_evaluation_example_2&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;policy-iteration-control&quot;&gt;Policy Iteration (Control)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Policy Improvement&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;정책 개선 이론 (policy improvement theorem) 이해하기&lt;/li&gt;
          &lt;li&gt;주어진 MDP 에서 더 나은 정책의 생성을 위해 정책에 가치함수 적용하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Policy Improvement&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_improvement_1.png&quot; alt=&quot;policy_improvement_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;최적 가치함수 $v_*$ 에 대해 greedy action 을 취한 정책을 최적 정책 (Optimal Policy) 이라 한다.&lt;/li&gt;
          &lt;li&gt;임의의 정책 $\pi$ 를 따르는 가치 함수 $v_\pi$ 에 대해 greedy action 을 $v_\pi$ 에 대한 탐욕 정책이라 했을 때…&lt;/li&gt;
          &lt;li&gt;현재의 정책 $\pi$ 와 위의 정책이 차이가 없을 경우 $\pi$ 는 이미 $v_\pi$ 에 대한 탐욕 정책이며,&lt;/li&gt;
          &lt;li&gt;이 경우 $v_\pi$ 가 벨만 최적성 방정식을 따른다면, $\pi$ 는 최적 정책이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Policy Improvement Theorem
        &lt;ul&gt;
          &lt;li&gt;$\pi$ 가 최적정책이 아니라면, $\pi$ 보다 엄격한 개선이 이루어진 새로운 정책이 존재한다.&lt;/li&gt;
          &lt;li&gt;$q_\pi (s, \pi’(s)) \geq q_\pi (s, \pi (s))$ for all $s \in S$  $\to \pi’ \geq \pi$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_improvement_theorem_1.png&quot; alt=&quot;policy_improvement_theorem_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;uniform random policy 를 따르는 $v_\pi$ 가치함수에 대해, greedy policy $\pi’$ 이 Policy Improvement Theorem 에 의해 더 개선된 정책임.&lt;/li&gt;
          &lt;li&gt;Policy Improvement Theorem 은 새로운 정책이 이전 정책보다 개선된 정책임 만을 보장한다.
            &lt;ul&gt;
              &lt;li&gt;개선된 정책이 최적 정책임은 보장하지 않음 (가치함수가 최적 가치함수가 아님)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy Iteration&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;최적 정책을 찾기 위한 정책 반복 알고리즘 (policy iteration algorithm) 을 정의하기&lt;/li&gt;
          &lt;li&gt;“the dance of policy and value” (평가와 개선을 반복하여 최적 정책을 찾는 것) 이해하기&lt;/li&gt;
          &lt;li&gt;정책 반복 (policy iteration) 을 적용하여 최적 정책과 최적 가치 함수 계산하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Policy Iteration&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_1.png&quot; alt=&quot;policy_iteration_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;더 이상 가치 함수를 통한 정책이 변경되지 않으면 그것이 최적 정책이다.&lt;/li&gt;
          &lt;li&gt;결정론적 정책을 사용하기에, 필연적으로 최적 정책에 도달한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_2.png&quot; alt=&quot;policy_iteration_2&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_1$ 에 대한 가치함수 $v_{\pi_1}$ 을 구하면, $\pi_1$ 은 더이상 greedy policy 가 아니게 되고…&lt;/li&gt;
          &lt;li&gt;greedy policy $\pi_2$ 를 구하면 더 이상 가치함수 $v_{\pi_1}$ 이 정확한 가치함수가 아니게 된다.&lt;/li&gt;
          &lt;li&gt;위의 과정을 반복하면 필연적으로 변하지 않는 정책 $\pi_*$ 와 정확한 가치함수 $v_*$ 를 구하게 된다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_psuedo_code.png&quot; alt=&quot;policy_iteration_psuedo_code&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;4x4 gridworld 예제의 정의&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_1.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_0$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_2.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_2&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$v_{\pi_0}$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_3.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_3&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_1$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_4.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_4&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$v_{\pi_1}$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_5.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_5&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_2$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_6.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_6&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$v_{\pi_2}$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_7.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_7&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_3$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_8.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_8&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_*$&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_example_4by4_gridworld_9.png&quot; alt=&quot;policy_iteration_example_4by4_gridworld_9&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;The Power of Policy Iteration&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_iteration_power_of_p_i.png&quot; alt=&quot;policy_iteration_power_of_p_i&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;최적 정책에 도달하기 전까지, 계속적으로 정책이 개선됨을 볼 수 있다.&lt;/li&gt;
          &lt;li&gt;최적 정책이 선형적이지 않을 때도 정책에 도달할 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generalized-policy-iteration&quot;&gt;Generalized Policy Iteration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Flexibility of the Policy Iteration Framework&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;일반화된 정책 반복 프레임워크 (framework of generalized policy iteration) 이해하기&lt;/li&gt;
          &lt;li&gt;가치 반복 (value iteration) 과 일반화된 정책 반복 (generalized policy iteration) 의 주요 예시&lt;/li&gt;
          &lt;li&gt;동기 (synchronous) / 비동기 (asynchronous) 동적 프로그래밍 방법 간 차이점을 이해하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;유연한 Policy Iteration&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/flexible_policy_iteration_1.png&quot; alt=&quot;flexible_policy_iteration_1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;현 정책에 가까운 가치 함수 예측치를 사용&lt;/li&gt;
          &lt;li&gt;좀 더 탐욕적인 정책을 사용하나, 완전히 탐욕적인 정책은 아닌 정책을 사용&lt;/li&gt;
          &lt;li&gt;이러한 진행 또한 최적 정책과 최적 가치함수를 향해 나아간다.&lt;/li&gt;
          &lt;li&gt;이러한 Policy Iteration 을 Generalized Policy Iteration 이라고 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Value Iteration
        &lt;ul&gt;
          &lt;li&gt;Generalized Policy Iteration 중 하나&lt;/li&gt;
          &lt;li&gt;모든 상태를 sweep 하고, 현 가치 함수에 대해 탐욕 정책을 사용하는 것은 같음.&lt;/li&gt;
          &lt;li&gt;그러나, 완전한 정책 평가를 하는 것은 아님
            &lt;ul&gt;
              &lt;li&gt;모든 상태에 대해 단 한번의 스윕만을 진행&lt;/li&gt;
              &lt;li&gt;스윕 진행 후 다시 탐욕 정책을 사용함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/value_iteration_psuedo_code.png&quot; alt=&quot;value_iteration_psuedo_code&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;이러한 업데이트 룰을 상태 가치함수에 바로 적용한다.
            &lt;ul&gt;
              &lt;li&gt;$V(s) \gets \max_a \sum_{s’,r} p(s’,r|s,a) [r + \gamma V(s’) ]$&lt;/li&gt;
              &lt;li&gt;업데이트가 어떠한 특정 정책을 참조하지 않기 때문에, 이 방식을 value iteration 이라 한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;이 방식은 iterative policy evaluation 과 매우 유사한데, 고정된 정책을 이용해 업데이트 하는 것이 아닌, 현재의 추정 값을 이용해 최대화 하는 것이 특징이다.&lt;/li&gt;
          &lt;li&gt;value iteration 또한 극한 값에서 $v_*$ 에 수렴한다.&lt;/li&gt;
          &lt;li&gt;우리는 극한으로 진행될 때까지 기다릴 수 없으므로, 종료 조건을 둔다.&lt;/li&gt;
          &lt;li&gt;최종적으로 구해진 최적 가치 함수에 대해 $\arg\max$ 를 취함으로서 최적 정책을 얻게 된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Avoiding full sweeps
        &lt;ul&gt;
          &lt;li&gt;Synchronous DP
            &lt;ul&gt;
              &lt;li&gt;value iteration 또한 policy evaluation iteration 과 마찬가지로 모든 상태를 (순차적으로) 스윕한다.&lt;/li&gt;
              &lt;li&gt;시스템 적으로 스윕을 하는 방식을 synchronus (동기 방식) 라 한다.&lt;/li&gt;
              &lt;li&gt;만약 상태공간이 크다면, 이러한 방식은 문제가 된다.
                &lt;ul&gt;
                  &lt;li&gt;모든 스윕 단계에 매우 긴 시간을 소모함&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Asynchronous DP
            &lt;ul&gt;
              &lt;li&gt;상태의 값을 특정 순서 없이 업데이트 한다. (시스템 적인 스윕이 아님).&lt;/li&gt;
              &lt;li&gt;다른 상태값이 한번 업데이트 되는 동안 특정 상태값을 여러 번 업데이트 할 수 있음.&lt;/li&gt;
              &lt;li&gt;수렴을 보장하기 위해서는, 계속하여 모든 상태의 값을 업데이트 해야함.
                &lt;ul&gt;
                  &lt;li&gt;예를 들어 다른 상태를 무시하고 3개의 상태값만 계속 업데이트 한다면, 다른 상태의 값이 옳을 리가 없기 때문에 수렴할 수가 없음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;이러한 선택적 업데이트 덕분에, 값 정보를 빠르게 전파할 수 있음.
                &lt;ul&gt;
                  &lt;li&gt;어떠한 경우에서는 시스템적인 스윕보다 더 효율적일 수 있음.
                    &lt;ul&gt;
                      &lt;li&gt;예를 들어 최근 값이 변한 상태의 주변 값들을 집중적으로 업데이트.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Efficiency of Dynamic Programming&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;최적 정책을 찾기 위한 대안으로서의 무작위 탐색 방법 (brute force search) 설명&lt;/li&gt;
          &lt;li&gt;가치 함수 학습을 위한 대안으로서의 몬테카를로 (Monte Carlo) 방식 설명&lt;/li&gt;
          &lt;li&gt;최적 정책 탐색에 있어서 동적 프로그래밍 (Dynamic programming) 과 부트스트래핑 (bootstrapping) 방식이 대안 전략과 비교하여 가지는 이점을 이해하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;A Sampling Alternative for Policy Evaluation (Monte Carlo)
        &lt;ul&gt;
          &lt;li&gt;Dynamic Programming 의 policy evaluation iteration 의 대안&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Monte Carlo 방식&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_evaluation_monte_carlo_1.png&quot; alt=&quot;policy_evaluation_monte_carlo_1&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;정책 $\pi$ 에 대한 많은 리턴값을 수집하여 평균 값을 구하는 방식&lt;/li&gt;
              &lt;li&gt;결국 값에 수렴하게 됨&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_evaluation_monte_carlo_2.png&quot; alt=&quot;policy_evaluation_monte_carlo_2&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;수렴을 위해서는 각 상태에 대한 많은 리턴 수집값이 필요
                &lt;ul&gt;
                  &lt;li&gt;이 값들은 $\pi$ 에 의해 선택된 random action, 환경 역학에 의한 random state transition 등 많은 random 성을 띄게 됨.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;이러한 과정을 모든 상태에 대해 별개로 진행해야 함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Dynamic Programming 의 이점&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_evaluation_monte_carlo_2.png&quot; alt=&quot;policy_evaluation_dp_bootstrapping_1&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;Dynamic Programming 의 핵심은 각 상태의 평가를 별개의 문제로 취급할 필요가 없다는 점이다.
                &lt;ul&gt;
                  &lt;li&gt;이미 계산해 놓은 다른 상태의 값을 이용할 수 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;이렇게 이후 상태의 추측값을 사용해 현재의 추측값을 개선하는 것을 부트스르래핑 (bootstrapping) 이라 한다.
                &lt;ul&gt;
                  &lt;li&gt;이 방식이 각각의 상태를 별개로 계산하는 몬테카를로 방식보다 훨씬 효율적임.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Brute-Force Search
        &lt;ul&gt;
          &lt;li&gt;Dynamic Programming 의 policy iteration 의 대안&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Brute-Force Search 방식&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/policy_improvement_brute_force_search_1.png&quot; alt=&quot;policy_improvement_brute_force_search_1&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;이 방식은 단순히 모든 결정론적인 정책을 하나하나 평가하여 가장 높은 값의 정책을 선택하는 것임.&lt;/li&gt;
              &lt;li&gt;정책의 수는 유한하고, 언제나 최적 결정론적 정책은 존재함으로 최적 정책을 찾을 수 있음.&lt;/li&gt;
              &lt;li&gt;그러나 결정론적인 정책의 수가 너무 많을 수도 있음.
                &lt;ul&gt;
                  &lt;li&gt;각각의 상태에 대해 하나의 행동을 선택해야 함.
                    &lt;ul&gt;
                      &lt;li&gt;$| \mathscr{A} | * | \mathscr{A} | * \cdots * | \mathscr{A} |$&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;즉, 결정론적 정책의 수는 지수적이다.
                    &lt;ul&gt;
                      &lt;li&gt;${| \mathscr{A} |}^{| \mathscr{S} |}$&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;따라서 이 프로세스는 시간이 매우 오래 걸린다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Policy Improvement Theorem 의 이점
            &lt;ul&gt;
              &lt;li&gt;점점 더 나은 정책을 찾게 된다.&lt;/li&gt;
              &lt;li&gt;이 점은 모든 정책에 대한 검색보다 훨씬 효율적이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Efficiency of Dynamic Programming
        &lt;ul&gt;
          &lt;li&gt;Policy Iteration : $| \mathscr{S} |$ 와 $| \mathscr{A}|$ 에 대한 다항식 곱의 복잡도&lt;/li&gt;
          &lt;li&gt;Brute-Force Search : ${| \mathscr{A} |}^{| \mathscr{S} |}$ 개의 정책&lt;/li&gt;
          &lt;li&gt;Dynamic Programming 은 Brute-Force Search 에 비해 지수적으로 빠름
            &lt;ul&gt;
              &lt;li&gt;예를 들어 4x4 Grid World 의 경우 DP 는 위의 예시에서 약 5번의 스윕을 통해 최적 정책을 찾아냈으나, Brute-Force Search 의 경우 $4^{16}$ 개의 정책을 확인해야 함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The Curse of Dimensionality (차원의 저주)
        &lt;ul&gt;
          &lt;li&gt;관계된 요소의 수가 늘어날 수록 상태 공간의 크기가 지수적으로 늘어남&lt;/li&gt;
          &lt;li&gt;MDP 문제는 상태의 크기가 커질 수록 풀기 어려워짐&lt;/li&gt;
          &lt;li&gt;하나의 에이전트가 Grid World 를 탐험하는 것은 괜찮지만, 대중교통을 설계하기 위해 몇천 명의 운전자가 수백 개의 지역을 돌아다니는 상태를 가정하면 어떻게 될까?&lt;/li&gt;
          &lt;li&gt;사실 이는 Dynamic Programming 의 문제가 아닌 문제 자체의 내제된 복잡성이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Warren Powell: Approximate Dynamic Programming for Fleet Management(Short)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Warren Powell: Approximate Dynamic Programming for Fleet Management(Long)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Week 4 Summary&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Policy evaluation : 특정한 정책 $\pi$ 로부터 상태가치함수 $v_\pi$ 를 구하는 것
        &lt;ul&gt;
          &lt;li&gt;Iterative Policy Evaluation
            &lt;ul&gt;
              &lt;li&gt;$v_\pi (s) = \sum_a \pi (a|s) \sum_{s’} \sum_r p(s’,r | s,a) [ r+\gamma v_\pi (s’) ]$&lt;/li&gt;
              &lt;li&gt;$v_{k+1} (s) \gets \sum_a \pi (a|s) \sum_{s’} \sum_r p(s’,r | s,a) [ r+\gamma v_k (s’) ]$&lt;/li&gt;
              &lt;li&gt;$v_\pi$ 에 대한 벨만 방정식을 업데이트 룰로 바꾼 것&lt;/li&gt;
              &lt;li&gt;반복 과정을 거치며 점점 더 근사하는 가치함수를 찾을 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Control : 정책을 발전시키는 과정
        &lt;ul&gt;
          &lt;li&gt;Policy improvement theorem
            &lt;ul&gt;
              &lt;li&gt;$\pi’ (s) \doteq \arg\max_a \sum_{s’} \sum_r p(s’,r | s,a) [ r+\gamma v_\pi (s’) ]$&lt;/li&gt;
              &lt;li&gt;새로운 정책 $\pi’$ 는 현 가치함수에서 단순히 탐욕화한 정책이다.&lt;/li&gt;
              &lt;li&gt;$\pi’$ 은 $\pi$ 보다 엄격히 개선된 정책임을 보장한다. ($\pi$ 가 최적정책이 아닐 경우)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Policy Iteration
            &lt;ul&gt;
              &lt;li&gt;$E \to I \to \cdots \to E \to I \to v_* , \pi_*$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Generalized Policy Iteration
            &lt;ul&gt;
              &lt;li&gt;Policy Iteration 과 달리 Evaluation 과 Improvement step 을 끝까지 진행하지 않고 반복하는 것
                &lt;ul&gt;
                  &lt;li&gt;value iteration : Generalized Policy Iteration 의 한 종류로, 모든 상태를 단 한번 스윕하고 정책을 개선시키는 것&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;asynchronous DP
                &lt;ul&gt;
                  &lt;li&gt;모든 상태를 시스템적으로 스윕하는 것이 아닌, 불규칙적인 방식으로 상태를 업데이트 하는 것&lt;/li&gt;
                  &lt;li&gt;모든 상태를 지속적으로 업데이트한다는 가정 하에 최적 정책으로 수렴하게 됨&lt;/li&gt;
                  &lt;li&gt;특정 상황에서 더 빠르게 수렴할 수 있으며, 상태 공간이 큰 문제에 효율적임&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Chapter Summary (RLbook2018 Pages 88-89)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;weekly-assessment&quot;&gt;Weekly Assessment&lt;/h2&gt;

&lt;h2 id=&quot;course-wrap-up&quot;&gt;Course Wrap-up&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bandits&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/wrap_up_1_bandits_1.png&quot; alt=&quot;wrap_up_1_bandits_1&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;각 레버의 보상의 분포를 모르기 때문에, 각각의 arm 을 많이 시도하여 평균을 구해야 했었음.&lt;/li&gt;
      &lt;li&gt;Exploration - Exploitation Trade-Off
        &lt;ul&gt;
          &lt;li&gt;지금 당장의 최선의 arm 을 당길 것인지, 다른 arm 을 탐험할 것인지?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bandit 문제는 늘 같은 state 에서 action 을 선택하는 문제였음.
        &lt;ul&gt;
          &lt;li&gt;불변하는 하나의 최상의 action 이 존재&lt;/li&gt;
          &lt;li&gt;보상은 지연되지 않고 즉시 지급되었음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MDP&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/wrap_up_2_mdp_1.png&quot; alt=&quot;wrap_up_2_mdp_1&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Bandits 보다 복잡한 현실 문제를 더 잘 반영한 모델&lt;/li&gt;
      &lt;li&gt;환경은 action 을 선택하였을 때 즉각적인 보상 뿐만 아니라 다음의 상태도 제공해 줌
        &lt;ul&gt;
          &lt;li&gt;이 상태는 미래의 보상에 잠재적인 영향을 준다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상은 미래에 받을 잠재적 보상값의 할인된 합계이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Basic Concepts of Reinforcement learning
    &lt;ul&gt;
      &lt;li&gt;정책 (policy) : 에이전트가 각 상태 (state) 에서 어떤 action 을 취할 것인지를 말함&lt;/li&gt;
      &lt;li&gt;가치함수 (value function)
        &lt;ul&gt;
          &lt;li&gt;$v_\pi (s) \doteq E_{\pi} [ G_t | S_t = s ]$&lt;/li&gt;
          &lt;li&gt;특정 정책 하에 각 상태에 대해 미래의 예상되는 리턴 값을 측정해줌&lt;/li&gt;
          &lt;li&gt;혹은 특정 정책 하에  상태-행동 쌍에 대한 미래의 예상되는 리턴 값을 측정해줌&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;벨만 방정식
        &lt;ul&gt;
          &lt;li&gt;각 상태 혹은 상태-행동 쌍의 값을 가능한 다음 값과 연결 시켜주는 방정식 (부트스트래핑)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dynamic Programming&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/wrap_up_3_dp_1.png&quot; alt=&quot;wrap_up_3_dp_1&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;prediction (Policy Evaluation)&lt;/li&gt;
      &lt;li&gt;control (Policy Improvement)&lt;/li&gt;
      &lt;li&gt;Dynamic Programming 의 경우 환경 역학 (Environment dynamics) 에 접근할 수 있어야 한다.&lt;/li&gt;
      &lt;li&gt;강화학습 문제, 혹은 현실의 문제에서는 이 환경역학에 접근할 수 없다. (시도해 보기 전엔 어떤 영향을 줄 지 알 수 없음.)&lt;/li&gt;
      &lt;li&gt;Dynamic Programming 은 강화학습 알고리즘의 핵심 기초가 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Dynamic Programming" /><summary type="html">관련 자료 (RLbook2018 Pages 73-88)</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 03. Week 3. Value Functions &amp;amp; Bellman Equations</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_03_Week3/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 03. Week 3. Value Functions &amp;amp; Bellman Equations" /><published>2023-03-16T15:00:00+09:00</published><updated>2023-03-16T15:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_03_Week3</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_03_Week3/">&lt;h2 id=&quot;관련-자료-rlbook2018-pages-58-67&quot;&gt;관련 자료 (RLbook2018 Pages 58-67)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;용어설명
    &lt;ul&gt;
      &lt;li&gt;휴리스틱 : 정립된 공식이 아닌 정보가 온전하지 않은 상황에서의 노력, 시행착오, 경험 등을 통해서 지식을 알게 되는 과정.
        &lt;ul&gt;
          &lt;li&gt;주먹구구식의 규칙 (Rule of Thumb) 을 통해 지식을 습득하게 되는 과정&lt;/li&gt;
          &lt;li&gt;잘 추측하는 기술 (art of good guessing)&lt;/li&gt;
          &lt;li&gt;알고리즘과 달리 휴리스틱은 해결책의 발견을 보장하지 않는다.&lt;/li&gt;
          &lt;li&gt;그러나 휴리스틱은 알고리즘보다 효율적이다. (쓸모없는 대안책들을 실제 시도하지 않고도 배제 가능)&lt;/li&gt;
          &lt;li&gt;출처 : &lt;a href=&quot;http://www.aistudy.com/heuristic/heuristic.htm&quot;&gt;http://www.aistudy.com/heuristic/heuristic.htm&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;휴리스틱 서치
        &lt;ul&gt;
          &lt;li&gt;깊이우선 탐색이나 너비우선 탐색 등의 blind search method 는 goal 까지 의 경로를 찾는 상당히 소모적인 (exhaustive) 방법임.&lt;/li&gt;
          &lt;li&gt;즉 문제에 대한 해를 제공하지만 너무 많은 노드를 확장시키므로 실용적이지 못하다.&lt;/li&gt;
          &lt;li&gt;탐색작업을 축소시키기 위해 항상 옳은 해를 제공하지는 못하지만 대부분의 경우에 잘맞는 경험에 의한 규칙 (rules of thumb) 을 이용&lt;/li&gt;
          &lt;li&gt;이렇게 그래프로써 표현된 문제에 대한 특별한 정보를 이용하여 탐색 (Search) 작업을 빠르게 진행시키는 방식&lt;/li&gt;
          &lt;li&gt;출처 : &lt;a href=&quot;http://www.aistudy.co.kr/heuristic/heuristic_search.htm&quot;&gt;http://www.aistudy.co.kr/heuristic/heuristic_search.htm&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policies and Value Functions&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;거의 대부분의 강화학습 알고리즘은 가치함수 (value function) 을 추정하는 것을 포함한다.
        &lt;ul&gt;
          &lt;li&gt;상태 (States) 혹은 상태-액션 쌍(State-action pairs) 에 대한 함수&lt;/li&gt;
          &lt;li&gt;에이전트에게 주어진 상태 (혹은 주어진 상태에서 주어진 액션) 이 얼마나 좋은지 (how good) 추정하는 것&lt;/li&gt;
          &lt;li&gt;얼마나 좋은지 (how good) 란 기대되는 미래 보상값 혹은 기대되는 결과값에 대한 이야기임&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;미래의 보상 값은, 에이전트가 어떤 액션을 취할지에 달려있음
        &lt;ul&gt;
          &lt;li&gt;따라서 가치함수 (value function) 는 특정한 행동 방식 ( = Policy) 에 따라 정의된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;일반적으로 정책 (Policy) 은 환경 (States) 에 따른 선택 가능한 행동 (Action) 을 선택하는 확률의 매핑 정보이다.
        &lt;ul&gt;
          &lt;li&gt;$\pi ( a \mid s )$
            &lt;ul&gt;
              &lt;li&gt;에이전트가 정책(policy) $\pi$ 를 따른다 가정할 경우&lt;/li&gt;
              &lt;li&gt;time t 시점에서 $S_t = s$ 일 때 $A_t = a$ 일 확률을 가리킨다.&lt;/li&gt;
              &lt;li&gt;mdp 함수 $p$ 와 같이 $\pi$ 또한 평범한 함수이다.
                &lt;ul&gt;
                  &lt;li&gt;$\mid$ 는 각 $s \in S$ 에 대한 $a \in A(s)$ 의 확률 분포를 나타낸다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;강화 학습 함수는 에이전트의 경험 결과에 따라 어떻게 정책이 바뀔지를 지정한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$v_\pi (s)$ : policy $\pi$ 를 따를 때 state $s$ 일 때 가치 함수 (value function)
        &lt;ul&gt;
          &lt;li&gt;$\pi$ 를 따를 때 $s$ 상태에서 시작할 경우 기대되는 결과값&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_1_state_value_function.png&quot; alt=&quot;3_5_1_state_value_function&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;$E_\pi [\cdot]$ : 에이전트가 policy $\pi$ 를 따르고, $t$ 는 임의의 time step 일 때 에이전트가 제공하는 무작위 변수에 대한 기대 리턴값&lt;/li&gt;
          &lt;li&gt;Terminal state 에 대한 위의 값은 0&lt;/li&gt;
          &lt;li&gt;우리는 $v_\pi$ 를 정책 $\pi$ 에 대한 state-value function 이라 한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_2_action_value_function.png&quot; alt=&quot;3_5_2_action_value_function&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;위와 유사하게 policy $\pi$ 아래에서 state $s$ 에 action $a$ 를 취할 때의 value 를 측정할 경우 이를 $q_\pi (s,a)$ 로 정의할 수 있다.&lt;/li&gt;
          &lt;li&gt;이 때 $E_\pi [\cdot]$ 는 정책 $\pi$ 를 따를 경우 상태 $s$ 에서 시작하여 행동 $a$ 를 행했을 때 기대되는 리턴값을 뜻한다.&lt;/li&gt;
          &lt;li&gt;우리는 $q_\pi$ 를 정책 $\pi$ 에 대한 action-value function 이라 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;가치함수 $v_\pi$ 와 $q_\pi$ 는 경험을 통해 추정할 수 있다.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 정책 $\pi$ 를 따르는 에이전트가 각각의 상태를 마주하고 이에 따른 리턴 값의 평균을 보관할 경우, 해당 상태에 직면하는 횟수가 무한에 가까워지면 평균 값은 상태의 가치, $v_\pi (s)$ 로 수렴한다.&lt;/li&gt;
          &lt;li&gt;마찬가지로 각 상태에 보관된 평균 값을 행동에 따라 분리한다면 이는 행동의 가치, $q_\pi (s,a)$ 로 수렴한다.&lt;/li&gt;
          &lt;li&gt;우리는 이러한 추정 방식을 Monte Carlo methods 라 한다. (많은 랜덤한 샘플의 실제 리턴값을 평균내는 것을 포함하고 있기 때문)&lt;/li&gt;
          &lt;li&gt;물론 이러한 방식은 상태값이 많으면 각 상태 별로 분리된 평균값을 각각 가지는 것에 어려움이 있다.&lt;/li&gt;
          &lt;li&gt;대신, 에이전트는  $v_\pi$ 와 $q_\pi$ 를 파라미터화된 함수로 유지하고 이 파라미터를 조정하는 방식을 사용한다.
            &lt;ul&gt;
              &lt;li&gt;이 또한 정확한 추정치를 구할 수 있다.&lt;/li&gt;
              &lt;li&gt;이 경우 파라미터화 된 function approximator (함수 근사) 에 달려있다. (이는 이후 과정에서 설명)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;가치함수의 핵심요소들은 강화학습 이나 다이나믹 프로그래밍 전반에 걸쳐 사용되며, 재귀적인 표현법으로 표현될 수 있다.
        &lt;ul&gt;
          &lt;li&gt;다음 조건은 $s$ 값과 그 값의 가능한 후속 states 사이에서 일관성있게 유지된다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_3_recursive_state_value_function.png&quot; alt=&quot;3_5_3_recursive_state_value_function&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 재귀식은 아래의 조건을 따른다.
            &lt;ul&gt;
              &lt;li&gt;$a \in A(s)$&lt;/li&gt;
              &lt;li&gt;$s’ \in S$ (Episodic problem 일 경우 $S^+$)&lt;/li&gt;
              &lt;li&gt;$r \in R$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위 식은 실제로 모든 변수 $a, s’, r$ 에 대한 합계이다.&lt;/li&gt;
          &lt;li&gt;예상값을 구하기 위해 $\pi(a|s)p(s’,r|s,a)$ 확률을 계산하여 [ ] 안의 값에 가중치를 부여하고 이 모든 확률에 대한 합계를 구한다.&lt;/li&gt;
          &lt;li&gt;위의 3.14 방정식은 Bellman equation for $v_\pi$ 이다.
            &lt;ul&gt;
              &lt;li&gt;이것은 상태의 가치, 그리고 그 상태의 후속 상태의 가치에 대한 관계를 나타낸다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_4_backup_diagram_for_v_pi.png&quot; alt=&quot;3_5_4_backup_diagram_for_v_pi&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;에이전트는 state $s$ 에서 policy $\pi$ 에 기반한 행동들 ($a \in A(s)$) 을 할 수 있다.&lt;/li&gt;
          &lt;li&gt;환경은 함수 $p$ 에 따라 주어진 역학 (dynamic) 을 통해 보상 $r$ 과 함께 여러 다음 상태 중 하나인 $s’$ 로 응답할 수 있음.&lt;/li&gt;
          &lt;li&gt;벨만 방정식 (The Bellman equation (3.14.)) 은 모든 가능성에 대해 평균을 내며, 발생 확률에 따라 가중치를 부여함.&lt;/li&gt;
          &lt;li&gt;시작 상태의 값은 다음 상태의 (할인된) 값과 보상값의 합과 반드시 같다.&lt;/li&gt;
          &lt;li&gt;위의 다이어그램을 backup diagrams 라 한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_4_1_backup_diagram_v_pi_q_pi.png&quot; alt=&quot;3_5_4_1_backup_diagram_v_pi_q_pi&quot; /&gt;
  &lt;img src=&quot;/assets/images/posts/3_5_4_2_backup_diagram_q_pi_v_pi.png&quot; alt=&quot;3_5_4_2_backup_diagram_q_pi_v_pi&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 다이어그램은 $v_\pi (s)$ 와 $q_\pi (s)$ 의 관계를 나타낸다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Example 3.5 : Gridworld&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_5_Gridworld_example.png&quot; alt=&quot;3_5_5_Gridworld_example&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위 gridworld 는 단순한 유한 MDP 를 나타낸다.&lt;/li&gt;
          &lt;li&gt;그리드의 각 셀은 환경의 state 를 나타낸다.&lt;/li&gt;
          &lt;li&gt;각 셀에서 동,서,남,북 의 action 이 가능하며, 해당 방향의 셀로 이동하게 된다.&lt;/li&gt;
          &lt;li&gt;가장자리에서의 이동은 이동 없이 해당 셀에 머물게 되나 보상으로 -1 을 얻게 된다.&lt;/li&gt;
          &lt;li&gt;특별한 상태인 A, B 를 제외하고 다른 곳에서의 이동은 보상으로 0 을 얻는다.&lt;/li&gt;
          &lt;li&gt;A 에서의 이동은 action 의 방향과 관계없이 A’ 로 이동하게 되며 +10의 보상을 얻는다.&lt;/li&gt;
          &lt;li&gt;B 에서의 이동은 action 의 방향과 관계없이 B’ 로 이동하게 되며 +5의 보상을 얻는다.&lt;/li&gt;
          &lt;li&gt;모든 상태에서 동일한 확률로 랜덤한 action 을 취하는 정책을 취할 때 우측의 그림은 그에따른 가치함수 (discounted reward $\gamma=0.9$)를 나타낸다.&lt;/li&gt;
          &lt;li&gt;위 가치함수는 (3.14) 의 선형방정식 (linear equations) 을 푼 결과이다.
            &lt;ul&gt;
              &lt;li&gt;가장자리는 값 이 낮은데, 이는 가장자리에 부딪힐 확률이 높기 때문이다.&lt;/li&gt;
              &lt;li&gt;State A 는 보상 값 +10 보다 낮은 가치를 가지는데 전이된 A’ 에서 감점될 확률이 높기 때문이다.&lt;/li&gt;
              &lt;li&gt;B 는 B’ 로 전이되었을 때 A’ 보다 감점될 확률이 적다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Example 3.6 : Golf&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_6_Golf_example.png&quot; alt=&quot;3_5_6_Golf_example&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;그림 중 위의 부분에 대한 설명임.&lt;/li&gt;
          &lt;li&gt;공을 홀에 넣을 때까지 각 스트로크에 대해 -1 의 페널티 (마이너스보상).&lt;/li&gt;
          &lt;li&gt;공의 위치가 상태이며, 상태 값은 홀로부터 상태 위치 까지의 스트로크 횟수 (음수) 값이다.&lt;/li&gt;
          &lt;li&gt;행동은 공을 조준, 스윙하는 방법과 어떤 클럽을 선택하는가 등이 있지만 전자는 주어진 것으로 받아들이고 후자의 선택에만 집중한다.&lt;/li&gt;
          &lt;li&gt;홀의 값은 0&lt;/li&gt;
          &lt;li&gt;그린의 어느 곳에서든 퍼팅을 할 수 있다고 가정하며, 이러한 상태의 값은 -1&lt;/li&gt;
          &lt;li&gt;그린 밖에서는 퍼팅으로 홀에 도달할 수 없으며, 퍼팅을 통해 그린에 도달할 수 있다면 -2&lt;/li&gt;
          &lt;li&gt;마찬가지로 -2 등고선으로 퍼팅이 가능한 모든 위치는 -3 값을 가지며, 위와 같은 방식으로 등고선이 그려짐&lt;/li&gt;
          &lt;li&gt;퍼팅으로 모래 함정에서 벗어날 수 없으므로 $-\infty$ 의 값을 가지게 됨&lt;/li&gt;
          &lt;li&gt;전반적으로 퍼팅으로 티에서 홀 까지 가는데 6타가 걸림.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimal Policies and Optimal Value Functions&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;강화학습 문제를 푼다는 것은, 간단히 말하면 긴 흐름 속에서 많은 보상을 획득하는 정책을 찾는 것이다.&lt;/li&gt;
      &lt;li&gt;유한 MDP 문제에서 우리는 Optimal policy 를 아래와 같이 정의할 수 있다.
        &lt;ul&gt;
          &lt;li&gt;가치함수는 전체 정책에서 부분적인 순서를 결정한다.&lt;/li&gt;
          &lt;li&gt;정책 $\pi$ 가 정책 $\pi’$ 와 비교하여 모든 상태에서 예상되는 리턴 값이 크거나 같을 경우 정책 $\pi$ 는 정책 $\pi’$ 보다 좋거나 동등하다고 본다.&lt;/li&gt;
          &lt;li&gt;$\pi \ge \pi’$, if and only if $v_\pi (s) \ge v_\pi’ (s)$ for all $s \in S$.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;항상 최소한 하나의, 타 정책보다 나은 정책이 있는데 이를 최적 정책 (optimal policy) 라 한다.
        &lt;ul&gt;
          &lt;li&gt;우리는 optimal policies 를 $\pi_*$ 로 표기한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_6_1_optimal_state_value_function.png&quot; alt=&quot;3_6_1_optimal_state_value_function&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;이는 optimal state-value function $v_*$ (for all $s \in S$) 를 공유한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_6_2_optimal_action_value_function.png&quot; alt=&quot;3_6_2_optimal_action_value_function&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;이는 또한 optimal action-value function $q_*$ (for all $s \in S$ and $a \in A(s)$) 를 공유한다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_6_3_optimal_action_state_value_function_relation.png&quot; alt=&quot;3_6_3_optimal_action_state_value_function_relation&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위 두 함수의 정의에 따라 위와같이 표현할 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Example 3.7 : Optimal Value Functions for Golf&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_5_6_Golf_example.png&quot; alt=&quot;3_5_6_Golf_example&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;그림 중 아래 부분에 대한 설명임. (optimal action-value function $q_*(s,driver)$)&lt;/li&gt;
          &lt;li&gt;드라이버로 먼저 스트로크를 한 다음 나중에 드라이버나 퍼터 중 더 나은 쪽을 선택하는 경우 각 상태의 값&lt;/li&gt;
          &lt;li&gt;드라이버를 사용하면 공을 더 멀리 칠 수 있지만 정확도는 떨어져, 홀에서 가까운 부분만 -1 이 됨.&lt;/li&gt;
          &lt;li&gt;스트로크가 두번 있는 경우 -2 등고선에서 볼 수 있듯 더 먼 곳에서 홀까지 도달할 수 있음.&lt;/li&gt;
          &lt;li&gt;즉, 그린에만 떨어지면 퍼터를 사용할 수 있음.&lt;/li&gt;
          &lt;li&gt;Optimal action-value function 은 처음 특정 행동을 한 이후에 값을 제공 (위의 경우 우선 드라이버를 사용하고 그 뒤에 무엇을 사용할지를 결정)&lt;/li&gt;
          &lt;li&gt;-3 등고선은 더 멀리 있으며 시작 티를 포함함. 티에서 가장 좋은 순서는 드라이버2개, 퍼트1개로 공을 홀에 넣는 것임.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;벨만 최적 방정식
        &lt;ul&gt;
          &lt;li&gt;$v_*$ 는 정책에 대한 가치함수이기 때문에 벨만 방정식(3.14) 에 의해 주어진 상태 값에 대한 자기 일관성 조건을 충족해야 한다.&lt;/li&gt;
          &lt;li&gt;하지만 최적 가치 함수이기 때문에 $v_*$ 의 일관성 조건은 특정 정책을 참조하지 않고 특별한 형태로 작성할 수 있음.&lt;/li&gt;
          &lt;li&gt;이것은 $v_*$ 에 대한 벨만 방정식 또는 벨만 최적 방정식 (Bellman Optimality equation) 이라 함.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;직관적으로 Bellman 최적 방정식은 최적 정책 하의 상태 값이 해당 상태에서 최상의 조치에 대한 기대 수익과 같아야 한다는 사실을 나타냄.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;마지막 두 방정식은 벨만 최적 방정식 ($v_*$) 의 두가지 형태를 나타낸다.	
  &lt;img src=&quot;/assets/images/posts/3_6_4_bellman_optimality_equation.png&quot; alt=&quot;3_6_4_bellman_optimality_equation&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;벨만 최적방정식 ($q_*$) 는 아래와 같이 표현할 수 있다.
  &lt;img src=&quot;/assets/images/posts/3_6_5_bellman_optimality_equation.png&quot; alt=&quot;3_6_5_bellman_optimality_equation&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;아래는 벨만 최적방정식에 대한 backup diagrams 이다.
  &lt;img src=&quot;/assets/images/posts/3_6_6_backup_diagrams_bellman_optimality_equation.png&quot; alt=&quot;3_6_6_backup_diagrams_bellman_optimality_equation&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;이는 $v_\pi$ 와 $q_\pi$ 의 backup diagrams 와 같으나, 호 (arc) 가 추가되었음.
            &lt;ul&gt;
              &lt;li&gt;에이전트 선택 지점에 호가 추가되어 타 정책에 의해 주어진 예상 값이 아닌 해당 선택에 대한 최대값이 취해짐을 나타냄.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;왼쪽 backup diagram 은 3.19의 식이, 오른쪽은 3.20의 식이 표현된 것임.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;유한 MDP 의 경우 $v_*$ (3.19) 에 대한 고유한 솔루션이 존재한다.&lt;/li&gt;
          &lt;li&gt;벨만 최적방정식은 각각의 state 별로 존재하는 방정식이다.
            &lt;ul&gt;
              &lt;li&gt;따라서 n개의 state 가 있을 경우 n 개의 방정식과 n 개의 미지수가 존재하게 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;만약 환경의 역학 $p$ 를 알면 비선형 방정식 시스템을 풀기 위한 다양한 방법 중 하나를 선택하여 $v_*$ 에 대한 방정식을 풀 수 있다.&lt;/li&gt;
          &lt;li&gt;$v_*$ 를 알게 되면, 최적 정책을 결정하기가 상대적으로 쉽다.
            &lt;ul&gt;
              &lt;li&gt;모든 상태 s 에 대해 벨만 최적 방정식의 최대값을 얻는 하나 또는 그 이상의 action 이 존재하고, 이런 action 에만 0 이 아닌 확률을 배분하는 정책이 최적 정책이다.&lt;/li&gt;
              &lt;li&gt;즉 최적 가치 함수 $v_*$ 를 알면, one-step-ahead search 를 통해 어떤 action 이 최적의 action 인지 알게 된다.&lt;/li&gt;
              &lt;li&gt;다른 용어로 (optimal evaluation function $v_*$) 의 측면에서 greedy policy 라 할 수 있음.
                &lt;ul&gt;
                  &lt;li&gt;greedy 함은 지역적 혹은 즉각적 고려를 통한 결정이며, 보다 나은 미래의 대안 가능성을 고려하지 않음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;$v_*$ 는 이미 미래 행동의 모든 가능성을 고려하고 반영한 결과값이므로, 긴 기간동안의 리턴값을 지역적, 즉각적인 수량으로 나타낸 값이 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$q_*$ 를 알면, 최적의 행동을 선택하기 더 쉬워진다.
            &lt;ul&gt;
              &lt;li&gt;에이전트는 one-step-ahead search 를 할 필요 없이 s 상태에서 $q_*(s,a)$ 를 최대화 할 행동 a 를 선택하면 된다.&lt;/li&gt;
              &lt;li&gt;$q_*$ 는 장기적인 기대 리턴값을 각각의 state-action pair 에 제한한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;action-value function 은 더 효율적으로 모든 one-step-ahead search 의 결과값을 캐싱한다.
            &lt;ul&gt;
              &lt;li&gt;가능한 후속 상태 및 해당 값들에 대해 알 필요 없이 (즉, 환경의 역학에 대해 알 필요 없이) 최적 행동을 선택할 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Example 3.8: Solving the Gridworld&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_6_7_solving_the_gridworld.png&quot; alt=&quot;3_6_7_solving_the_gridworld&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Example 3.5 의 Gridworld 문제에 대한 벨만 방정식 (Bellman equation for $v_\pi$) 를 풀었다고 가정&lt;/li&gt;
          &lt;li&gt;중간 이미지는 optimal value function $v_*$ 를 나타낸다.&lt;/li&gt;
          &lt;li&gt;우측 이미지는 그에 상응하는 최적 정책 (optimal policies) 이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Example 3.9: Bellman Optimality Equations for the Recycling Robot&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_6_8_bellman_optimality_equations_for_the_recycling_robot.png&quot; alt=&quot;3_6_8_bellman_optimality_equations_for_the_recycling_robot&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;3.19 의 수식을 이용, 벨만 최적 방정식을 Recycling robot example 에 적용할 수 있다.&lt;/li&gt;
          &lt;li&gt;states : high, row / actions : search, wait, recharge 를 각각 h, l, s, w, re 로 축약한다.&lt;/li&gt;
          &lt;li&gt;상태가 2개이므로 벨만 최적 방정식은 2개의 방정식으로 구성된다.&lt;/li&gt;
          &lt;li&gt;$v_* (h)$ 에 대한 방정식은 위와 같이 표기할 수 있다.&lt;/li&gt;
          &lt;li&gt;$r_s, r_w, \alpha, \beta, \gamma$ ($0 \le \gamma &amp;lt; 1, 0 \le \alpha,\beta \le 1$) 의 어떠한 케이스를 선택해도 정확히 한 쌍의 숫자, $v_* (h), v_* (l)$ 이 남는다. 이는 동시에 두 비선형 방정식을 만족함을 보여준다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bellman 최적 방정식의 한계
        &lt;ul&gt;
          &lt;li&gt;Bellman 최적 방정식의 해결은 최적의 정책을 찾고, 강화 학습 문제를 해결할 수 있음을 보여줌.&lt;/li&gt;
          &lt;li&gt;그러나 이 솔루션은 실제로 유용하지 않음.
            &lt;ul&gt;
              &lt;li&gt;모든 가능성을 내다보고 예상되는 보상 측면에서 발생 가능성을 계산하는 검색과 유사함.&lt;/li&gt;
              &lt;li&gt;실제로 거의 적용되지 않는 최소한 세 가지 가정에 의존함
                &lt;ol&gt;
                  &lt;li&gt;환경의 역학을 정확하게 알고 있음.&lt;/li&gt;
                  &lt;li&gt;솔루션 계산을 완료하기에 충분한 계산 리소스가 있음&lt;/li&gt;
                  &lt;li&gt;Markov 속성&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
              &lt;li&gt;예를 들어 첫 번째와 세 번째 가정은 백개먼 게임에 아무런 문제가 없지만 두 번째 가정은 큰 장애물이 됨.&lt;/li&gt;
              &lt;li&gt;게임에는 약 $10^{20}$ 개의 상태가 있기 때문에&lt;/li&gt;
              &lt;li&gt;오늘날의 가장 빠른 컴퓨터에서 $v_*$ 에 대한 Bellman 방정식을 푸는 데 수천 년이 걸리며&lt;/li&gt;
              &lt;li&gt;$q_*$ 를 찾는 것도 마찬가지이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화 학습에서는 일반적으로 근사 솔루션 (approximate solutions) 으로 만족해야 함.
        &lt;ul&gt;
          &lt;li&gt;Bellman 최적 방정식을 근사로 푸는 방법으로 다양한 의사 결정 방법이 있음.
            &lt;ul&gt;
              &lt;li&gt;예를 들어 휴리스틱 검색 방법은 (3.19) 의 오른쪽을 여러 번 확장하여 어느 정도 깊이까지 형성함&lt;/li&gt;
              &lt;li&gt;이를 통해 가능성에 대한 트리구조를 만들고, 휴리스틱 평가 함수를 사용하여 리프 노드에서 $v_*$ 를 근사함.
                &lt;ul&gt;
                  &lt;li&gt;$A^*$ 와 같은 휴리스틱 검색방법은 거의 항상 에피소드 사례를 기반으로 함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;동적 계획법 (dynamic programming) 이 Bellman 최적 방정식과 훨씬 더 밀접한 관계가 있음.&lt;/li&gt;
          &lt;li&gt;많은 강화 학습 방법은 transition 에 대한 지식 대신 transition 에 대한 경험을 토대로 근사치 해결을 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimality and Approximation
    &lt;ul&gt;
      &lt;li&gt;최적 가치 함수와 최적 정책에 대해 정의를 했고, 실제로 이를 학습한 에이전트는 수행을 잘 하였으나 이러한 경우는 매우 드물다.
        &lt;ul&gt;
          &lt;li&gt;이러한 종류의 작업은 엄청난 계산비용으로만 생성할 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;환경의 역학에 대해 완전하고 정확한 모델이 있더라도 벨만 최적 방정식을 풀어 최적의 정책을 계산하는 것은 일반적으로 불가능하다.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 체스와 같은 보드게임도 여전히 최적의 수를 계산할 수 없음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;에이전트가 직면하는 문제
        &lt;ul&gt;
          &lt;li&gt;단일 time-step 에서 수행할 수 있는 계산의 양&lt;/li&gt;
          &lt;li&gt;사용 가능한 메모리 양 (가치함수, 정책, 모델 근사치의 구축 등에 필요)&lt;/li&gt;
          &lt;li&gt;작고 유한한 상태집합이 있는 작업에서는 각 상태 혹은 상태-행동 쌍 에 대해 배열 또는 테이블을 사용하여 근사치를 형성할 수 있음
            &lt;ul&gt;
              &lt;li&gt;우리는 이런 경우를 tabular case, tabluar method (테이블 형식) 이라 하나 실질적인 문제들은 테이블 항목으로 표현할 수 없을 정도로 훨씬 더 많은 상태를 가지고 있음&lt;/li&gt;
              &lt;li&gt;이러한 경우 더 간결한 형태의 매개 변수화된 함수 표현을 사용하여 근사화 해야 함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;근사화
        &lt;ul&gt;
          &lt;li&gt;강화학습의 온라인 특성
            &lt;ul&gt;
              &lt;li&gt;자주 경험하는 상태에 대해 좋은 결정을 내릴 수 있도록 더 많은 노력을 기울임&lt;/li&gt;
              &lt;li&gt;드물게 경험하는 상태는 상대적으로 더 적은 노력을 기울임
                &lt;ul&gt;
                  &lt;li&gt;최선의 선택이 아닌 선택을 할 확률이 매우 낮은 보상에 미미한 영향을 주는 상태들&lt;/li&gt;
                  &lt;li&gt;예를 들어 백개먼 게임에서 실제로 거의 일어나지 않는 상황에서 악수를 둘 수 있지만, 전문가와 뛰어난 기술로 플레이 할 수 있음&lt;/li&gt;
                  &lt;li&gt;수 많은 다양한 환경에서 실제로 잘못된 결정을 내릴 확률이 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;이는 MDP 를 근사화 하여 해결하는 다른 접근방식과 강화학습을 구별하는 핵심 속성중 하나임&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;policies-and-value-functions&quot;&gt;Policies and Value Functions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Specifying Policies&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;정책 (Policy) 은 각 가능한 상태 (State)에 대한 행동 (Action) 의 분포임을 인지한다.&lt;/li&gt;
          &lt;li&gt;확률론적 정책 (Stochastic Policies) 과 결정론적 정책 (Deterministic Policies) 의 유사점과 차이점을 설명한다.&lt;/li&gt;
          &lt;li&gt;잘 정의된 정책의 특성을 식별한다.&lt;/li&gt;
          &lt;li&gt;주어진 MDP 에 유효한 정책의 예시&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Deterministic policy (결정론적 정책)
        &lt;ul&gt;
          &lt;li&gt;각 상태에 하나의 행동을 매핑&lt;/li&gt;
          &lt;li&gt;$\pi(s) = a$&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;테이블로 표현 가능&lt;/p&gt;

            &lt;table&gt;
              &lt;thead&gt;
                &lt;tr&gt;
                  &lt;th style=&quot;text-align: left&quot;&gt;State&lt;/th&gt;
                  &lt;th style=&quot;text-align: left&quot;&gt;Action&lt;/th&gt;
                &lt;/tr&gt;
              &lt;/thead&gt;
              &lt;tbody&gt;
                &lt;tr&gt;
                  &lt;td style=&quot;text-align: left&quot;&gt;$s_0$&lt;/td&gt;
                  &lt;td style=&quot;text-align: left&quot;&gt;$a_1$&lt;/td&gt;
                &lt;/tr&gt;
                &lt;tr&gt;
                  &lt;td style=&quot;text-align: left&quot;&gt;$s_1$&lt;/td&gt;
                  &lt;td style=&quot;text-align: left&quot;&gt;$a_0$&lt;/td&gt;
                &lt;/tr&gt;
                &lt;tr&gt;
                  &lt;td style=&quot;text-align: left&quot;&gt;$s_2$&lt;/td&gt;
                  &lt;td style=&quot;text-align: left&quot;&gt;$a_0$&lt;/td&gt;
                &lt;/tr&gt;
              &lt;/tbody&gt;
            &lt;/table&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;예시&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/example_deterministic_policy.png&quot; alt=&quot;example_deterministic_policy&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Stochastic policy (확률론적 정책)
        &lt;ul&gt;
          &lt;li&gt;각 상태에서 행동이 가지는 확률을 표현&lt;/li&gt;
          &lt;li&gt;하나의 상태에서도 각각 다른 행동이 선택될 수 있음 (확률이 0이 아닐 경우)&lt;/li&gt;
          &lt;li&gt;$\pi(a|s)$&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;확률 그래프로 표현 가능&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/example_stochastic_policy.png&quot; alt=&quot;example_stochastic_policy&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;예시&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/example_stochastic_policy_2.png&quot; alt=&quot;example_stochastic_policy_2&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;정책은 오직 현재 상태에만 영향을 받는 것이 중요함.
        &lt;ul&gt;
          &lt;li&gt;시간이나 이전 상태와 같은 요소에 영향을 받지 않아야 함.
            &lt;ul&gt;
              &lt;li&gt;50% : 50% 의 행동 확률이라고 번갈아 가며 행동하지 않음 (이것은 현 상태 외의 영향을 받은 정책임.)&lt;/li&gt;
              &lt;li&gt;이런 면을 상태의 요구사항이며, 에이전트의 제한은 아닌 것으로 여기는 편이 좋음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;현재 상태에 현재 행동을 결정할 모든 요소가 포함되어 있어야 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 에서는 상태가 결정을 위한 모든 정보를 포함한 것으로 가정한다.
        &lt;ul&gt;
          &lt;li&gt;만약 번갈아 가며 하는 행동이 높은 보상값을 제공한다면, 상태값에 마지막 행동값이 포함되어야 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Value Functions&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;학습목표&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;강화학습에서 상태가치함수 (state-value functions) 와 행동가치함수 (action-value functions) 의 역할에 대한 설명&lt;/li&gt;
          &lt;li&gt;가치 함수 (value function) 와 정책 (policy) 간의 관계 설명&lt;/li&gt;
          &lt;li&gt;주어진 MDP 에 대해 유효한 가치함수 생성&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;개념 설명
        &lt;ul&gt;
          &lt;li&gt;가치함수는 지연된 보상을 나타낸다.&lt;/li&gt;
          &lt;li&gt;강화학습에서는 장기적으로 최대의 보상을 얻는 정책을 학습하는 것을 목표로 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;State-value functions&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$&lt;/li&gt;
          &lt;li&gt;$v(s) \doteq E [G_t|S_t=s]$&lt;/li&gt;
          &lt;li&gt;주어진 환경에 대해 기대되는 보상 값을 의미&lt;/li&gt;
          &lt;li&gt;이 의미에 의해 value function 은 주어진 policy (agent 가 어떤 action을 취할 것인지) 에 영향을 받는다는 것을 의미&lt;/li&gt;
          &lt;li&gt;$v_\pi (s) \doteq E_\pi [G_t|S_t=s]$&lt;/li&gt;
          &lt;li&gt;주어진 정책 하에 현 상태에 기대되는 리턴값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Action-value functions&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$q_\pi (s,a) \doteq E_\pi [G_t|S_t=s,A_t=a]$&lt;/li&gt;
          &lt;li&gt;s에서 a를 선택한 후 정책을 따랐을 시 기대되는 리턴값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Value function 의 의미
        &lt;ul&gt;
          &lt;li&gt;장기적인 결과를 관찰하기 위해 기다리는 대신&lt;/li&gt;
          &lt;li&gt;현재 상황의 품질을 질의할 수 있음&lt;/li&gt;
          &lt;li&gt;이 리턴 값은 즉시 사용할 수 없음&lt;/li&gt;
          &lt;li&gt;정책 및 환경 역학의 확률로 인해 리턴 값이 무작위일 수 있음&lt;/li&gt;
          &lt;li&gt;Value function 은 미래의 모든 기대되는 리턴값을 평균값으로 요약함&lt;/li&gt;
          &lt;li&gt;이를 토대로 다른 정책들의 질을 판단할 수 있게 됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Value function 의 예시 : Chess
        &lt;ul&gt;
          &lt;li&gt;체스는 episodic MDP 이다.&lt;/li&gt;
          &lt;li&gt;State : 모든 말의 현 위치&lt;/li&gt;
          &lt;li&gt;Action : 합법적인 이동&lt;/li&gt;
          &lt;li&gt;Reward : 게임의 승리(+1), 패배 또는 무승부(0)&lt;/li&gt;
          &lt;li&gt;위의 보상으로 경기 중 에이전트가 얼마나 잘 플레이하는지에 대해 알 수 없음.&lt;/li&gt;
          &lt;li&gt;또한 보상을 보려면 에피소드가 끝날 때 까지 기다려야 함.&lt;/li&gt;
          &lt;li&gt;이 때 가치함수는 훨씬 더 많은 것을 알려줄 수 있음.
            &lt;ul&gt;
              &lt;li&gt;상태 가치함수 값은 단순히 현 Policy 를 따랐을 경우 이길 확률을 말함.&lt;/li&gt;
              &lt;li&gt;상대방의 움직임은 상태 전이이다.&lt;/li&gt;
              &lt;li&gt;action value function 은 policy 를 따랐을 경우 현 동작을 통해 이길 확률을 나타낸다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rich Sutton and Andy Barto : A brief History of RL&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bellman-equations&quot;&gt;Bellman Equations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bellman Equation Derivation&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;상태가치함수 (state-value function) 에 대한 Bellman 방정식 유도&lt;/li&gt;
          &lt;li&gt;행동가치함수 (action-value function) 에 대한 Bellman 방정식 유도&lt;/li&gt;
          &lt;li&gt;Bellman 방정식이 현재와 미래 가치를 연관시키는 방법을 이해&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;State-value Bellman equation
        &lt;ul&gt;
          &lt;li&gt;$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$&lt;/li&gt;
          &lt;li&gt;$v_\pi (s) \doteq E_\pi [G_t|S_t=s]$&lt;/li&gt;
          &lt;li&gt;$=E_\pi [R_{t+1} + \gamma G_{t+1}|S_t=s]$&lt;/li&gt;
          &lt;li&gt;$=\sum_a \pi(a|s) \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma E_\pi [G_{t+1}|S_{t+1}=s’]]$&lt;/li&gt;
          &lt;li&gt;$=\sum_a \pi(a|s) \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma v_\pi (s’)]$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Action-value Bellman equation
        &lt;ul&gt;
          &lt;li&gt;$q_\pi (s,a) \doteq E_\pi [G_t|S_t=s,A_t=a]$&lt;/li&gt;
          &lt;li&gt;$=\sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma E_\pi [G_{t+1}|S_{t+1}=s’]]$&lt;/li&gt;
          &lt;li&gt;$=\sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma \sum_{a’} \pi(a’|s’)E_\pi[G_{t+1}|S_{t+1}=s’,A_{t+1}=a’]]$&lt;/li&gt;
          &lt;li&gt;$=\sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma \sum_{a’} \pi(a’|s’)q_\pi (s’,a’)]$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;현 state value 혹은 state/action value 는 미래의 state value 혹은 state/action value 표현법으로 재귀적 표현이 가능하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why Bellman Equations?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;Bellman 방정식을 이용해 가치함수 (value functions) 를 계산&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;예제 : Gridworld&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/example_gridworld_bellman_equations_1.png&quot; alt=&quot;example_gridworld_bellman_equations_1&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/example_gridworld_bellman_equations_2.png&quot; alt=&quot;example_gridworld_bellman_equations_2&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/example_gridworld_bellman_equations_3.png&quot; alt=&quot;example_gridworld_bellman_equations_3&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/example_gridworld_bellman_equations_4.png&quot; alt=&quot;example_gridworld_bellman_equations_4&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;벨만 방정식은 가능한 모든 미래의 값을 무한히 더해가는 과정을 선형대수 문제로 치환시켜 준다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;벨만 방정식의 한계
        &lt;ul&gt;
          &lt;li&gt;체스게임과 같이 가능한 상태의 양이 많은 경우&lt;/li&gt;
          &lt;li&gt;위의 예시의 경우 상태가 4개이기 때문에 4개의 선형 방정식을 풀면 되지만..&lt;/li&gt;
          &lt;li&gt;체스 게임의 경우 $10^{45}$ 개의 선형 방정식을 풀어야 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimality-optimal-policies--value-functions&quot;&gt;Optimality (Optimal Policies &amp;amp; Value Functions)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Optimal Policies&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;개요
        &lt;ul&gt;
          &lt;li&gt;정책 : 에이전트가 어떻게 행동할지를 나타내는 것&lt;/li&gt;
          &lt;li&gt;정책이 결정된 후 value function 을 찾아볼 수 있다.&lt;/li&gt;
          &lt;li&gt;강화학습의 목표는 특정 정책을 평가하는 것이 아닌 최적의 정책을 찾는 것이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;Optimal policy 에 대한 정의&lt;/li&gt;
          &lt;li&gt;특정 정책이 어떻게 모든 상태에서 다른 모든 정책만큼 좋을 수 있는 것인지를 이해&lt;/li&gt;
          &lt;li&gt;주어진 MDP 에 대한 최적의 정책 식별&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Optimal Policy 란?&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/optimal_policy.png&quot; alt=&quot;optimal_policy&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;어떠한 상태에서도 타 정책과 같거나 더 좋은 경우&lt;/li&gt;
          &lt;li&gt;최소한 하나 이상의 Optimal Policy 가 존재
            &lt;ul&gt;
              &lt;li&gt;특정 상황에 $\pi_2$ 가 $\pi_1$ 보다 결과가 좋을 경우&lt;/li&gt;
              &lt;li&gt;해당 상황에서는 $\pi_2$ 정책을 사용하고 그 외의 경우 $\pi_1$ 을 사용하는 정책 $\pi_3$ 를 사용&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;작은 MDP 의 경우 직접적으로 풀 수 있지만…
            &lt;ul&gt;
              &lt;li&gt;2개의 결정론적 정책이 있을 경우 Brute-Force Search 로 문제 해결&lt;/li&gt;
              &lt;li&gt;하지만 일반적인 MDP 의 경우 $|A|^{|S|}$ 개의 결정론적 정책이 존재하여 Brute-Force Search 로 문제 해결이 불가함.&lt;/li&gt;
              &lt;li&gt;위의 경우 Bellman Optimality Equations 로 문제에 접근해야 함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimal Value Functions&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;상태가치함수 (state-value functions) 에 대한 Bellman 최적 방정식 유도&lt;/li&gt;
          &lt;li&gt;행동가치함수 (action-value functions) 에 대한 Bellman 최적 방정식 유도&lt;/li&gt;
          &lt;li&gt;Bellman 최적 방정식이 이전에 소개된 Bellman 방정식과 어떻게 관련되는지 이해&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Optimal Value Functions
        &lt;ul&gt;
          &lt;li&gt;$v_*$ : $v_{\pi_*} (s) \doteq E_{\pi_*} [G_t|S_t=s] = \underset{\pi}{\max} v_\pi (s)$ for all $s \in S$&lt;/li&gt;
          &lt;li&gt;$v_* (s) = \sum_a \pi_* (a|s) \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma v_*(s’)]$&lt;/li&gt;
          &lt;li&gt;$v_* (s) = \underset{a}{\max} \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma v_*(s’)]$
            &lt;ul&gt;
              &lt;li&gt;언제나 하나 이상의 결정론적인 최적 정책이 존재한다.&lt;/li&gt;
              &lt;li&gt;모든 상태에서 하나의 최적 행동을 선택한다.&lt;/li&gt;
              &lt;li&gt;즉, 가장 높은 리턴값을 가지는 하나의 행동의 확률이 1이고, 나머지 행동의 확률은 0이 된다.&lt;/li&gt;
              &lt;li&gt;Bellman Optimality Equation for $v_*$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$q_*$ : $q_{\pi_*} (s,a) = \underset{\pi}{\max} q_\pi (s,a)$ for all $s \in S$ and $a \in A$&lt;/li&gt;
          &lt;li&gt;$q_* (s,a) = \sum_{s’} \sum_r p(s’,r|s,a) [r+ \gamma \sum_{a’} \pi_{*} (a’|s’) q_* (s’,a’)]$&lt;/li&gt;
          &lt;li&gt;$q_* (s,a) = \sum_{s’} \sum_r p(s’,r|s,a) [r+ \gamma \underset{a’}{\max} q_{*} (s’,a’)]$
            &lt;ul&gt;
              &lt;li&gt;Bellman Optimality Equation for $q_*$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/bellman_optimality_equation_for_qastar_linear_system_solver.png&quot; alt=&quot;bellman_optimality_equation_for_qastar_linear_system_solver&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;위의 벨만 최적 방정식으로는 $v_{*}$ 를 풀어낼 수가 없는데, $\max$ 함수가 선형이 아니기 때문이다.&lt;/li&gt;
          &lt;li&gt;$\pi_{*}$ 값을 이용해 같은 방식으로 $v_{*}$ 를 구할 수도 없는데, $\pi_{*}$ 값을 모를 뿐더러, $\pi_{*}$ 를 구하는 것 자체가 목적이기 때문이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Optimal Value Functions to Get Optimal Policies&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;학습목표
        &lt;ul&gt;
          &lt;li&gt;최적가치함수 (Optimal value function) 과 최적정책 (Optimal Policy) 의 연관성 이해&lt;/li&gt;
          &lt;li&gt;주어진 MDP 에 대한 최적가치함수 (Optimal value function) 확인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Optimal Policy 와 Optimal Value Function 의 관계
  &lt;img src=&quot;/assets/images/posts/optimal_policy_and_optimal_value_function_1.png&quot; alt=&quot;optimal_policy_and_optimal_value_function_1&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;$p$ 와 $v_*$ 에 접근할 수 있다고 가정&lt;/li&gt;
          &lt;li&gt;한 단계 진행 시의 값을 구할 수 있을 경우, $A_2$ 가 최대의 값을 가짐을 알 수 있다.&lt;/li&gt;
          &lt;li&gt;$\max$ 는 최대의 값을, $\arg\max$ 는 박스가 최대의 값을 가지게 하는 $a$ 값 자체를 나타낸다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/optimal_policy_and_optimal_value_function_2.png&quot; alt=&quot;optimal_policy_and_optimal_value_function_2&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;$p$ 와 $v_*$ 에 접근할 수 있을 때 계산법&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/optimal_policy_and_optimal_value_function_3.png&quot; alt=&quot;optimal_policy_and_optimal_value_function_3&quot; /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;$p$ 는 확률적 요소여서 알기 힘들지만, 충분히 많이 접근하면 위의 수식에 따라 최적 정책을 구할 수 있게 된다.&lt;/li&gt;
          &lt;li&gt;$q_*$ 를 알 경우 최적 정책을 구하기 훨씬 쉬워지는데, 다음 스텝의 계산을 할 필요가 없기 때문이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Week 3 Summary&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Chapter Summary (RLbook2018 Pages 68-69)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Reinforcement learning : Learning from interaction how to behave in order to achieve a goal.
        &lt;ul&gt;
          &lt;li&gt;interaction : 에이전트와 환경이 이산 스텝의 시퀀스에 따라 상호작용하는 것&lt;/li&gt;
          &lt;li&gt;actions : 에이전트에 의해 이루어지는 선택&lt;/li&gt;
          &lt;li&gt;states : 선택에 영향을 주는 요소&lt;/li&gt;
          &lt;li&gt;rewards : 선택을 평가하는 요소&lt;/li&gt;
          &lt;li&gt;모든 agent 내의 요소들은 완전히 알고 있고, 에이전트에 의해 컨트롤된다.&lt;/li&gt;
          &lt;li&gt;모든 agent 밖의 요소는 불완전하게 제어되며, 완전히 알고 있는 것일수도, 그렇지 않은 것일수도 있다.&lt;/li&gt;
          &lt;li&gt;policy : 에이전트가 상태값을 인자로 한 함수를 통해 행동을 선택하는 확률적 규칙&lt;/li&gt;
          &lt;li&gt;agent 의 목적 : 전체 시간 동안 받을 수 있는 보상을 최대화 하는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Markov Decision Process (MDP)
        &lt;ul&gt;
          &lt;li&gt;위의 강화학습 설정이, 잘 정의된 전환 확률로 공식화되면 Markov Decision Process (MDP) 로 정의됨&lt;/li&gt;
          &lt;li&gt;유한 MDP : 유한한 상태, 행동 및 보상 세트가 있는 MDP&lt;/li&gt;
          &lt;li&gt;강화학습 이론의 대부분은 유한 MDP 로 제한하지만, 방법과 아이디어는 더 일반적으로 적용됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;return : 미래의 보상에 대한 함수로 agent 가 최대화 하려는 기대값
        &lt;ul&gt;
          &lt;li&gt;작업의 특성과 지연된 보상의 할인 정도에 따라 다른 정의를 가질 수 있음&lt;/li&gt;
          &lt;li&gt;할인이 적용되지 않은 return 식은 episodic tasks 에 맞는 방식
            &lt;ul&gt;
              &lt;li&gt;episodic tasks : 상호작용이 에피소드에 따라 자연스럽게 중지되는 형태&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;할인이 적용된 return 식은 continuing tasks 에 맞는 방식
            &lt;ul&gt;
              &lt;li&gt;continuing tasks : 상호작용이 자연스럽게 중단되지 않고 제한 없이 계속 이어지는 형태&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;value functions and optimal value functions
        &lt;ul&gt;
          &lt;li&gt;정책의 가치함수는 에이전트가 해당 정책을 사용하는 경우, 각 상태 혹은 상태-행동 쌍과 그에 예상되는 수익을 할당함&lt;/li&gt;
          &lt;li&gt;최적 정책의 가치함수는 각 상태 혹은 상태-행동 쌍에 모든 정책 중 달성할 수 있는 최대의 기대 수익을 할당함&lt;/li&gt;
          &lt;li&gt;최적 가치함수를 사용하는 정책을 최적 정책이라 한다.
            &lt;ul&gt;
              &lt;li&gt;최적 정책은 하나이거나 하나 이상일 수 있다. (예: 50 : 50 의 확률론적 최적 정책)&lt;/li&gt;
              &lt;li&gt;최적 가치 함수와 관련하여 탐욕적인 모든 정책은 최적 정책임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bellman optimality equations
        &lt;ul&gt;
          &lt;li&gt;최적 가치 함수을 만족하는 특별한 일관성 조건&lt;/li&gt;
          &lt;li&gt;이론적으로 최적 가치 함수를 풀 수 있는 방정식&lt;/li&gt;
          &lt;li&gt;최적 정책을 상대적으로 쉽게 결정할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습은 주어진 조건에 따라 다양한 방식으로 제기될 수 있음
        &lt;ul&gt;
          &lt;li&gt;에이전트의 지식
            &lt;ul&gt;
              &lt;li&gt;환경의 역학 (역학 함수 $p$ 의 4개의 인자) 을 아는 경우와 모르는 경우&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;계산 퍼포먼스 및 메모리 이슈
            &lt;ul&gt;
              &lt;li&gt;테이블 방식의 접근을 할지, 근사함수를 사용할 지에 대한 사항&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습 문제는 최적 솔루션을 찾는 것 보다, 어떻게 근사해야 할지에 더 집중하는 것이 바람직하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Value Functions" /><category term="Bellman Equations" /><summary type="html">관련 자료 (RLbook2018 Pages 58-67)</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 02. Week 2. Markov Decision Processes</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 02. Week 2. Markov Decision Processes" /><published>2023-02-07T18:00:00+09:00</published><updated>2023-02-07T18:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2/">&lt;h2 id=&quot;course-introduction&quot;&gt;Course Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;문제를 접하게 되었을 때, 가장 중요한 단계가 문제를 Markov Decision Process(MDP) 로 전환하는 것이다.&lt;/li&gt;
  &lt;li&gt;솔루션의 질은 이 전환과 밀접한 관계가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;관련-자료-rlbook2018-pages-47-56&quot;&gt;관련 자료 (RLbook2018 Pages 47-56)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Finite Markov Decision Processes (유한한 마르코프 결정 프로세스)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;bandits 문제와 같이 피드백에 대한 평가를 포함&lt;/li&gt;
      &lt;li&gt;또한 연관성 측면 (다른 상황에서 다른 액션을 선택하는 것) 의 성격도 가지고 있음
        &lt;ul&gt;
          &lt;li&gt;bandits 문제는 동일 상황에서 다른 액션을 선택하는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 는 고전적인 정형화된 sequential decision making (순차 결정) 이다.
        &lt;ul&gt;
          &lt;li&gt;액션이 즉각적인 보상에만 영향을 주는 것이 아닌,&lt;/li&gt;
          &lt;li&gt;후속 상황(situations), 에이전트의 상태(states), 미래의 보상 (future rewards) 에 영향을 줌.&lt;/li&gt;
          &lt;li&gt;그러므로 MDP 는 지연된 보상, 그리고 지연된 보상과 즉각적인 보상 간의 tradeoff (거래합의) 가 필요하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 수식 관련
        &lt;ul&gt;
          &lt;li&gt;bandit problems
            &lt;ul&gt;
              &lt;li&gt;estimate the value $q_*(a)$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;MDP
            &lt;ul&gt;
              &lt;li&gt;estimate the value $q_*(s,a)$ : a : action, s : state&lt;/li&gt;
              &lt;li&gt;or estimate the value $\upsilon_*(s)$ : 상태 기반 추정값&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 는 강화학습 문제에 있어 수학적으로 이상적인 형태
        &lt;ul&gt;
          &lt;li&gt;수학적 구조 상 핵심 요소들 : 보상, 가치함수, 벨먼 방정식 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습은 적용 가능성의 폭과 수학적 취급 용이성 사이에 있어 tradeoff (거래합의) 가 필요하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Agent-Environment Interface&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_1_agent_environment_in_MDP.png&quot; alt=&quot;3_1_1_agent_environment_in_MDP&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;MDP 는 목표 달성을 위해 상호작용을 통한 학습 문제에 있어 직접적인 틀이다.&lt;/li&gt;
      &lt;li&gt;학습자 (learner) 와 결정자 (decision maker) 는 에이전트 (agent) 라 한다.&lt;/li&gt;
      &lt;li&gt;에이전트 (agent) 밖의 모든 구성으로 상호작용을 하는 것을 환경 (environment) 이라 한다.&lt;/li&gt;
      &lt;li&gt;상호작용은 연속적으로 발생한다.
        &lt;ul&gt;
          &lt;li&gt;에이전트는 액션을 선택하고 환경은 액션에 반응한다.
            &lt;ul&gt;
              &lt;li&gt;환경은 새로운 상황을 에이전트에 제공한다.&lt;/li&gt;
              &lt;li&gt;환경은 에이전트에 보상을 제공한다.&lt;/li&gt;
              &lt;li&gt;보상은 특정한 수치값으로 에이전트가 액션을 선택하는 것을 통해 최대화하길 원하는 값이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;특별히, 에이전트는 환경과 이산적 타임스텝 (0, 1, 2, 3) 의 순서로 상호작용한다고 가정한다.
            &lt;ul&gt;
              &lt;li&gt;많은 아이디어들이 시간 연속적인 것이 될 수 있어도, 단순성을 유지하기 위해 케이스를 제한한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;각 타임스텝 t 에 따라…
            &lt;ul&gt;
              &lt;li&gt;agent 는 환경의 상태 (environment’s state) 값을 받음 : $S_t \in$ S&lt;/li&gt;
              &lt;li&gt;이에 따라 agent 는 액션을 선택함 : $A_t \in$ A$(s)$&lt;/li&gt;
              &lt;li&gt;액션에 따른 결과로 에이전트는 수치적 보상을 얻음 : $R_{t+1} \in R \subset$ R&lt;/li&gt;
              &lt;li&gt;이후 다른 환경의 상태 값을 제공받음 : $S_{t+1}$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위에 따라 MDP 와 에이전트는 아래와 같은 시퀀스, 혹은 궤적 (trajectory) 를 남기게 됨
            &lt;ul&gt;
              &lt;li&gt;$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, …$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_2_MDP_function_p_1.png&quot; alt=&quot;3_1_2_MDP_function_p_1&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;유한 MDP (finite MDP) 에서 모든 상태, 액션, 보상의 집합은 유한한 수의 요소 (element) 이다.&lt;/li&gt;
      &lt;li&gt;$R_t$ 와 $S_t$ 는 이전 상태와 액션값에만 영향을 받는 이산확률분포 (discrete probability distributions) 로 정의됨
        &lt;ul&gt;
          &lt;li&gt;이산확률분포 : 확률변수가 가질 수 있는 값이 명확하고 셀 수 있는 경우의 분포 (주사위의 눈 $1, …, 6$)&lt;/li&gt;
          &lt;li&gt;연속확률분포 : 확률변수가 가질 수 있는 값이 연속적인 실수여서 셀 수 없는 경우의 분포 ($y = f(x)$)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;우선 제공된 state 와 action 에 따른 time t 시점의 확률 값이 존재 (상단 수식)&lt;/li&gt;
      &lt;li&gt;모든 $s’, s \in S, r \in R, a \in A(s)$ 에 대해 function $p$ 는 dynamics of the MDP 로 정의된다.&lt;/li&gt;
      &lt;li&gt;일반적인 함수 표현, The dynamics function $p : S \times R \times S \times A \to [0,1]$&lt;/li&gt;
      &lt;li&gt;| : 조건부 확률에 대한 표기법이지만, 여기서는 $p$ 가 $s, a$ 의 각 선택에 대한 확률분포를 지정한다는 것을 뜻함.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_3_MDP_function_p_2.png&quot; alt=&quot;3_1_3_MDP_function_p_2&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;MDP (Markov decision process) 에서 $p$ 로 주어진 확률은 환경의 역학을 완벽히 특성화함.
        &lt;ul&gt;
          &lt;li&gt;즉, 가능한 $S_t$, $R_t$ 값에 대한 가능성은 이전에 즉시 제공된 $S_{t-1}$, $A_{t-1}$ 값에만 의존한다.&lt;/li&gt;
          &lt;li&gt;그 이전의 상태값과 액션은 영향을 주지 않는다.&lt;/li&gt;
          &lt;li&gt;이것은 결정 과정에 대한 제한이 아닌 상태에 대한 제한을 의미한다.&lt;/li&gt;
          &lt;li&gt;상태에 미래의 차이를 발생 시킬 수 있는 이전 에이전트-환경 간 상호작용에 대한 측면이 모두 정보로 포함되어 있어야 한다.&lt;/li&gt;
          &lt;li&gt;만약 그렇다면, 이 상태를 마르코프 속성 (Markov property) 이라 한다.
            &lt;ul&gt;
              &lt;li&gt;이후 과정에서 Markov 속성에 의존하지 않는 근사 방법 (approximation methods) 을 고려할 것&lt;/li&gt;
              &lt;li&gt;17장에서는 Markov 가 아닌 관찰값으로부터 어떻게 Markov 상태를 구성하고 학습할지를 고려할 것&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;4개의 인수를 가진 dynamic function $p$ 에서 state-transition probabilities (상태 전이 확률) 와 같은 알고싶은 모든 것을 계산할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_4_MDP_function_p_3.png&quot; alt=&quot;3_1_4_MDP_function_p_3&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;$p : S \times S \times A \to [0,1]$&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;또한 예상되는 보상값을 2개 인자 (state,action) 의 r 함수로 계산할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_5_MDP_function_p_4.png&quot; alt=&quot;3_1_5_MDP_function_p_4&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;$r : S \times \ A \to R$&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;예상되는 보상값을 3개 인자 (state, action, next state) 의 r 함수로 계산할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_6_MDP_function_p_5.png&quot; alt=&quot;3_1_6_MDP_function_p_5&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;$r : S \times \ A \times S \to R$&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;이 책에서 우리는 주로 4개 인자의 $p$ 함수 (3.2) 를 사용하지만 때때로 다른 함수가 편리할 경우도 있음.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;MDP 프레임워크는 추상적이고 유연하여 다양한 문제에 적용할 수 있음
            &lt;ul&gt;
              &lt;li&gt;스텝 (Step) 은 고정된 시간 간격일 필요가 없음. 의사 결정과 행동의 임의적 연속 단계일 수 있음&lt;/li&gt;
              &lt;li&gt;동작 (Action) 은 로봇 팔의 모터 전압과 같이 낮은 수준의 제어부터 대학원에 갈 것인지 여부를 정하는 높은 수준의 결정일 수 있음.&lt;/li&gt;
              &lt;li&gt;상태 (State) 는 직접적인 센서의 데이터처럼 낮은 수준의 감각일 수 있고, 방에 있는 물체에 대한 상직적 설명과 같이 높은 수준의 추상일 수 있음.&lt;/li&gt;
              &lt;li&gt;또한 상태 (State) 는 과거 감각의 기억이거나 정신적, 주관적인 것일 수 있음.
                &lt;ul&gt;
                  &lt;li&gt;예를 들어 에이전트는 물체가 어디있는지 확신하지 못할 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;액션 (Action) 은 정신적일수도, 계산적일 수도 있음
                &lt;ul&gt;
                  &lt;li&gt;일부 동작은 에이전트가 어디에 집중할지 를 변경하는 것일 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;즉 액션은 우리가 결정을 내리는 방법을 배우고자 하는 모든 결정, 상태는 결정을 내리는 데 유용한 모든 것이 될 수 있음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;MDP 에서 에이전트와 환경의 경계는 일반적인 물리적 경계와 동일하지 않음.
            &lt;ul&gt;
              &lt;li&gt;대게 경계는 중간지점보다 에이전트에 더 가깝게 그려짐
                &lt;ul&gt;
                  &lt;li&gt;예를 들어 로봇의 기계적 감지 하드웨어는 에이전트의 일부가 아닌 환경의 일부로 간주되어야 함.&lt;/li&gt;
                  &lt;li&gt;생물체일 경우 근육, 골격 및 감각 기관은 환경의 일부로 간주되어야 함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;보상 또한 인공 학습 시스템 본체 내부에서 계산되지만, 에이전트 외부에 있는 것으로 간주됨.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉, 에이전트가 임의로 변경할 수 없는 모든 것은 환경의 일부로 간주한다는 것임.
            &lt;ul&gt;
              &lt;li&gt;에이전트는 환경의 일부를 알고 있음
                &lt;ul&gt;
                  &lt;li&gt;에이전트는 자신의 행동과 상태에 대해 보상이 어떻게 함수로 계산되는지에 대해 알고 있음.&lt;/li&gt;
                  &lt;li&gt;그러나 보상 계산은 에이전트 밖의 영역임.&lt;/li&gt;
                  &lt;li&gt;우리가 루빅큐브가 어떻게 작동하는지 알지만, 여전히 해결할 수 없는 문제가 될 수 있는 것처럼&lt;/li&gt;
                  &lt;li&gt;환경이 어떻게 작동하는 지에 모든 것을 알고있으면서도 여전히 어려운 강화 학습 작업에 직면할 수 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;즉, 에이전트와 환경의 경계는 에이전트의 지식이 아닌 제어의 한계를 나타냄.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;에이전트와 환경의 경계는 서로 다른 목적을 위해 다르게 위치할 수 있음
            &lt;ul&gt;
              &lt;li&gt;복잡한 로봇을 예로 들면 각각 고유한 경계를 가진 여러 에이전트가 한번에 작동할 수 있음
                &lt;ul&gt;
                  &lt;li&gt;높은 수준의 결정을 구현하는 에이전트가 낮은 수준의 에이전트가 직면한 상태의 일부를 형성하는 높은 수준의 결정을 내릴 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;에이전트와 환경 간의 경계는 상태, 액션, 보상을 결정한 뒤 결정을 위한 관심사를 식별한 뒤에 정해짐.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉, MDP 프레임워크는 상호작용으로부터 목표 지향적 학습을 하는 문제를 추상화한 것이다.
            &lt;ul&gt;
              &lt;li&gt;감각, 기억, 제어 장치의 세부사항이 무엇이든&lt;/li&gt;
              &lt;li&gt;달성하려는 목표가 무엇이든&lt;/li&gt;
              &lt;li&gt;목표 지향적 행동을 학습하는 문제는 에이전트와 그 환경 사이를 오가는 3가지 신호로 축소될 수 있다고 제안하는 것.
                &lt;ul&gt;
                  &lt;li&gt;Actions : 에이전트의 선택을 나타내는 신호&lt;/li&gt;
                  &lt;li&gt;States : 선택이 이루어지는 기준(상태) 를 나타내는 신호&lt;/li&gt;
                  &lt;li&gt;Rewards : 에이전트의 목표를 정의하는 신호&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;이 프레임워크는 모든 의사결정 학습문제를 유용하게 나타내기에 충분하지 않을 수 있지만, 널리 이용 가능하고 적용 가능한 것으로 입증됨.
            &lt;ul&gt;
              &lt;li&gt;물론 states 와 actions 는 작업마다 크게 다르며, 이것을 표현하는 방식이 성능에 큰 영향을 줌&lt;/li&gt;
              &lt;li&gt;다른 종류의 학습과 마찬가지로 강화학습에서 이러한 표현 선택은 과학보다 예술에 가까움.&lt;/li&gt;
              &lt;li&gt;이 책에서 우리는 상태와 행동을 나타내는 좋은 방법에 관한 몇 가지 조언과 예를 제공하지만,&lt;/li&gt;
              &lt;li&gt;우리의 주요 초점은 일반적인 원칙 (표현법이 선택되면 그것이 어떻게 동작하는지) 을 배우는 것에 있음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예제 3.1 : Bioreactor (바이오 리액터 - 세포배양기)
            &lt;ul&gt;
              &lt;li&gt;강화 학습이 세포배양기의 순간 온도와 교반 속도 (뒤섞는 교반기의 회전 속도) 를 결정하기 위해 적용되어 있다고 가정&lt;/li&gt;
              &lt;li&gt;Actions : 목표 온도와 목표 교반속도를 달성하기 위해 발열체와 모터를 직접 조종하는 하위 제어시스템에 전달되는 값&lt;/li&gt;
              &lt;li&gt;States : 여과되고 지연된 열전대, 기타 센서 값과 대상 화학물질을 나타내는 상징적 입력값 등&lt;/li&gt;
              &lt;li&gt;Rewards : 유용한 화합물이 생성되었는지에 대한 순간적인 측정값&lt;/li&gt;
              &lt;li&gt;여기서 상태 값은 list 혹은 vector 값 (센서 측정값, 상징적 입력값)&lt;/li&gt;
              &lt;li&gt;액션 값은 vector 값 (목표 온도와 교반속도로 이루어진)&lt;/li&gt;
              &lt;li&gt;보상 값은 항상 단일 숫자값&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예제 3.2 : Pick-and-Place Robot
            &lt;ul&gt;
              &lt;li&gt;반복적인 물체 이동 작업에서 로봇 팔의 동작을 제어하기 위해 강화학습을 사용한다고 가정&lt;/li&gt;
              &lt;li&gt;빠르고 부드러운 동작을 학습하려면 학습 에이전트가 모터를 직접 제어하고, 기계 연결장치의 현재 위치와 속도에 대한 짧은 지연시간의 정보를 가져와야 함&lt;/li&gt;
              &lt;li&gt;Actions : 각 관절의 모터에 적용되는 전압&lt;/li&gt;
              &lt;li&gt;States : 관절의 각도와 속도의 최신 값&lt;/li&gt;
              &lt;li&gt;Rewards : 잘 운반된 개체에 대해 + 1 값&lt;/li&gt;
              &lt;li&gt;부드러운 움직임을 장려하기 위해 각 단계의 순간순간의 갑작스러운 움직임에 작은 처벌 (보상의 음수값) 을 부여&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예제 3.3 : Recycling Robot (재활용 로봇)
            &lt;ul&gt;
              &lt;li&gt;모바일 로봇은 사무실 환경에서 빈 음료수 캔을 모으는 일을 함&lt;/li&gt;
              &lt;li&gt;로봇은 캔을 감지하는 센서, 캔을 집어 일체형 통에 넣을 수 있는 팔과 그리퍼를 가지고 있음&lt;/li&gt;
              &lt;li&gt;로봇은 충전식 배터리로 작동&lt;/li&gt;
              &lt;li&gt;로봇의 제어 시스템에는 센서 정보를 해석하는 파츠, 탐색기능 파츠, 팔과 그리퍼를 컨트롤 하는 파츠 가 있음.&lt;/li&gt;
              &lt;li&gt;높은 수준의 의사결정 (빈 캔을 어떻게 찾을지) 은 현재 배터리 충전 레벨에 따라 강화학습 에이전트가 내림.&lt;/li&gt;
              &lt;li&gt;간단한 예제를 만들기 위해 충전정도는 2가지 레벨만 구분한다고 가정, 작은 상태 집합 $S = \lbrace high,low \rbrace$  으로 구성&lt;/li&gt;
              &lt;li&gt;각 상태에서 에이전트는 다음 중 하나의 결정을 할 수 있음.
                &lt;ul&gt;
                  &lt;li&gt;(1) 일정 시간 동안 적극적으로 캔을 찾음&lt;/li&gt;
                  &lt;li&gt;(2) 움직이지 않고 누군가가 캔을 가져올 때까지 기다림&lt;/li&gt;
                  &lt;li&gt;(3) 배터리 충전을 위해 충전 장소로 돌아감&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;배터리가 많이 남았을 때 (high), 충전은 언제나 멍청한 선택이므로 해당 상태의 action set 으로 포함시키지 않는다.
                &lt;ul&gt;
                  &lt;li&gt;action set 은 $A(high) = \lbrace search,wait \rbrace$ 와 $A(low) = \lbrace search, wait, recharge \rbrace$ 이다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;대부분의 경우 보상은 0 이지만, 로봇이 빈 캔을 확보하면 (+), 배터리가 완전히 방전되면 큰 수치의 (-) 가 됨.&lt;/li&gt;
              &lt;li&gt;캔을 찾는 가장 좋은 방법은 능동적 탐색이지만 배터리가 소모됨 (배터리가 고갈될 가능성이 있음)&lt;/li&gt;
              &lt;li&gt;기다리는 것은 배터리가 소모되지 않음&lt;/li&gt;
              &lt;li&gt;배터리가 고갈되면 로봇은 종료되고 구조될 때까지 기다려야 함 (낮은 보상 생성)&lt;/li&gt;
              &lt;li&gt;배터리가 많으면 배터리 고갈 위험 없이 빈 캔 탐색을 완료할 수 있음
                &lt;ul&gt;
                  &lt;li&gt;배터리가 많을 때의 탐색 기간에서 배터리가 많이 남아 있을 확률 $\alpha$&lt;/li&gt;
                  &lt;li&gt;배터리가 많을 때의 탐색 기간에서 배터리가 적어질 확률 $1 - \alpha$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;반대로 배터리가 적으면
                &lt;ul&gt;
                  &lt;li&gt;배터리가 적을 때의 탐색 기간에서 배터리가 적어질 확률 $\beta$&lt;/li&gt;
                  &lt;li&gt;배터리가 적을 때의 탐색 기간에서 배터리가 고갈될 확률 $1 - \beta$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;후자의 경우 로봇은 구조되어야만 하며, 이 경우 배터리는 다시 완충된다.&lt;/li&gt;
              &lt;li&gt;로봇에 의해 수집된 캔의 수량 만큼 보상이 주어지며, 로봇이 구출되었을 때 보상값 -3 이 주어진다.&lt;/li&gt;
              &lt;li&gt;$r_{search}, r_{wait}$ with $r_{search} &amp;gt; r_{wait}$ : 로봇이 검색 혹은 대기 중에 수집할 것으로 추정되는 캔 수 (즉, 예상되는 보상)&lt;/li&gt;
              &lt;li&gt;충전을 위해 충전 장소로 돌아갈 때나, 배터리가 고갈되었을 때에는 캔을 회수할 수 없음&lt;/li&gt;
              &lt;li&gt;이 시스템은 유한한 MDP 이며 전확 확률과 예상 보상을 아래와 같이 표기할 수 있다.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_7_MDP_transition_probabilities.png&quot; alt=&quot;3_1_7_MDP_transition_probabilities&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 표는 현 상태 $s$, 액션 $a \in A(s)$, 다음 상태 $s’$ 의 각각의 가능한 조합으로 구성된 표이다.&lt;/li&gt;
              &lt;li&gt;몇몇 전환은 일어날 가능성이 0 이므로, 기대되는 보상 또한 없다.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_8_MDP_transition_graph.png&quot; alt=&quot;3_1_8_MDP_transition_graph&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 그래프는 transition graph 로, 유한 MDP 의 역학을 요약하는 또다른 방법이다.&lt;/li&gt;
              &lt;li&gt;state nodes 와 action nodes 가 있음.&lt;/li&gt;
              &lt;li&gt;state nodes : 큰 빈 원, 원 안에 라벨링 된 이름&lt;/li&gt;
              &lt;li&gt;action nodes : 작은 검은색 원과 state nodes 를 연결 시키는 화살표&lt;/li&gt;
              &lt;li&gt;상태 $s$ 에서 작업 $a$ 를 수행하면 상태노드 $s$ 에서 작업노드 $(s,a)$ 로 이동함.&lt;/li&gt;
              &lt;li&gt;그 다음 환경은 $(s,a)$ 를 떠나는 화살표 중 하나를 통해 다음 상태의 노드 $s’$ 로 전환시킴&lt;/li&gt;
              &lt;li&gt;각 화살표는 $(s, s’, a)$ 에 해당함.&lt;/li&gt;
              &lt;li&gt;여기서 $s’$ 는 다음 상태이고, 전환확률 $p(s’|s,a)$ 와 해당 전환에 대한 예상보상은 $r(s,a,s’)$ 임.&lt;/li&gt;
              &lt;li&gt;작업 노드를 떠나는 화살표의 전환 확률의 합계는 항상 1임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goals and Rewards&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;강화학습의 목적, 목표는 특별한 신호로 정형화 되는데 이를 보상 (Reward) 이라 하고 환경이 에이전트에 전달하는 값이다.&lt;/li&gt;
      &lt;li&gt;각 스텝마다 보상은 단순한 숫자 값이다. $R_t \in R$&lt;/li&gt;
      &lt;li&gt;비공식적으로, 에이전트의 목표는 총 보상의 양을 최대화 하는 것이다.
        &lt;ul&gt;
          &lt;li&gt;이 최대화의 의미는 당장의 보상을 최대화 하는 것이 아닌 장기적 누적 보상을 최대화 하는 것이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상 측면에서 위 목표를 공식화하는 것이 처음에는 제한적으로 보일 수 있지만, 실은 유연하고 광범위하게 적용될 수 있다.
        &lt;ul&gt;
          &lt;li&gt;예 1. 로봇이 걷는 법을 배움 : 보상은 로봇이 전진할 때마다 주어짐&lt;/li&gt;
          &lt;li&gt;예 2. 로봇이 미로를 통과하는 법을 배움 : 매 스텝마다 -1 의 보상을 주어 에이전트가 최대한 빨리 미로를 탈출하는 것을 장려한다.&lt;/li&gt;
          &lt;li&gt;예 3. 재활용 캔 수집을 하는 법을 배움 : 캔을 모을 때마다 +1 의 보상을 주며, 뭔가에 부딪히거나 누군가 소리를 지를 경우 - 보상을 준다.&lt;/li&gt;
          &lt;li&gt;예 4. 체스 두는 법을 배움 : 이길 때 +1점, 질 때 -1점, 비길 때 0점의 보상을 부여&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;즉, 에이전트가 우리의 목표를 달성하게 하기 위해 최대화 할 수 있는 보상을 제공해줘야 한다.
        &lt;ul&gt;
          &lt;li&gt;따라서 보상이 정말 우리가 원하는 결과를 나타내는 것인지가 매우 중요하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상은 우리가 원하는 것을 달성하기 위한 사전지식을 전달하는 곳이 아니다.
        &lt;ul&gt;
          &lt;li&gt;이것에 적합한 위치는 초기 정책 또는 초기 가치 함수, 혹은 이들에 영향을 줄 수 있는 값이다.&lt;/li&gt;
          &lt;li&gt;예를 들어 체스 게임일 경우 상대의 말을 잡거나 센터를 장악하는 것에 보상이 주어저서는 안 된다.
            &lt;ul&gt;
              &lt;li&gt;에이전트는 승패의 여부와 관계없이 이 서브 목표를 달성하는 것을 학습할 것이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉, 보상은 에이전트에게 달성해야 하는 것에 대한 소통을 하는 방법이고, 어떻게 달성할지에 대한 것을 이야기 하는 것이 아니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Returns and Episodes&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_3_1_MDP_sum_of_rewards.png&quot; alt=&quot;3_3_1_MDP_sum_of_rewards&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;정확히 시퀀스의 어떤 면을 최대화 하길 원하는지에 대하여, 가장 단순히 보상의 합계를 표현한 형태&lt;/li&gt;
      &lt;li&gt;final time step $T$ 라는 개념이 있을 경우
        &lt;ul&gt;
          &lt;li&gt;환경과 에이전트 간 상호작용이 자연스럽게 끊길 때, 우리는 이것을 에피소드 (episode) 라 표현&lt;/li&gt;
          &lt;li&gt;예를 들어 게임 한 판, 하나의 미로를 통과하는 것 혹은 어떠한 종류의 반복적인 상호작용 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;에피소드가 끝난 상태를 terminal state 라 한다.
        &lt;ul&gt;
          &lt;li&gt;이후 표준 시작 상태로 재설정하거나, 시작 상태의 표준 분포에서 샘플을 재설정한다.&lt;/li&gt;
          &lt;li&gt;에피소드가 다른 방식으로 끝나더라도 (예를들어 게임에서의 승리,패배) 다음 에피소드는 이전 에피소드와 독립적으로 시행된다.&lt;/li&gt;
          &lt;li&gt;즉 에피소드는 모두 terminal state 로 끝나나, 보상과 결과는 다를 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;위와 같은 에피소드 형태의 일을 episodic tasks 라 한다.
        &lt;ul&gt;
          &lt;li&gt;$S$ : 모든 non-terminal state&lt;/li&gt;
          &lt;li&gt;$S^+$ : 모든 non-terminal state + terminal state&lt;/li&gt;
          &lt;li&gt;$T$ : The time of termination&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;반면 많은 경우 환경과 에이전트의 상호작용은 자연스럽게 별개의 에피소드로 끊기지 않는다.
        &lt;ul&gt;
          &lt;li&gt;이를 continuing tasks 라 한다.&lt;/li&gt;
          &lt;li&gt;final time step $T=\infty$ 이기 때문에 위 3.7의 수식으로 표현할 수 없다.&lt;/li&gt;
          &lt;li&gt;(우리가 최대화를 원하는 리턴 값이 쉽게 무한대의 값이 될 수 있다.)&lt;/li&gt;
          &lt;li&gt;즉, discounting 이라는 추가적 개념이 필요하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_3_2_MDP_discounted_return.png&quot; alt=&quot;3_3_2_MDP_discounted_return&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;에이전트는 미래의 할인된 보상값의 합 (sum of the discounted rewards) 이 최대값이 되도록 액션 ($A_t$)을 선택한다.&lt;/li&gt;
      &lt;li&gt;$0 \le \gamma \le 1$ 인 $\gamma$ 파라미터를 discount rate 라 한다.&lt;/li&gt;
      &lt;li&gt;discount rate 는 미래 보상의 현재 가치를 결정한다.
        &lt;ul&gt;
          &lt;li&gt;미래 $k$ time steps 에서 받을 보상은 현재의 가치로 따졌을 때 $\gamma^{k-1}$ 배의 가치만 있다.&lt;/li&gt;
          &lt;li&gt;$\gamma &amp;lt; 1$ : 극한 합 (3.8) 의 값이 유한한 값으로 수렴한다. ($R_k$ 가 제한된 값일 경우)&lt;/li&gt;
          &lt;li&gt;$\gamma = 0$ : 에이전트는 오직 $A_t$ 를 선택해 $R_{t+1}$ 값을 최대화하는 방법만을 학습한다. (근시안적)&lt;/li&gt;
          &lt;li&gt;$\gamma \to 1$ : 1에 가까워질 수록 미래의 보상값을 더 강하게 계산. (원시안적)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_3_3_MDP_discounted_return.png&quot; alt=&quot;3_3_3_MDP_discounted_return&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;연속적인 time step (successive time step) 에서의 보상은 상호간 연관관계에 있고, 이는 강화학습에서 중요한 개념이다.&lt;/li&gt;
      &lt;li&gt;위 공식은 모든 time steps $t &amp;lt; T$ 에서 적용되며, 심지어 $t + 1$ 에서 termination 이 일어나는 경우에도 $G_T = 0$ 으로 정의함으로써 적용가능하다.&lt;/li&gt;
      &lt;li&gt;3.8 의 인자 수가 무한대이지만, 보상이 0 이 아니고 상수 일 경우 여전히 유한한 숫자가 된다.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 보상이 상수 $+1$ 이고 , $\gamma &amp;lt; 1$ 일 경우 보상 값은 아래의 값이 된다.
  &lt;img src=&quot;/assets/images/posts/3_3_4_MDP_discounted_return.png&quot; alt=&quot;3_3_4_MDP_discounted_return&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;예제 3.4 : Pole-Balancing&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_4_1_cart_pole.png&quot; alt=&quot;3_4_1_cart_pole&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;목표 : 카트에 힘을 적용하여 트랙 위를 카트가 이동할 수 있되, 폴이 서 있고 넘어지지 않게 유지하는 것&lt;/li&gt;
          &lt;li&gt;실패 : 폴이 주어진 각도를 넘어서 넘어지는것 혹은 카트가 트랙에서 탈선하는 것&lt;/li&gt;
          &lt;li&gt;폴은 매 실패 이후 리셋되어 수직으로 서게 된다 &amp;gt; 이 일은 episodic 으로 간주될 수 있음
            &lt;ul&gt;
              &lt;li&gt;자연스러운 에피소드가 반복됨&lt;/li&gt;
              &lt;li&gt;보상은 실패하기 전 모든 타임 스텝에 +1 을 줄 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위의 설정은, 폴이 영원히 성공적으로 벨런스를 잡게 되면 보상은 무한수가 된다는 뜻이다.&lt;/li&gt;
          &lt;li&gt;또는 위 문제를 continuing task 로 간주할 수도 있다. (using discounting)
            &lt;ul&gt;
              &lt;li&gt;위의 경우 실패할 경우 보상은 -1 이 되고, 그 외에는 0 이 된다.&lt;/li&gt;
              &lt;li&gt;즉 보상 값은 $- \gamma^K$ 가 되며, $K$ 는 실패 이전의 타임스텝이 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;양쪽의 경우 모두 폴의 균형을 최대한 유지할 수록 보상이 최대화 된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-markov-decision-processes&quot;&gt;Introduction to Markov Decision Processes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Markov Decision Process (MDP) 에 대해 이해하기&lt;/li&gt;
      &lt;li&gt;MDP 프로세스가 정의되는 방법 이해&lt;/li&gt;
      &lt;li&gt;Markov Decision Process 의 그래픽 표현 이해&lt;/li&gt;
      &lt;li&gt;MDP 프레임워크로 다양한 프로세스를 작성하는 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Markov Decision Processes
    &lt;ul&gt;
      &lt;li&gt;k-Armed Bandit Problem : 같은 상황에서 같은 Action 이 항상 최적화된 선택이 되는 문제유형 (현실과 다름)
        &lt;ul&gt;
          &lt;li&gt;예 : 토끼의 왼쪽에 브로콜리 (보상 : +3) 가 있고 오른쪽에 당근 (보상 : +10) 이 있을 경우&lt;/li&gt;
          &lt;li&gt;토끼는 오른쪽의 당근을 먹는 것을 선택한다.&lt;/li&gt;
          &lt;li&gt;갑자기 두 음식의 위치가 바뀔경우 k-Armed Bandit Problem 으로 정의할 수 없는 문제가 된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;현실 : 다른 상황이 다른 반응을 야기하며, 현재 선택한 Action 이 미래의 보상의 양을 결정한다.
        &lt;ul&gt;
          &lt;li&gt;예 : 토끼의 왼쪽에 브로콜리 (보상 : +3) 가 있고 오른쪽에 당근 (보상 : +10) 이 있을 경우&lt;/li&gt;
          &lt;li&gt;단, 오른쪽 방향에 호랑이가 있음&lt;/li&gt;
          &lt;li&gt;장기적 이익 관점에서 토끼는 브로콜리를 선택해야 한다. (즉각적 보상이 적다 하더라도)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/The_dynamics_of_an_MDP.png&quot; alt=&quot;The_dynamics_of_an_MDP&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;The dynamics of an MDP
        &lt;ul&gt;
          &lt;li&gt;출처 : Coursera / Fundamentals of Reinforcement Learning / W2. Markov Decision Process / 동영상 : Markov Decision Processes&lt;/li&gt;
          &lt;li&gt;The dynamics of an MDP : 확률 분포에 의해 정의됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Examples of MDPs
    &lt;ul&gt;
      &lt;li&gt;Recycling Robot 에 대한 예시
        &lt;ul&gt;
          &lt;li&gt;State : 배터리 잔량 High, 배터리 잔량 Low&lt;/li&gt;
          &lt;li&gt;Action :
            &lt;ul&gt;
              &lt;li&gt;배터리 잔량 High : Search, Wait, Recharge&lt;/li&gt;
              &lt;li&gt;배터리 잔량 Low : Search, Wait&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/Dynamics_of_the_Recycling_Robot.png&quot; alt=&quot;Dynamics_of_the_Recycling_Robot&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Dynamics_of_the_Recycling_Robot
        &lt;ul&gt;
          &lt;li&gt;출처 : Coursera / Fundamentals of Reinforcement Learning / W2. Markov Decision Process / 동영상 : Examples of MDPs&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 는 다양한 문제에 적용이 가능하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;goal-of-reinforcement-learning&quot;&gt;Goal of Reinforcement Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;에이전트의 목표와 보상이 어떻게 관련되는지 설명&lt;/li&gt;
      &lt;li&gt;에피소드에 대한 이해 및 에피소드로 표현될 수 있는 작업 식별&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The Goal of Reinforcement Learning
    &lt;ul&gt;
      &lt;li&gt;Bandits Problems : 즉각적 보상의 최대화&lt;/li&gt;
      &lt;li&gt;MDP : 타임스텝 전체의 보상 합의 최대화
        &lt;ul&gt;
          &lt;li&gt;토끼가 오른쪽의 당근을 먹는 것은 즉각적 보상의 최대화를 야기하나 이후 바뀐 상태에 의해 호랑이에 잡아먹힘&lt;/li&gt;
          &lt;li&gt;로봇에게 걷는 방법을 가르쳐 줄 때, 보상 값이 앞으로 나아간 수치라 하여 이를 즉각적으로 최대화 할 경우 넘어져 걷질 못하게 됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Episodic  Tasks : 마지막 스텝을 밟은 뒤 다시 초기상태로 돌아감 (에이전트-환경 간 상호작용의 종료)
        &lt;ul&gt;
          &lt;li&gt;체스게임&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Michael Littman: The Reward Hypothesis
    &lt;ul&gt;
      &lt;li&gt;Reinforcement Learning 에서 보상을 정의하는 것
        &lt;ul&gt;
          &lt;li&gt;물고기를 주면 하루를 먹고 : Programming (Good Old-Fashioned AI) &amp;gt; 새로운 환경에서 새 프로그래밍을 해야 함&lt;/li&gt;
          &lt;li&gt;물고기를 잡는 법을 알려주면 평생을 먹고 : Supervised Learning &amp;gt; 학습 데이터를 제공해야 함&lt;/li&gt;
          &lt;li&gt;물고기의 맛을 알려주면 어떻게 물고기를 잡을지 알아낸다. : Reinforcement learning &amp;gt; 최적화문제&lt;/li&gt;
          &lt;li&gt;Ashraf : All goals can be described by the maximization of expected cumulative rewards&lt;/li&gt;
          &lt;li&gt;Silver : All goasl can be described by the maximization of expected cumulative rewards&lt;/li&gt;
          &lt;li&gt;Sutton : What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습의 정의
        &lt;ul&gt;
          &lt;li&gt;Intelligent behavior arises from the actions of an individual seeking to maximize its received reward signals in a complex and changing world.
            &lt;ul&gt;
              &lt;li&gt;identify where reward signals come from,&lt;/li&gt;
              &lt;li&gt;develop algorithms that search the space of behaviors to maximize reward signals.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;보상은 복잡한 실 세계 속에서 단순해야 한다.&lt;/li&gt;
          &lt;li&gt;에이전트가 최적화해야 할 보상이 무엇인지 알아내야 한다.&lt;/li&gt;
          &lt;li&gt;위 보상을 최대화 할 알고리즘을 디자인해야 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상 정의의 어려움
        &lt;ul&gt;
          &lt;li&gt;공통 통화 정의의 어려움
            &lt;ul&gt;
              &lt;li&gt;온도조절장치 제어의 경우 : 난방/에어컨 등의 에너지 비용 vs 거주자의 불편도?&lt;/li&gt;
              &lt;li&gt;위의 정도를 달러로 표기? (자연스럽지가 않음)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;하나의 예는 목적을 달성했을 때 +1, 그렇지 않은 경우 0 (goal reward Representation)&lt;/li&gt;
          &lt;li&gt;반대로 목적이 달성하지 않았을때  매 스텝마다 -1 (action penalty Representation)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상의 정의 방식에 따른 차이 발생
        &lt;ul&gt;
          &lt;li&gt;Goal-reward  Representation : 긴급함이 없음&lt;/li&gt;
          &lt;li&gt;Action-Penalty Representation : 진행이 막혀 더이상 목적을 달성할 수 없는 확률이 있을 경우 오작동&lt;/li&gt;
          &lt;li&gt;두 경우 모두 흐름이 길 경우 큰 문제가 발생함.&lt;/li&gt;
          &lt;li&gt;어떠한 중간 보상은 에이전트가 올바른 방향으로 학습하는데 큰 도움이 될 수도 있음&lt;/li&gt;
          &lt;li&gt;보상의 출처가 사람일 경우
            &lt;ul&gt;
              &lt;li&gt;사람은 보상 기능과 다르게 에이전트가 학습하는 방식에 따라 보상을 변경하는 경향이 있음 (Non-stationary Rewards)
                &lt;ul&gt;
                  &lt;li&gt;고전적인 강화학습 알고리즘은 위와 같은 경우 잘 적용되지 않음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상의 지정
        &lt;ul&gt;
          &lt;li&gt;프로그래밍 방식
            &lt;ul&gt;
              &lt;li&gt;학습 에이전트에 대한 보상을 정의하는 가장 일반적인 방법&lt;/li&gt;
              &lt;li&gt;예 : 상태를 가져오고 보상을 출력하는 프로그램을 작성&lt;/li&gt;
              &lt;li&gt;프로그래밍 언어의 예 - temporal logic : 사람과 기계 언어의 중간 지점&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예를 들어 사람이 주는 보상을 복사하는 방법을 배우는 것 : Mimic reward&lt;/li&gt;
          &lt;li&gt;이 접근 방식의 흥미로운 버전 : 역 강화학습 : Inverse Reinforcement Learning
            &lt;ul&gt;
              &lt;li&gt;역 강화학습에서 지도자는 원하는 행동의 예를 보여주고 학습자는 지도자가 이 행동을 최적으로 만들기 위해 최대화한 보상이 무엇인지 파악&lt;/li&gt;
              &lt;li&gt;강화학습이 보상에서 행동으로 진행되는 반면, 역강화학습은 행동에서 보상으로 진행됨.&lt;/li&gt;
              &lt;li&gt;일단 보상이 식별되면 이러한 보상은 다른 설정에서 최대화 될 수 있으므로 환경 간에 강력한 일반화가 가능함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;최적화 프로세스를 통해 간접적으로 파생되는 보상
            &lt;ul&gt;
              &lt;li&gt;점수를 생성할 수 있는 높은 수준의 행동이 있는 경우&lt;/li&gt;
              &lt;li&gt;해당 행동을 장려하는 보상을 고려할 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Meta reinforcement learning
            &lt;ul&gt;
              &lt;li&gt;동 목표를 수행하는 단일 에이전트가 아닌 다수 에이전트가 있을 경우&lt;/li&gt;
              &lt;li&gt;행동의 결과로 보상을 받는 것 뿐만 아닌 이 행동에 대한 인센티브로 보상을 사용하여 평가할 수 있게 함&lt;/li&gt;
              &lt;li&gt;강화학습 에이전트는 더 좋은 보상 기능과 이를 최대화하기 위한 더 좋은 알고리즘이 있는 경우 생존함&lt;/li&gt;
              &lt;li&gt;개별 수준에서 더 나은 학습 방법을 만드는 진화 수준에서의 학습방법&lt;/li&gt;
              &lt;li&gt;에이전트가 자손에게 보상 함수를 계승함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상 이론의 도전과제
        &lt;ul&gt;
          &lt;li&gt;보상을 극대화 하는 것 이외에 다른 일을 하는것 처럼 보이는 행동의 예
            &lt;ul&gt;
              &lt;li&gt;위험 회피 행동 : 최선이 아닐 수 있지만 최악의 결과가 발생할 가능성을 최소화 하는 행동을 선택&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;원하는 행동이 항상 최선의 것이 아니라 여러 가지를 균형있게 수행하는 것이라면?
            &lt;ul&gt;
              &lt;li&gt;예 : 음악 추천 시스템 (최근 재생된 곡의 경우 그 곡에 대한 보상이 줄어야 함)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;올바른 결정이 단순히 하나의 목표를 추구하는 것이 아니라면?
            &lt;ul&gt;
              &lt;li&gt;더 나은 목표를 만드는 것&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;그럼에도 보상을 극대화 하는 것이 지능형 에이전트에게 동기를 부여하는 훌륭한 근사치일 수 있다는 가능성을 염두&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;continuing-tasks&quot;&gt;Continuing Tasks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;연속적인 작업에 대해 할인을 이용한 보상의 수식화&lt;/li&gt;
      &lt;li&gt;연속적인 time step 에서 보상이 상호간 어떻게 관련되어 있는지 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Continuing Tasks
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Episodic tasks 와 달리 현실에서는 에이전트와 환경이 계속 상호작용 하는 경우 (Continuing) 가 많다.&lt;/p&gt;

        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th style=&quot;text-align: left&quot;&gt;Episodic Tasks&lt;/th&gt;
              &lt;th style=&quot;text-align: left&quot;&gt;Continuing Tasks&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;상호작용이 자연스럽게 끝남&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;상호작용이 계속 유지됨&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;각각의 에피소드는 Terminal State 로 끝남&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;Terminal state 가 없음&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;에피소드는 상호독립적임&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + … + R_T$&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + …$ (무한)&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;예 ) 건물 안 온도조절기 : 환경과의 상호작용에 끝이 없음.
        &lt;ul&gt;
          &lt;li&gt;상태 : 시간, 건물 내 사람 수&lt;/li&gt;
          &lt;li&gt;액션 : 켜기, 끄기&lt;/li&gt;
          &lt;li&gt;보상 : 누군가가 온도를 수동조절 할 경우 -1, 그렇지 않은 경우 0&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;State 가 무한함에 따라 보상의 합이 무한으로 발산하는 것을 방지하기 위해 Discounting 을 한다.
        &lt;ul&gt;
          &lt;li&gt;Discounting 계수 $\gamma$ , $0 \leq \gamma &amp;lt; 1$&lt;/li&gt;
          &lt;li&gt;$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + … + \gamma^{k-1} R_{t+k} + … =  \sum_{k=0}^\infty\gamma^k R_{t+k+1}$&lt;/li&gt;
          &lt;li&gt;이것은 현재의 1 달러가 1년 뒤에 받을 1달러 보다 가치가 있다고 생각하면 이치에 맞는다.&lt;/li&gt;
          &lt;li&gt;$G_t$ 가 무한하지 않음 (Finite) 을 증명하는 수식
            &lt;ul&gt;
              &lt;li&gt;$G_t = \sum_{k=0}^\infty\gamma^k R_{t+k+1} \leq \sum_{k=0}^\infty\gamma^k R_{max} = R_{max} \sum_{k=0}^\infty\gamma^k = R_{max} \times {1 \over 1-\gamma}$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$\gamma$ 값에 따른 에이전트의 성향
            &lt;ul&gt;
              &lt;li&gt;$\gamma = 0$ : 즉각적인 보상만을 추구하는 에이전트 (Short-sighted)&lt;/li&gt;
              &lt;li&gt;$\gamma \to 1$ : 미래의 보상을 중시함 (Far-sighted)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Return 값의 재귀적 표현법
        &lt;ul&gt;
          &lt;li&gt;$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + … = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + … )$&lt;/li&gt;
          &lt;li&gt;$G_t = R_{t+1} + \gamma G_{t+1}$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Markov Decision Processes" /><summary type="html">Course Introduction</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 01. Week 1</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 01. Week 1" /><published>2023-01-26T18:00:00+09:00</published><updated>2023-01-26T18:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/">&lt;h2 id=&quot;course-introduction&quot;&gt;Course Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised Learning (지도학습)
    &lt;ul&gt;
      &lt;li&gt;학습자가 답이 기입되어 있는 라벨링된 예시에 접근함&lt;/li&gt;
      &lt;li&gt;정답이 무엇이었는지 말해주는 선생이 존재&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;학습데이터 : 정답 (혹은 정답인 동작) 을 가르쳐주는 정보를 학습한다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;피드백 : 취해야 할 올바른 조치를 나타낸다. (이번 행동이 얼마나 좋았는지는 알려주지 않는다.)&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;피드백은 행동에 대해 완전 독립적.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning (자율학습)
    &lt;ul&gt;
      &lt;li&gt;데이터 기저에 있는 구조를 추출 (데이터 표현법)
        &lt;ul&gt;
          &lt;li&gt;이 데이터 표현법은 지도학습이나 강화학습에 도움을 줄 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reinforcement Learning (강화학습)
    &lt;ul&gt;
      &lt;li&gt;학습자에게 최근의 행동에 대한 보상을 제공함&lt;/li&gt;
      &lt;li&gt;좋은행동이 어떤것일지 식별해주는 환경이 존재하나 정확히 어떻게 해야하는지는 알려주지 않음&lt;/li&gt;
      &lt;li&gt;지도학습이나 자율학습을 통해 일반화 (Input 이 달라져도 출력성능에 영향을 주지 않음) 를 개선할 수 있음&lt;/li&gt;
      &lt;li&gt;바뀌는 환경속에서 상호작용하며 학습하는 것에 주안점을 둠
        &lt;ul&gt;
          &lt;li&gt;학습자가 단순히 반복되는 환경에서 계산을 통해 좋은 행동이 무엇인지 학습하는 것이 아님.&lt;/li&gt;
          &lt;li&gt;학습자가 시행착오를 통해 변화하는 환경에서 목표를 바꾸어 가며 더 잘하는 것을 추구&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;학습데이터 : 행동을 수행한 것에대한 평가를 학습한다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;피드백 : 이번 행동이 얼마나 좋았는지를 피드백한다. (그것이 최선/최악인지는 알 수 없다)&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;피드백은 행동에 완전 종속됨.&lt;/li&gt;
          &lt;li&gt;때문에 최선의 행동을 찾아 끊임없이 탐험해야 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;관련-자료-rlbook2018-pages-25-36&quot;&gt;관련 자료 (RLbook2018 Pages 25-36)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;용어설명
    &lt;ul&gt;
      &lt;li&gt;Stationary (정상성) Probability : 여러 시간 구간마다 통계적 성질이 동일한 것
        &lt;ul&gt;
          &lt;li&gt;예) 주사위 던지기 : 시간에 무관하게 동일한 확률값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Non-Stationary (비정상성) Probability : 시간에 따라 통계적 성질이 변하는 것
        &lt;ul&gt;
          &lt;li&gt;예) 변덕스러운 날씨, 기후 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nonassociative setting : 다른 Action(행동), 다른 Environment(환경) 을 가정하지 않는 세팅
        &lt;ul&gt;
          &lt;li&gt;full reinforcement learning problem 의 복잡성을 배제 (다양한 환경, 비정상성, 행동의 보상이 즉각적이지 않음 - 현실세계)&lt;/li&gt;
          &lt;li&gt;이미 환경이 어떠한 피드백을 줄지 알고 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multi-armed Bandits
    &lt;ul&gt;
      &lt;li&gt;간단한 세팅 환경 (K-armed bandit problem 의 단순화 버전)
        &lt;ul&gt;
          &lt;li&gt;한가지 이상 상황에서의 동작을 학습하는 것을 배제 (Nonassociative setting)&lt;/li&gt;
          &lt;li&gt;목적1 : 평가 피드백 (Evaluative feedback) 이 정답을 알려주는 피드백 (Instructive Feedback) 과 어떻게 다른지 확인&lt;/li&gt;
          &lt;li&gt;목적2 : 둘이 결합될 수 있는지 확인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;목표
        &lt;ul&gt;
          &lt;li&gt;기초적 학습법 (Learning methods) 안내&lt;/li&gt;
          &lt;li&gt;이 Bandit problem 이 associative (연관성) 성질을 가지게 되었을 때 어떻게 변할지 확인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;A k-armed Bandit Problem
        &lt;ul&gt;
          &lt;li&gt;문제에 대한 설명
            &lt;ul&gt;
              &lt;li&gt;반복적으로 k 개의 다른 옵션/행동을 선택&lt;/li&gt;
              &lt;li&gt;각 선택 마다 보상 (선택한 행동에 따른 정상성 확률 분산을 가진 보상) 을 받는 문제&lt;/li&gt;
              &lt;li&gt;목적 : 특정 기간동안 최대한의 보상을 받는 것&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;문제에 대한 예시 1
            &lt;ul&gt;
              &lt;li&gt;슬롯머신 혹은 “one-armed bandit” 문제와 동일하나 단지 레버가 k 가임.&lt;/li&gt;
              &lt;li&gt;각 행동은 여러 대의 슬롯머신 중 한 대의 레버를 당기는 것과 동일&lt;/li&gt;
              &lt;li&gt;가장 이익을 극대화 할 수 있는 레버에 집중하여 보상을 많이 받는 것이 목표&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;문제에 대한 예시 2
            &lt;ul&gt;
              &lt;li&gt;의사가 심각한 질병의 환자에게 실험적 치료들 중 하나를 선택&lt;/li&gt;
              &lt;li&gt;보상 : 환자의 생존/ 치유율&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_1_1_value_of_action.png&quot; alt=&quot;2_1_1_value_of_action&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 문제를 풀었을 때 ($q_*$) 의 value of the action 에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;value of the action : 각각의 k 행동들이 가지는 기대/평균 보상값&lt;/li&gt;
              &lt;li&gt;$A_t$ : time-step $t$ 시점에 선택한 action&lt;/li&gt;
              &lt;li&gt;$R_t$ : $A_t$ 에 상응하는 보상&lt;/li&gt;
              &lt;li&gt;$q_*(a)$ : $a$ 가 선택되었을때 기대되는 보상값&lt;/li&gt;
              &lt;li&gt;$\doteq$ : is defined as&lt;/li&gt;
              &lt;li&gt;If you knew the “value of the each action” : 해당 문제를 풀었다고 볼 수 있음.&lt;/li&gt;
              &lt;li&gt;$Q_t(a)$ : $q_*(a)$ 와 유사한 값 (중간값)&lt;/li&gt;
              &lt;li&gt;$q_*$ 값은 에이전트가 알고 있는 값이 아님&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;greedy actions
            &lt;ul&gt;
              &lt;li&gt;action value 를 계속 추정하다 보면 어느 시점에서나 적어도 하나의 가장 큰 예측값을 가지는 action 이 존재&lt;/li&gt;
              &lt;li&gt;이것을 greedy actions 라고 함.&lt;/li&gt;
              &lt;li&gt;이 greedy action 을 선택하는 것 : 현재 알고 있는 values of the action 값을 exploiting 한다.&lt;/li&gt;
              &lt;li&gt;이 greedy action 을 선택하지 않는 것 : 현재 exploring 중이라고 한다.&lt;/li&gt;
              &lt;li&gt;Exploitation (이기적 이용)
                &lt;ul&gt;
                  &lt;li&gt;현 step 에서 예측되는 보상값을 최대화 하는 옳은 방법&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Exploration (탐색)
                &lt;ul&gt;
                  &lt;li&gt;장기 관점으로 보았을 때 이쪽의 보상 총합이 더 클 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Balancing exploration and exploitation
            &lt;ul&gt;
              &lt;li&gt;k-armed bandit 과 이와 유사한 문제의 특정 수학적 공식에 대해 탐색과 이용의 밸런스를 맞출 수 있는 정교한 방법이 존재
                &lt;ul&gt;
                  &lt;li&gt;하지만 이런 방법들은 정상성에대한 강한 가정을 전재하고, full reinforcement learning 환경이나 어플리케이션을 침해하거나  증명할 수 없는 사전지식을 이용한다.&lt;/li&gt;
                  &lt;li&gt;즉 활용 불가. (Full reinforcement learning 환경에서 이 밸런스 문제는 도전적인 과제임)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;k-armed bandit problem 에서는 균형에 대해서만 고려하고, 단순 Exploitation 하는 것보다 균형을 맞춘 방식이 더 잘 작동한다는 것을 증명할 것임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Action-value Methods
        &lt;ul&gt;
          &lt;li&gt;행동 선택을 위해 행동(Action) 에 대한 가치를 측정하는 방법을 통상 Action-value method 라 한다.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명 1&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_2_1_averaging_the_rewards.png&quot; alt=&quot;2_2_1_averaging_the_rewards&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 action-value method 중 보상평균값으로 추정하는 방식에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;Action-value 는 통상적으로 해당 Action 이 선택되었을 때 보상 값의 평균치를 말한다.&lt;/li&gt;
              &lt;li&gt;1(predictate) 는 행위를 하였을 때는 1, 아닌 경우 0 (가상의 1/0, 횟수의 개념)&lt;/li&gt;
              &lt;li&gt;분모가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.&lt;/li&gt;
              &lt;li&gt;이 방식을 sample-average method 라 부르며, 이는 Action-value 를 구하기 위한 많은 방법 중 하나이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명 2&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_2_2_greedy_action_selection_method.png&quot; alt=&quot;2_2_2_greedy_action_selection_method&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 greedy action selection method 에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;action selection 에서 가장 단순한 방법은 가장 큰 예측값에 해당하는 action을 선택하는 것이다.&lt;/li&gt;
              &lt;li&gt;현 시점 이전까지 가장 탐욕적인 action 으로 정의된 행동을 하는 것.&lt;/li&gt;
              &lt;li&gt;$\underset{a}{\arg\max}$ : 후술되는 값이 최대값이 되는 action $a$ 를 의미&lt;/li&gt;
              &lt;li&gt;Greedy action selection 은 현 지식을 이용해 당장의 보상을 최대화 하는 방식&lt;/li&gt;
              &lt;li&gt;정말 더 나은 방식을 찾기 위해 열등한 방식을 샘플링하는데 시간을 할애하지 않음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$\varepsilon$-greedy methods
            &lt;ul&gt;
              &lt;li&gt;위와 다른 대안으로 대부분 탐욕스러운 행동을 하되&lt;/li&gt;
              &lt;li&gt;아주 작은 확률($\varepsilon$)로 action-value 와 관계 없이 균등한 확률로 $a$ 를 선택하는 방법이 있음.&lt;/li&gt;
              &lt;li&gt;이 방법을 통한 샘플링 횟수가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.&lt;/li&gt;
              &lt;li&gt;이것은 점근적인 보장일 뿐, 실질적인 효과에 대한 것은 아니다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The 10-armed Testbed
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;조건 설명&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_3_1_10_armed_testbed.png&quot; alt=&quot;2_3_1_10_armed_testbed&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;10-armed testbed 의 bandit problem 문제 예시&lt;/li&gt;
          &lt;li&gt;각 10 개 action 의 true value $q_*(a)$ 값 은 평균 0, 분산 1 인 정규분포 (표준정규분포) 를 따른다.&lt;/li&gt;
          &lt;li&gt;위 조건으로 2000번의 서로 다른 bandit problem 을 수행, 평균 값을 취함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;결과&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_3_2_result_10_armed_testbed.png&quot; alt=&quot;2_3_2_result_10_armed_testbed&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;보상의 분산이 클수록 더 많은 탐험이 필요하며 $\varepsilon$-greedy methods 가 greedy methods 보다 더 잘 작동한다.&lt;/li&gt;
          &lt;li&gt;만약 보상의 분산이 0이라면 각각의 action 후에 true value 값을 바로 알 수 있게 된다.&lt;/li&gt;
          &lt;li&gt;위의 경우 greedy methods 가 더 잘 작동하게 된다. (탐험할 필요가 없음.)&lt;/li&gt;
          &lt;li&gt;그러나 이런 결정론적인 상황에서도 몇가지 가정이 불확실하다면 탐험하는 쪽이 유리하다.
            &lt;ul&gt;
              &lt;li&gt;비정상성 환경 (시간에 따라 true action-value 가 변함.)&lt;/li&gt;
              &lt;li&gt;즉 Reinforcement learning 은 탐색과 이용에 균형이 필요함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Incremental Implementation
    &lt;ul&gt;
      &lt;li&gt;action-value methods 를 획득한 보상의 평균 값으로 추정한다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이 경우 어떻게 전산화 하여 계산할지?&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_1_estimated_value_of_single_action.png&quot; alt=&quot;2_4_1_estimated_value_of_single_action&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;특정 단일 Action 에 대한 Action-value 를 예측, action 은 $n-1$ 번 선택됨.&lt;/li&gt;
          &lt;li&gt;위 명확한 계산법은 모든 이전 기록을 가지고 있어야 하고, 예측값이 필요할 때마다 계산해야함.
            &lt;ul&gt;
              &lt;li&gt;메모리가 많이 필요하고 연산량이 상당해진다.&lt;/li&gt;
              &lt;li&gt;위의 방법이 아니라 이전 평균값에서 이번 보상값을 업데이트 하는 방식을 취하는 것이 효율적이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_2_update_action_value.png&quot; alt=&quot;2_4_2_update_action_value&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 구현법을 이용하면 메모리는 $Q_n$ 과 $n$ 값만을 저장하고 있고, 작은 계산을 통해 매번 새로운 예측값을 구할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_3_update_action_value.png&quot; alt=&quot;2_4_3_update_action_value&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;수식의 뜻은 위와 같음&lt;/li&gt;
          &lt;li&gt;Target - OldEstimate = error
            &lt;ul&gt;
              &lt;li&gt;위 에러 값은 예측값이 Target 에 가까워질 수록 작아짐&lt;/li&gt;
              &lt;li&gt;Target 은 예측이 움직이기 원하는 방향을 가리킴&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;StepSize : 각 타임스텝마다 변하는 step-size parameter 이며, 이 책에서는 $\alpha$ 혹은 $\alpha_t(a)$ 로 나타낸다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_4_simple_bandit_algorithm.png&quot; alt=&quot;2_4_4_simple_bandit_algorithm&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;완성된 simple bandit algorithm&lt;/li&gt;
          &lt;li&gt;breaking ties randomly.. 동점 기록일 경우 랜덤하게 선택한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tracking a Nonstationary Problem
    &lt;ul&gt;
      &lt;li&gt;보상의 확률이 변하는 reinforcement learning 문제에서는 오랜 과거 보상보다 최근 보상에 비중을 더 두는 것이 설득력있다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이러한 방법 중 하나로 상수 파라미터 (a constant step-size parameter) 를 사용하는 것이 유명하다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_5_1_constant_step_size_parameter.png&quot; alt=&quot;2_5_1_constant_step_size_parameter&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 수식을 weighted average 라고도 하는데 각 가중치의 합이 1이 되기 때문이다.
            &lt;ul&gt;
              &lt;li&gt;$(1-\alpha)^n + \sum_{i=1}^n\alpha(1-\alpha)^{n-i} = 1$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$1-\alpha$ 값이 1보다 작기 때문에 승수가 커질수록 (이전 step 의 값일수록) 가중치 값이 decay 됨&lt;/li&gt;
          &lt;li&gt;때로는 위 수식을 exponential recency-weighted average 라고도 함 (지수적 최근성 가중치 평균)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;때로는 step 별로 변동하는 step-size parameter 를 사용하는 것이 편할 때가 있음.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 $\frac{1}{n}$ step-size parameter (sample-average method) 는 충분히 큰 step 을 진행할 경우 true action value 로 수렴하는 것을 보장한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;확률적 근사 이론은 확률 1로 수렴을 보장하는 데 필요한 조건을 제공한다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_5_2_conditions_required_to_convergence_with_prob1.png&quot; alt=&quot;2_5_2_conditions_required_to_convergence_with_prob1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;첫번째 조건은 초기 조건이나 무작위 변동을 극복할 수 있을 정도로 step-size 가 큰 것을 보장&lt;/li&gt;
          &lt;li&gt;두번째 조건은 step-size 가 수렴을 확신할 정도로 충분히 작은 것을 보장&lt;/li&gt;
          &lt;li&gt;$\frac{1}{n}$ 은 이 두 조건을 모두 만족하나, 상수 step 파라미터는 두번째 조건을 충족하지 않아 가장 최근의 보상값에 의해 완전히 수렴하지 못하게 됨.
            &lt;ul&gt;
              &lt;li&gt;이것은 비정상성 환경에서 필요한 내용이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;두 조건이 만족하더라도 매우 느리게 수렴하거나, 만족스러운 수렴율을 얻기 위해 파라미터를 튜닝해야 할 수도 있음.&lt;/li&gt;
          &lt;li&gt;위 이론은 이론적인 내용에는 자주 사용되나, 실제 적용 환경에서는 잘 사용되지 않음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic Initial Values
    &lt;ul&gt;
      &lt;li&gt;위에 언급한 모든 방법들은 initial action-value estimates ($Q_1(a)$) 에 어느정도 영향을 받는다.&lt;/li&gt;
      &lt;li&gt;통계적 표현으로 이러한 방식들은 biased by their initial estimates (초기추정치에 의해 편향된다) 라고 한다.&lt;/li&gt;
      &lt;li&gt;예를 들어 sample-average methods 는 이 편향이 모든 action을 한 번 이상씩 수행했을 때 사라진다면
        &lt;ul&gt;
          &lt;li&gt;액션 a=1 to k 에 대해 초기값 $Q(a)$, $N(a)$ 를 초기값으로 쓰고, 한 번이라도 a 가 시도되면 수식에 따른 값으로 변경&lt;/li&gt;
          &lt;li&gt;n 으로 나눌 때 0 으로 나눌 수 없으므로..&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;상수 a 를 사용하는 경우 이 편향은 영구적이다. (시간의 흐름에 따라 점차 감소하지만)
        &lt;ul&gt;
          &lt;li&gt;별도의 초기값 할당이 아니라 처음부터 수식을 사용하되, 수식 내 0번째 스텝의 값이 초기값임.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;장점 : 예상할 수 있는 보상 수준에 대한 사전지식을 제공하는 쉬운 방법&lt;/li&gt;
      &lt;li&gt;단점 : 모든 매개변수를 0 으로 설정하는 경우 사용자가 반드시 파라미터를 선택해야 한다.&lt;/li&gt;
      &lt;li&gt;간단한 탐색 장려의 방법 : 초기 값을 0 대신 +5 로 설정 (10-armed testbed 상황으로 가정)
        &lt;ul&gt;
          &lt;li&gt;초기값 +5 의 값은 매우 낙관적인 수치&lt;/li&gt;
          &lt;li&gt;특정 action 을 선택하고 받는 보상 값이 예측치보다 작음 (disappointed with the rewards)&lt;/li&gt;
          &lt;li&gt;학습자는 다른 action 을 선택하게 되고 이 상황을 몇번 반복됨. (greedy action 일지라도…)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_6_1_optimistic_greedy.png&quot; alt=&quot;2_6_1_optimistic_greedy&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;10-armed bandit testbed 에서 greedy method 를 초기값 $Q_1(a) = +5$ 로 세팅한 결과&lt;/li&gt;
          &lt;li&gt;비교군은 $\varepsilon$-greedy method 에 초기값 $Q_1(a) = 0$&lt;/li&gt;
          &lt;li&gt;이 trick 은 stationary problem 에서 꽤 효과적이나, 탐색을 장려하는 일반적인 방법은 아님.&lt;/li&gt;
          &lt;li&gt;이러한 비판은 sample-average methods 에서도 통용된다.
            &lt;ul&gt;
              &lt;li&gt;초기 시점을 특수한 이벤트로 여긴다.&lt;/li&gt;
              &lt;li&gt;모든 보상을 똑같은 가중치로 평균을 구한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upper-Confidence-Bound Action Selection
    &lt;ul&gt;
      &lt;li&gt;action-value 추정값의 불확실성 때문에 탐험은 반드시 필요하다.&lt;/li&gt;
      &lt;li&gt;greedy actions 는 현재 시점에는 가장 최적의 선택이나 다른 action 이 실제로는 더 좋은 것일 수 있다.&lt;/li&gt;
      &lt;li&gt;$\varepsilon$-greedy action selection 은 강제적으로 non-greedy action 을 선택하지만 선호도 없이 무차별적인 선택을 하여 greedy 한 선택을 하게 될 수도 있다.
        &lt;ul&gt;
          &lt;li&gt;강제적인 선택을 할 때는 non-greedy 한 선택을 하는 것이 좋음&lt;/li&gt;
          &lt;li&gt;추정치가 최대치에 얼마나 가까운지와 추정치의 불확실성을 모두 고려&lt;/li&gt;
          &lt;li&gt;실제로 최적일 가능성에 따라 탐욕스럽지 않은 action 을 선택하는 것이 좋음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_7_1_UCB.png&quot; alt=&quot;2_7_1_UCB&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;UCB (Upper-Confidence-Bound) Action Selection 은 그러한 효율적인 방식 중 하나이다.&lt;/li&gt;
          &lt;li&gt;$N_t(a)$ 는 t step 이 진행되었을때 a action 이 선택된 횟수로, 많이 선택될 수록 우항의 피연산자 값이 작아진다.&lt;/li&gt;
          &lt;li&gt;$\ln t$ 는 step 이 커짐에 따라 값이 무한대까지 증가 (수렴하지 않음) 하나 그 증가폭이 서서히 줄어든다&lt;/li&gt;
          &lt;li&gt;$c$ 는 탐험의 정도 (강도) 를 나타내는 수치이다.&lt;/li&gt;
          &lt;li&gt;즉 action이 많이 선택될수록 $Q_t(a)$ 의 값은 정확해지고, 우항의 피연산자 값은 작아진다.&lt;/li&gt;
          &lt;li&gt;action 이 한번도 선택되지 않을 경우 해당 a 를 maximizing action 으로 여긴다. (해당 a 에 대한 무조건적인 탐험)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_7_2_UCB.png&quot; alt=&quot;2_7_2_UCB&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;이러한 UCB 방식은 10-armed testbed 에서 $\varepsilon$-greedy 보다 나은 성과를 보여주기도 한다.&lt;/li&gt;
          &lt;li&gt;단 몇가지 단점으로 인해 실용적이지 않은 방식이다.
            &lt;ul&gt;
              &lt;li&gt;bandits 문제에서 reinforcement learning 문제로 확장하기 어렵다.&lt;/li&gt;
              &lt;li&gt;nonstationary 한 문제들에는 더 어려운 방식의 action-value method 가 필요하다.&lt;/li&gt;
              &lt;li&gt;훨신 더 거대한 환경 (state space) 에서의 적용 (특히 뒤에 배울 function approximation 방식) 이 어렵다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-1-the-k-armed-bandit-problem&quot;&gt;Lesson 1: The K-Armed Bandit Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define reward&lt;/li&gt;
      &lt;li&gt;Understand the temporal nature of the bandit problem&lt;/li&gt;
      &lt;li&gt;Define k-armed bandit&lt;/li&gt;
      &lt;li&gt;Define action-values&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sequential Decision Making with Evaluative Feedback
    &lt;ul&gt;
      &lt;li&gt;불확실성 아래에서의 의사결정
        &lt;ul&gt;
          &lt;li&gt;의사가 3가지 처방약으로 환자에게 실험적 처방을 할 때…&lt;/li&gt;
          &lt;li&gt;몇번의 환자 반응을 보고 가장 잘 듣는 약을 고집할 경우
            &lt;ul&gt;
              &lt;li&gt;더이상 다른 약의 데이터를 모을 수 없음&lt;/li&gt;
              &lt;li&gt;나머지 두 약이 실제로 나음에도 몇몇 결과가 나쁘게 나왔는지 알 수 없음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;다른 약으로 계속 실험할 경우
            &lt;ul&gt;
              &lt;li&gt;나쁜 결과를 계속 초래할 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습에서의 기초적 컨셉 용어
        &lt;ul&gt;
          &lt;li&gt;에이전트 (Agent) : action을 선택하는 존재 - 의사&lt;/li&gt;
          &lt;li&gt;액션 (Action) : 선택지 - 3가지 약 중 하나를 선택하는 것&lt;/li&gt;
          &lt;li&gt;보상 (Rewards) : 액션에 대한 결과 - 환자의 상태&lt;/li&gt;
          &lt;li&gt;값 (Value Function - Action-Value (Function)) : 기대되는 보상 값 - 환자의 혈압 값
            &lt;ul&gt;
              &lt;li&gt;에이전트가 액션을 선택했을 때 그 값 (Action-Value = $q_*$)이 최대화 할 경우 목적을 달성함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$q_*(a)$ 값 구하기
        &lt;ul&gt;
          &lt;li&gt;각 약의 결과값이 서로 다른 확률분포를 가졌을 경우 $q_*$ 는 각 분포의 평균이 될 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bandits Problem 을 고려하는 이유
        &lt;ul&gt;
          &lt;li&gt;문제와 알고리즘 디자인 선택에 있어 가장 간단한 세팅&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-2-what-to-learn-estimating-action-values&quot;&gt;Lesson 2: What to Learn? Estimating Action Values&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define action-value estimation methods&lt;/li&gt;
      &lt;li&gt;Define exploration and exploitation&lt;/li&gt;
      &lt;li&gt;Select actions greedily using an action-value function&lt;/li&gt;
      &lt;li&gt;Define online learning&lt;/li&gt;
      &lt;li&gt;Understand a simple online sample-average action-value estimation method&lt;/li&gt;
      &lt;li&gt;Define the general online update equation&lt;/li&gt;
      &lt;li&gt;Understand why we might use a constant step-size in the case of non-stationarity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning Action Values
    &lt;ul&gt;
      &lt;li&gt;Estimate action values using the sample-average method
        &lt;ul&gt;
          &lt;li&gt;의사가 처방 후 환자가 나아질 경우 1, 그렇지 않은 경우 0 으로 표기하고 3개의 약을 처방해 평균을 구함&lt;/li&gt;
          &lt;li&gt;Step 이 많이 진행될 수록 평균 데이터는 더 정확해짐&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Describe greedy action selection
        &lt;ul&gt;
          &lt;li&gt;의시가 해당 시점에 가장 기대치가 큰 약을 처방할 경우 이 행동을 Greedy action 이라고 함&lt;/li&gt;
          &lt;li&gt;greedy action 을 선택하는 것을 현 지식을 활용한 이용 (exploitation) 이라고 함&lt;/li&gt;
          &lt;li&gt;당장의 기대되는 보상을 포기하고 다른 선택을 하는 것을 non-greedy action 이라고 하고 이를 탐험 (exploration) 이라고 함&lt;/li&gt;
          &lt;li&gt;탐험을 통해 기대되는 보상을 희생하고 non-greedy action 의 보상에 대한 더 많은 정보를 얻게 됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Introduce the exploration-exploitation dilemma
        &lt;ul&gt;
          &lt;li&gt;에이전트는 동시에 탐험과 이용을 할 수 없음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Estimating Action Values Incrementally
    &lt;ul&gt;
      &lt;li&gt;action value 의 점진적 표현법 (sample-average method 를 이용)
        &lt;ul&gt;
          &lt;li&gt;Incremental update rule&lt;/li&gt;
          &lt;li&gt;NewEstimate &amp;lt;- OldEstimate + StepSize(Target - OldEstimate)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;점진적 업데이트 룰(Incremental update rule)이 더 널리 쓰이는 이유
        &lt;ul&gt;
          &lt;li&gt;모든 이전 값들을 기억할 필요가 없다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보편적 업데이트 룰을 non-stationary bandit problem 에 적용하는 방법
        &lt;ul&gt;
          &lt;li&gt;Non-stationary Bandit Problem : 의사의 3가지 약중 특정 하나의 약이 겨울이 되면 효율이 높아진다.&lt;/li&gt;
          &lt;li&gt;보상의 분포가 시간에 따라 변하게 되는 경우를 Non-stationary 하다 라고 함.&lt;/li&gt;
          &lt;li&gt;StepSize 파라미터가 상수 (예:0.1) 일 경우 이전 Step 일수록 영향도가 작아지고, 최신 Step의 보상값을 더 반영함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-3-exploration-vs-exploitation-tradeoff&quot;&gt;Lesson 3: Exploration vs. Exploitation Tradeoff&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define epsilon-greedy&lt;/li&gt;
      &lt;li&gt;Compare the short-term benefits of exploitation and the long-term benefits of exploration&lt;/li&gt;
      &lt;li&gt;Understand optimistic initial values&lt;/li&gt;
      &lt;li&gt;Describe the benefits of optimistic initial values for early exploration&lt;/li&gt;
      &lt;li&gt;Explain the criticisms of optimistic initial values&lt;/li&gt;
      &lt;li&gt;Describe the upper confidence bound action selection method&lt;/li&gt;
      &lt;li&gt;Define optimism in the face of uncertainty&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What is the trade-off?
    &lt;ul&gt;
      &lt;li&gt;exploration-exploitation 의 등가교환
        &lt;ul&gt;
          &lt;li&gt;Exploration (탐색) : 장기적 이익을 위해 지식을 늘림&lt;/li&gt;
          &lt;li&gt;Exploitation (이용) : 단기적 이익을 위해 지식을 이용&lt;/li&gt;
          &lt;li&gt;탐색만 하면 단기적 보상이 작아지고, 이용만 하면 타 선택지의 true value 를 모르기 때문에 장기적으로 손해일 수 있음&lt;/li&gt;
          &lt;li&gt;한번의 선택에서 탐색 과 이용 둘 중 하나만 가능&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Epsilon-Greedy 방식 (탐색과 이용의 균형을 맞추는 쉬운 방법)
        &lt;ul&gt;
          &lt;li&gt;Epsilon-Greedy 는 대부분 Exploitation (Greedy method) 하고, 적은 확률로 Exploration (Random choice) 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;각 방식을 비교할 때 한 번의 진행으로는 노이즈가 많아 확인이 어려움.
        &lt;ul&gt;
          &lt;li&gt;1000 개의 에이전트로 보상 데이터를 모아 평균값으로 비교&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic Initial Values
    &lt;ul&gt;
      &lt;li&gt;Optimistic initial values 가 초기 탐색을 장려하는 이유
        &lt;ul&gt;
          &lt;li&gt;초기 예상 보상치를 높게 잡고 시작&lt;/li&gt;
          &lt;li&gt;첫 선택의 보상이 주어지더라도 평균값으로 인해 값이 떨어짐&lt;/li&gt;
          &lt;li&gt;에이전트는 첫 선택에 실망을 하고 선택되지 않은 다른 높은 초기치의 옵션 중 하나를 선택&lt;/li&gt;
          &lt;li&gt;초기 탐험을 통해 모든 Action 을 골고루 선택하게 됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Optimistic initial values 의 한계
        &lt;ul&gt;
          &lt;li&gt;초기 단계에서만 탐색을 진행한다. 이는 Non-stationary Problems 에서 문제가 됨.&lt;/li&gt;
          &lt;li&gt;Maximal Reward 를 시작하기 전 알 방법이 없기에 Optimistic Initial Value 를 어느정도로 설정해야 할지 모른다.&lt;/li&gt;
          &lt;li&gt;그럼에도 Optimistic initial values 는 휴리스틱 (충분한 정보 없이 빠르게 사용할 수 있는 직관적인) 한 방법으로 자주 활용됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upper-Confidence Bound(UCB) Action Selection
    &lt;ul&gt;
      &lt;li&gt;UCB action-selection 방식이 예측의 불확실성을 이용해 탐색을 유도하는 방법&lt;/li&gt;
      &lt;li&gt;UCB의 C는 confidence 를 뜻하며, 예측한 $Q(a)$ 값의 오차범주 범위 $(c)$ 내에 실제 값이 들어올 것이라 확신하는 정도의 수치이다.
        &lt;ul&gt;
          &lt;li&gt;범위가 작을수록 결과에 더욱 확신한다는 뜻임&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;불확실성 (범위) 에 마주했을때 가장 높은 값 (Upper-Confidence Bound) 을 선택&lt;/li&gt;
      &lt;li&gt;수식에 대한 풀이
        &lt;ul&gt;
          &lt;li&gt;c : 사용자 정의 파라미터 (얼마나 탐험을 할 건지 컨트롤)&lt;/li&gt;
          &lt;li&gt;좌측 피연산자 : Exploit (이용)&lt;/li&gt;
          &lt;li&gt;우측 피연산자 : Explore (탐험)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Contextual Bandits for Real World Reinforcement Learning
    &lt;ul&gt;
      &lt;li&gt;Contextual Bandits 는 Reinforcement Learning 이 현실에 배포되는 방식임&lt;/li&gt;
      &lt;li&gt;Reinforcement 는 현실과 어떻게 다른가?
        &lt;ul&gt;
          &lt;li&gt;Reinforcement Learning 은 시뮬레이터로 동작한다.
            &lt;ul&gt;
              &lt;li&gt;시뮬레이터는 관측치를 제공&lt;/li&gt;
              &lt;li&gt;Learning 알고리즘은 어떤 Action을 선택할 지 정책을 가진다.&lt;/li&gt;
              &lt;li&gt;시뮬레이터는 실행하고 보상을 제공한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;현실이 제공하는 관측값은 시뮬레이터와 다르다.
            &lt;ul&gt;
              &lt;li&gt;같은 정책이라 해도 관측치가 다르기 때문에 시뮬레이터에서의 Action 과 다른 Action 을 수행하게 된다.&lt;/li&gt;
              &lt;li&gt;시뮬레이터와 현실은 보상도 다르다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉 현실과 시뮬레이터에는 갭이 존재한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Real World based Reinforcement Learning : 현실 기반 강화학습을 하려면 우선순위를 변경해야 함
        &lt;ul&gt;
          &lt;li&gt;Temporal Credit Assignment &amp;lt; Generalization
            &lt;ul&gt;
              &lt;li&gt;Temporal Credit Assignment Problem (CAP) : 시간적 기여도 할당문제. 일련의 행동이 모두 끝난 뒤 보상을 얻을 수 있는 환경에서 수행한 행동들 중 어떠한 행동이 기여도가 있고 어떠한 행동이 벌점을 줄 것인지 결정하는 문제.&lt;/li&gt;
              &lt;li&gt;Generalization : 다양한 관찰값을 통한 일반화&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Control environment &amp;lt; Environment controls
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션 환경에서는 자유자재로 한 스텝을 더 진행하거나 할 수 있다면 (컨트롤 가능)&lt;/li&gt;
              &lt;li&gt;현실에서는 환경이 지배함. (환경에 의해 컨트롤당함)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Computational efficiency &amp;lt; Statistical efficiency
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션 환경에서 계산효율이 중요하다면 (학습할 샘플이 많으므로)&lt;/li&gt;
              &lt;li&gt;현실에서는 통계적 효율이 중요함 (현실이 주는 샘플만 가질 수 있다.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;State &amp;lt; Features
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션에서는 상태를 생각 (상태값은 결정을 내리는 데 기반이 되는 요소)&lt;/li&gt;
              &lt;li&gt;현실에서는 매우 복잡한 관측값을 가지는 경우 (필요한 것보다 정보가 많음) 가 많아 어느 것이 핵심 요인인지가 중요&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Learning &amp;lt; Evaluation
            &lt;ul&gt;
              &lt;li&gt;현실에서는 “Off Policy Evaluation” 이 중요하다.
                &lt;ul&gt;
                  &lt;li&gt;Off Policy : 정책 업데이트에 어떤 데이터를 써도 상관이 없는 경우&lt;/li&gt;
                  &lt;li&gt;즉, 최신 업데이트 정책에서 수집된 데이터가 아니어도 사용가능&lt;/li&gt;
                  &lt;li&gt;학습 알고리즘이 학습 뿐만 아니라 정책 평가에 쓰일 수 있는 부산물 데이터 또한 제공&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Last policy &amp;lt; Every Policy
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션에서는 가장 최신의 정책이 중요&lt;/li&gt;
              &lt;li&gt;현실에서는 모든 포인트의 데이터가 세상과의 어느 정도 상호작용이 포함되어 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-summary-rlbook2018-pages-42-43&quot;&gt;Chapter Summary (RLbook2018 Pages 42-43)&lt;/h2&gt;

&lt;h2 id=&quot;weekly-assessment&quot;&gt;Weekly Assessment&lt;/h2&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">Course Introduction</summary></entry><entry><title type="html">Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020</title><link href="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/" rel="alternate" type="text/html" title="Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020" /><published>2023-01-26T11:00:00+09:00</published><updated>2023-01-26T11:00:00+09:00</updated><id>https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020</id><content type="html" xml:base="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/">&lt;h2 id=&quot;1-구매정보&quot;&gt;1. 구매정보&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;빈티지 : 2020&lt;/li&gt;
  &lt;li&gt;가격 : 40,000 원 (할인 및 페이백 후 추정가)&lt;/li&gt;
  &lt;li&gt;구매처 : 편의점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-이미지&quot;&gt;2. 이미지&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/Textbook_Napa_Cabernet_Sauvignon.jpg&quot; alt=&quot;Textbook_Napa_Cabernet_Sauvignon.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;about:blank&quot; target=&quot;_blank&quot;&gt;링크없음&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-감상&quot;&gt;3. 감상&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;1일차
    &lt;ul&gt;
      &lt;li&gt;레드 와인&lt;/li&gt;
      &lt;li&gt;향이 강하지 않음&lt;/li&gt;
      &lt;li&gt;맛이 굉장히 가벼운 느낌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-총점&quot;&gt;3. 총점&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;3/5
    &lt;ul&gt;
      &lt;li&gt;많은 기대를 한 와인이었으나, 기대에 못미침&lt;/li&gt;
      &lt;li&gt;가볍고 경쾌한 느낌이어서 많이 마실 수 있었음&lt;/li&gt;
      &lt;li&gt;하지만 이번 와인을 계기로 난 와인보다 양주나 정종이 더 맞다는 결론을 내림.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Wine" /><category term="와인" /><summary type="html">1. 구매정보 빈티지 : 2020 가격 : 40,000 원 (할인 및 페이백 후 추정가) 구매처 : 편의점</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;강화학습의 기반 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 분석 학습을 위한 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 04. Real-world time series data</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 04. Real-world time series data" /><published>2022-11-07T14:00:00+09:00</published><updated>2022-11-07T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/">&lt;h1 id=&quot;real-world-time-series-data&quot;&gt;Real-world time series data&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;실사용 데이터 (태양의 흑점활동) 이용하기&lt;/li&gt;
  &lt;li&gt;Conv1D, LSTM, DNN 을 결합한 모델을 활용할 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions&quot;&gt;Convolutions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, // 32개의 필터를 학습할 1D Conv
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9)

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bi-directional-lstms&quot;&gt;Bi-directional LSTMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), // LSTM 의 입력값을 재구성하는 Lambda 레이어를 없앰
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500) // epoch 늘리기
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;windowed_dataset 헬퍼 함수&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):

          ds = tf.data.Dataset.from_tensor_slices(series)
          ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
          ds = ds.flat_map(lambda w: w.batch(window_size + 1))
          ds = ds.shuffle(shuffle_buffer)
          ds = ds.map(lambda w: (w[:-1], w[-1]))

          return ds.batch(batch_size).prefetch(1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bi-directional LSTM&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), 
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), // 양방향
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;결과수치는 긍정적이나 검증세트에 예측을 플로팅 해보면 과적합이 보여 일부 파라미터에 변화를 줄 필요가 있음&lt;/li&gt;
      &lt;li&gt;MAE 로 손실을 플로팅하면 문제점을 확인할 수 있음
        &lt;ul&gt;
          &lt;li&gt;스파이크 현상은 배치 크기가 작아서 무작위 노이즈가 많기 때문임&lt;/li&gt;
          &lt;li&gt;배치사이즈를 줄이거나 늘이는 것에 따라 학습과 예측이 달라짐&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-batch-sizing&quot;&gt;More on batch sizing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;신경망을 더 빠르게 학습하도록 하는 최적화 알고리즘
    &lt;ul&gt;
      &lt;li&gt;머신러닝 : 잘 작동되는 모델을 찾기 위해 많은 훈련을 거쳐야 하는 반복적인 과정
        &lt;ul&gt;
          &lt;li&gt;모델을 빠르게 훈련시키는 것이 매우 중요함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;딥러닝은 빅데이터에서 가장 잘 작동됨 -&amp;gt; 훈련이 어려움 (큰 데이터 세트에서 훈련하는 것은 매우 느린과정)&lt;/li&gt;
      &lt;li&gt;좋은 최적화 알고리즘을 찾는 것은 효율성을 향상시켜준다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;미니배치 경사 하강법
    &lt;ul&gt;
      &lt;li&gt;벡터화 : m개의 샘플에 대한 계산을 효율적으로 만들어줌. 명시적인 반복문 없이도 훈련 세트를 진행할 수 있도록 함.&lt;/li&gt;
      &lt;li&gt;훈련 샘플을 받아서 큰 벡터에 저장함
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^m]&lt;/code&gt; : shape : (n_x,m)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = [y^1, y^2, ..., y^m]&lt;/code&gt; : shape : (1,m)&lt;/li&gt;
          &lt;li&gt;하지만 m 의 수치가 크면 여전히 학습은 느리다.&lt;/li&gt;
          &lt;li&gt;예를들어 m 의 수치가 500만 이라면?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;전체 훈련 세트에 대한 경사 하강법을 구현하면 경사 하강법의 작은 한 단계를 밟기 전에 모든 훈련 세트를 처리해야 함
        &lt;ul&gt;
          &lt;li&gt;즉, 경사하강법의 다음 단계를 밟기 전에 500만 개의 전체 훈련 샘플을 처리해야 함&lt;/li&gt;
          &lt;li&gt;500만 개의 전체 훈련 샘플을 모두 훈련하기 전에 경사 하강법이 진행되도록 하면 더 빠른 알고리즘을 얻을 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;훈련 세트를 더 작은 훈련세트 (미니배치) 로 나눔
        &lt;ul&gt;
          &lt;li&gt;mini-batch 가 1000개의 샘플을 갖는다고 가정&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^1000 | x^1001, ..., x^2000 | ... | ... x^m]&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = X^{1},                   X^{2}, ... ,               X^{5000}&lt;/code&gt; : shape : (n_x, 1000)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = Y^{1},                   Y^{2}, ... ,               Y^{5000}&lt;/code&gt; : shape : (1, 1000)&lt;/li&gt;
          &lt;li&gt;Mini-batch t : X^{t}, Y^{t}&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;표기법 정의
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x^(i)&lt;/code&gt; : i 번째 훈련 샘플&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z^[l]&lt;/code&gt; : l 번째 신경망의 z 값&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X^{t}&lt;/code&gt; : t 번째 미니배치 X&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Batch Gradient Descent : 일반적인 경사하강법, 모든 훈련 세트를 동시에 훈련시킴, 훈련 샘플의 모든 배치를 진행시킨다는 관점&lt;/li&gt;
      &lt;li&gt;미니배치 : 전체 훈련 세트 X,Y 를 한번에 진행시키지 않고, 하나의 미니배치 X^{t}, Y^{t} 를 동시에 진행시키는 알고리즘
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for t=1 ,..., 5000 : 총 미니배치의 수 (5000개)
      // 1 step of gradient descent using X^{t}, Y^{t}
      // (as if m=1000)
      // 모든 1000개의 샘플에 대해 명시적인 반복문을 갖는 것보다 벡터화를 사용해 모든 1000개의 샘플을 동시에 진행함
      Forward prop on X^{t}
          Z^[1] = W^[1] * X^{t} + b^[1] : Vectorized Implementation (1000 examples)
          A^[1] = g^[1] * (Z^[1]) : Vectorized Implementation (1000 examples)
          ...
          A^[l] = g^[l] * (Z^[l]) : Vectorized Implementation (1000 examples)
      Compute cost J^{t} = 1/1000 * Sum (( i = 1 to l) Loss(expect(y^(i)), y^(i)))
          + 정규화 항
      Backprop to compute Gradients cost J^(t) using X^{t}, Y^{t}
          W^[l] = W^[l] - adW^[l], b^[l] = b^[l] - adb^[l]
      ...
      1 epoch : pass through training set (5000 개의 경사하강단계)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm-notebook&quot;&gt;Convolutions with LSTM notebook&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm&quot;&gt;Convolutions with LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;real-data---sunspots&quot;&gt;Real data - sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;소스 내려받기 : 케글에서 내려받거나, 이번학습을 위한 데이터 제공 저장소를 사용 (후자)&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv -O /tmp/sunspots.csv&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CSV 읽기&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      import csv
      time_step = []
      sunspots = []

      with open(&apos;/tmp/sunspots.csv&apos;) as csvfile:
          reader = csv.reader(csvfile, delimiter=&apos;,&apos;)
          next(reader)
          for row in reader :
              sunspots.append(float(row[2]))
              time_step.append(int(row[0]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;numpy 배열로 전환&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      series = np.array(sunspots) // numpy 에 항목을 추가할 때마다 목록을 복제하는데,
      time = np.array(time_step) // 메모리 관리 과정이 많이 진행되기 때문에 데이터 양이 많으면 느려질 수 있음
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;시계열을 훈련 및 검증 데이터 세트로 분할&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      split_time = 1000
      time_train = time[:split_time]
      x_train = series[:split_time]
      time_valid = time[split_time:]
      x_valid = series[split_time:]

      window_size = 20
      batch_size = 32
      shuffle_buffer_size = 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이전 windowed_dataset 함수와 동일 코드를 사용&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
          dataset = tf.data.Dataset.from_tensor_slices(series)
          dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
          dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
          dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
          dataset = dataset.batch(batch_size).prefetch(1)
          return dataset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-and-tune-the-model&quot;&gt;Train and tune the model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;이전 수업에서 배웠던 모델로 예측하여 플로팅하면, 결과는 괜찮아보이나 MAE 가 매우 큼&lt;/li&gt;
  &lt;li&gt;이는 이전 window_size 가 20 (여기서는 약 2년이 안되는 시간) 이나, 사실 흑점 데이터의 주기는 11년 혹은 22년으로 추정됨&lt;/li&gt;
  &lt;li&gt;window_size 를 11년에 해당하는 132로 두고 다시 훈련을 하면 차트는 더 잘 나오나 MAE는 더 커짐
    &lt;ul&gt;
      &lt;li&gt;데이터를 되돌아보면 11년 주기의 계절성을 갖지만 창 안에 계절 전체가 있어야 할 필요는 없음&lt;/li&gt;
      &lt;li&gt;플롯을 확대해보면 전형적인 시계열 형태 데이터임&lt;/li&gt;
      &lt;li&gt;나중에 오는 값이 앞선 값과 연관이 있지만 노이즈가 많음&lt;/li&gt;
      &lt;li&gt;그래서 훈련 시에 창 크기가 클 필요는 없을 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;data 의 분할을 1000을 훈련, 2500을 검증으로 설정하였는데 이는 좋지 못한 분할임
    &lt;ul&gt;
      &lt;li&gt;3500과 500으로 지정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망 설계와 파라미터의 크기 변경
    &lt;ul&gt;
      &lt;li&gt;10, 10, 1 레이어 를 30, 15, 1 로 값을 바꿔서 훈련 (입력 Shape 값이 30)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예측
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.predict(series[3205:3235][np.newaxis])&lt;/code&gt; : 7.077 개의 흑점 예상 (실 데이터 8.7 개)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;설정 변경 : MAE 13.7&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  split_time = 3000
  window_size = 60
	
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(20, input_shape=[window_size], activation=&quot;relu&quot;),
      tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
      tf.keras.layers.Dense(1)
  ])
	
  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;8.13 예측 (실제값 8.7)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots-notebooks-lab-2--lab-3&quot;&gt;Sunspots notebooks (Lab 2 &amp;amp; Lab 3)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 으로 대체&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots&quot;&gt;Sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;입력 창의 크기를 60으로 함&lt;/li&gt;
  &lt;li&gt;DNN 을 Dense 20, 10, 1 로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;combining-our-tools-for-analysis&quot;&gt;Combining our tools for analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      window_size = 60
      batch_size = 64
      train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=&quot;casual&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(30, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 400)
      ])

      lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch / 20))
      optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-8, momentum=0.9)
      model.compile(loss=tf.keras.losses.Huber(), optimizer = optimizer, metrics=[&quot;mae&quot;])
      history = model.fit(train-set, epochs=100, callbacks=[lr_schedule])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;batch_size 의 변경 (256) : loss 에 노이즈가 생길 경우 고려해볼 파라미터&lt;/li&gt;
  &lt;li&gt;하이퍼파라미터를 다양하게 실험해 봐야 함&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Real-world time series data</summary></entry></feed>