<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://bluesplatter.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bluesplatter.com/" rel="alternate" type="text/html" /><updated>2023-02-07T17:40:04+09:00</updated><id>https://bluesplatter.com/feed.xml</id><title type="html">Bluesplatter</title><subtitle>전문적이지 않은 정보들, 감상, 즉흥적인 내용들</subtitle><author><name>HY03</name><email>hyunik03@gmail.com</email></author><entry><title type="html">Fundamentals of Reinforcement Learning - 01. Week 1</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 01. Week 1" /><published>2023-01-26T18:00:00+09:00</published><updated>2023-01-26T18:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/">&lt;h2 id=&quot;course-introduction&quot;&gt;Course Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised Learning (지도학습)
    &lt;ul&gt;
      &lt;li&gt;학습자가 답이 기입되어 있는 라벨링된 예시에 접근함&lt;/li&gt;
      &lt;li&gt;정답이 무엇이었는지 말해주는 선생이 존재&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;학습데이터 : 정답 (혹은 정답인 동작) 을 가르쳐주는 정보를 학습한다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;피드백 : 취해야 할 올바른 조치를 나타낸다. (이번 행동이 얼마나 좋았는지는 알려주지 않는다.)&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;피드백은 행동에 대해 완전 독립적.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning (자율학습)
    &lt;ul&gt;
      &lt;li&gt;데이터 기저에 있는 구조를 추출 (데이터 표현법)
        &lt;ul&gt;
          &lt;li&gt;이 데이터 표현법은 지도학습이나 강화학습에 도움을 줄 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reinforcement Learning (강화학습)
    &lt;ul&gt;
      &lt;li&gt;학습자에게 최근의 행동에 대한 보상을 제공함&lt;/li&gt;
      &lt;li&gt;좋은행동이 어떤것일지 식별해주는 환경이 존재하나 정확히 어떻게 해야하는지는 알려주지 않음&lt;/li&gt;
      &lt;li&gt;지도학습이나 자율학습을 통해 일반화 (Input 이 달라져도 출력성능에 영향을 주지 않음) 를 개선할 수 있음&lt;/li&gt;
      &lt;li&gt;바뀌는 환경속에서 상호작용하며 학습하는 것에 주안점을 둠
        &lt;ul&gt;
          &lt;li&gt;학습자가 단순히 반복되는 환경에서 계산을 통해 좋은 행동이 무엇인지 학습하는 것이 아님.&lt;/li&gt;
          &lt;li&gt;학습자가 시행착오를 통해 변화하는 환경에서 목표를 바꾸어 가며 더 잘하는 것을 추구&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;학습데이터 : 행동을 수행한 것에대한 평가를 학습한다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;피드백 : 이번 행동이 얼마나 좋았는지를 피드백한다. (그것이 최선/최악인지는 알 수 없다)&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;피드백은 행동에 완전 종속됨.&lt;/li&gt;
          &lt;li&gt;때문에 최선의 행동을 찾아 끈임없이 탐험해야 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;관련-자료-rlbook2018-pages-25-36&quot;&gt;관련 자료 (RLbook2018 Pages 25-36)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;용어설명
    &lt;ul&gt;
      &lt;li&gt;Stationary (정상성) Probability : 여러 시간 구간마다 통계적 성질이 동일한 것
        &lt;ul&gt;
          &lt;li&gt;예) 주사위 던지기 : 시간에 무관하게 동일한 확률값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Non-Stationary (비정상성) Probability : 시간에 따라 통계적 성질이 변하는 것
        &lt;ul&gt;
          &lt;li&gt;예) 변덕스러운 날씨, 기후 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nonassociative setting : 다른 Action(행동), 다른 Environment(환경) 을 가정하지 않는 세팅
        &lt;ul&gt;
          &lt;li&gt;full reinforcement learning problem 의 복잡성을 배제 (다양한 환경, 비정상성, 행동의 보상이 즉각적이지 않음 - 현실세계)&lt;/li&gt;
          &lt;li&gt;이미 환경이 어떠한 피드백을 줄지 알고 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multi-armed Bandits
    &lt;ul&gt;
      &lt;li&gt;간단한 세팅 환경 (K-armed bandit problem 의 단순화 버전)
        &lt;ul&gt;
          &lt;li&gt;한가지 이상 상황에서의 동작을 학습하는 것을 배제 (Nonassociative setting)&lt;/li&gt;
          &lt;li&gt;목적1 : 평가 피드백 (Evaluative feedback) 이 정답을 알려주는 피드백 (Instructive Feedback) 과 어떻게 다른지 확인&lt;/li&gt;
          &lt;li&gt;목적2 : 둘이 결합될 수 있는지 확인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;목표
        &lt;ul&gt;
          &lt;li&gt;기초적 학습법 (Learning methods) 안내&lt;/li&gt;
          &lt;li&gt;이 Bandit problem 이 associative (연관성) 성질을 가지게 되었을 때 어떻게 변할지 확인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;A k-armed Bandit Problem
        &lt;ul&gt;
          &lt;li&gt;문제에 대한 설명
            &lt;ul&gt;
              &lt;li&gt;반복적으로 k 개의 다른 옵션/행동을 선택&lt;/li&gt;
              &lt;li&gt;각 선택 마다 보상 (선택한 행동에 따른 정상성 확률 분산을 가진 보상) 을 받는 문제&lt;/li&gt;
              &lt;li&gt;목적 : 특정 기간동안 최대한의 보상을 받는 것&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;문제에 대한 예시 1
            &lt;ul&gt;
              &lt;li&gt;슬롯머신 혹은 “one-armed bandit” 문제와 동일하나 단지 레버가 k 가임.&lt;/li&gt;
              &lt;li&gt;각 행동은 여러 대의 슬롯머신 중 한 대의 레버를 당기는 것과 동일&lt;/li&gt;
              &lt;li&gt;가장 이익을 극대화 할 수 있는 레버에 집중하여 보상을 많이 받는 것이 목표&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;문제에 대한 예시 2
            &lt;ul&gt;
              &lt;li&gt;의사가 심각한 질병의 환자에게 실험적 치료들 중 하나를 선택&lt;/li&gt;
              &lt;li&gt;보상 : 환자의 생존/ 치유율&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_1_1_value_of_action.png&quot; alt=&quot;2_1_1_value_of_action&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 문제를 풀었을 때 (q*) 의 value of the action 에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;value of the action : 각각의 k 행동들이 가지는 기대/평균 보상값&lt;/li&gt;
              &lt;li&gt;At : time step t 시점에 선택한 action&lt;/li&gt;
              &lt;li&gt;Rt : At 에 상응하는 보상&lt;/li&gt;
              &lt;li&gt;q*(a) : a 가 선택되었을때 기대되는 보상값&lt;/li&gt;
              &lt;li&gt;≐ : is defined as&lt;/li&gt;
              &lt;li&gt;If you knew the “value of the each action” : 해당 문제를 풀었다고 볼 수 있음.&lt;/li&gt;
              &lt;li&gt;Qt(a) : q*(a) 와 유사한 값 (중간값)&lt;/li&gt;
              &lt;li&gt;Q Star 값은 에이전트가 알고 있는 값이 아님&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;greedy actions
            &lt;ul&gt;
              &lt;li&gt;action value 를 계속 추정하다 보면 어느 시점에서나 적어도 하나의 가장 큰 예측값을 가지는 action 이 존재&lt;/li&gt;
              &lt;li&gt;이것을 greedy actions 라고 함.&lt;/li&gt;
              &lt;li&gt;이 greedy action 을 선택하는 것 : 현재 알고 있는 values of the action 값을 exploiting 한다.&lt;/li&gt;
              &lt;li&gt;이 greedy action 을 선택하지 않는 것 : 현재 exploring 중이라고 한다.&lt;/li&gt;
              &lt;li&gt;Exploitation (이기적 이용)
                &lt;ul&gt;
                  &lt;li&gt;현 step 에서 예측되는 보상값을 최대화 하는 옳은 방법&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Exploration (탐색)
                &lt;ul&gt;
                  &lt;li&gt;장기 관점으로 보았을 때 이쪽의 보상 총합이 더 클 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Balancing exploration and exploitation
            &lt;ul&gt;
              &lt;li&gt;k-armed bandit 과 이와 유사한 문제의 특정 수학적 공식에 대해 탐색과 이용의 밸런스를 맞출 수 있는 정교한 방법이 존재
                &lt;ul&gt;
                  &lt;li&gt;하지만 이런 방법들은 정상성에대한 강한 가정을 전재하고, full reinforcement learning 환경이나 어플리케이션을 침해하거나  증명할 수 없는 사전지식을 이용한다.&lt;/li&gt;
                  &lt;li&gt;즉 활용 불가. (Full reinforcement learning 환경에서 이 밸런스 문제는 도전적인 과제임)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;k-armed bandit problem 에서는 균형에 대해서만 고려하고, 단순 Exploitation 하는 것보다 균형을 맞춘 방식이 더 잘 작동한다는 것을 증명할 것임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Action-value Methods
        &lt;ul&gt;
          &lt;li&gt;행동 선택을 위해 행동(Action) 에 대한 가치를 측정하는 방법을 통상 Action-value method 라 한다.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명 1&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_2_1_averaging_the_rewards.png&quot; alt=&quot;2_2_1_averaging_the_rewards&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 action-value method 중 보상평균값으로 추정하는 방식에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;Action-value 는 통상적으로 해당 Action 이 선택되었을 때 보상 값의 평균치를 말한다.&lt;/li&gt;
              &lt;li&gt;1(predictate) 는 행위를 하였을 때는 1, 아닌 경우 0 (가상의 1/0, 횟수의 개념)&lt;/li&gt;
              &lt;li&gt;분모가 무한하게 커지면 Qt(a) 의 값은 q*(a) 의 값에 수렴한다.&lt;/li&gt;
              &lt;li&gt;이 방식을 sample-average method 라 부르며, 이는 Action-value 를 구하기 위한 많은 방법 중 하나이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명 2&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_2_2_greedy_action_selection_method.png&quot; alt=&quot;2_2_2_greedy_action_selection_method&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 greedy action selection method 에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;action selection 에서 가장 단순한 방법은 가장 큰 예측값에 해당하는 action을 선택하는 것이다.&lt;/li&gt;
              &lt;li&gt;현 시점 이전까지 가장 탐욕적인 action 으로 정의된 행동을 하는 것.&lt;/li&gt;
              &lt;li&gt;argmaxa : 후술되는 값이 최대값이 되는 action a 를 의미&lt;/li&gt;
              &lt;li&gt;Greedy action selection 은 현 지식을 이용해 당장의 보상을 최대화 하는 방식&lt;/li&gt;
              &lt;li&gt;정말 더 나은 방식을 찾기 위해 열등한 방식을 샘플링하는데 시간을 할애하지 않음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;e-greedy methods
            &lt;ul&gt;
              &lt;li&gt;위와 다른 대안으로 대부분 탐욕스러운 행동을 하되&lt;/li&gt;
              &lt;li&gt;아주 작은 확률(e)로 action-value 와 관계 없이 균등한 확률로 a 를 선택하는 방법이 있음.&lt;/li&gt;
              &lt;li&gt;이 방법을 통한 샘플링 횟수가 무한하게 커지면 Qt(a) 의 값은 q*(a) 의 값에 수렴한다.&lt;/li&gt;
              &lt;li&gt;이것은 점근적인 보장일 뿐, 실질적인 효과에 대한 것은 아니다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The 10-armed Testbed
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;조건 설명&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_3_1_10_armed_testbed.png&quot; alt=&quot;2_3_1_10_armed_testbed&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;10-armed testbed 의 bandit problem 문제 예시&lt;/li&gt;
          &lt;li&gt;각 10 개 action 의 true value q*(a) 값 은 평균 0, 분산 1 인 정규분포 (표준정규분포) 를 따른다.&lt;/li&gt;
          &lt;li&gt;위 조건으로 2000번의 서로 다른 bandit problem 을 수행, 평균 값을 취함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;결과&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_3_2_result_10_armed_testbed.png&quot; alt=&quot;2_3_2_result_10_armed_testbed&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;보상의 분산이 클수록 더 많은 탐험이 필요하며 e-greedy methods 가 greedy methods 보다 더 잘 작동한다.&lt;/li&gt;
          &lt;li&gt;만약 보상의 분산이 0이라면 각각의 action 후에 true value 값을 바로 알 수 있게 된다.&lt;/li&gt;
          &lt;li&gt;위의 경우 greedy methods 가 더 잘 작동하게 된다. (탐험할 필요가 없음.)&lt;/li&gt;
          &lt;li&gt;그러나 이런 결정론적인 상황에서도 몇가지 가정이 불확실하다면 탐험하는 쪽이 유리하다.
            &lt;ul&gt;
              &lt;li&gt;비정상성 환경 (시간에 따라 true action-value 가 변함.)&lt;/li&gt;
              &lt;li&gt;즉 Reinforcement learning 은 탐색과 이용에 균형이 필요함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Incremental Implementation
    &lt;ul&gt;
      &lt;li&gt;action-value methods 를 획득한 보상의 평균 값으로 추정한다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이 경우 어떻게 전산화 하여 계산할지?&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_1_estimated_value_of_single_action.png&quot; alt=&quot;2_4_1_estimated_value_of_single_action&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;특정 단일 Action 에 대한 Action-value 를 예측, action 은 n-1 번 선택됨.&lt;/li&gt;
          &lt;li&gt;위 명확한 계산법은 모든 이전 기록을 가지고 있어야 하고, 예측값이 필요할 때마다 계산해야함.
            &lt;ul&gt;
              &lt;li&gt;메모리가 많이 필요하고 연산량이 상당해진다.&lt;/li&gt;
              &lt;li&gt;위의 방법이 아니라 이전 평균값에서 이번 보상값을 업데이트 하는 방식을 취하는 것이 효율적이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_2_update_action_value.png&quot; alt=&quot;2_4_2_update_action_value&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 구현법을 이용하면 메모리는 Qn 과 n 값만을 저장하고 있고, 작은 계산을 통해 매번 새로운 예측값을 구할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_3_update_action_value.png&quot; alt=&quot;2_4_3_update_action_value&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;수식의 뜻은 위와 같음&lt;/li&gt;
          &lt;li&gt;Target - OldEstimate = error
            &lt;ul&gt;
              &lt;li&gt;위 에러 값은 예측값이 Target 에 가까워질 수록 작아짐&lt;/li&gt;
              &lt;li&gt;Target 은 예측이 움직이기 원하는 방향을 가리킴&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;StepSize : 각 타임스텝마다 변하는 step-size parameter 이며, 이 책에서는 알파 값 혹은 at(a) 로 나타낸다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_4_simple_bandit_algorithm.png&quot; alt=&quot;2_4_4_simple_bandit_algorithm&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;완성된 simple bandit algorithm&lt;/li&gt;
          &lt;li&gt;breaking ties randomly.. 동점 기록일 경우 랜덤하게 선택한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tracking a Nonstationary Problem
    &lt;ul&gt;
      &lt;li&gt;보상의 확률이 변하는 reinforcement learning 문제에서는 오랜 과거 보상보다 최근 보상에 비중을 더 두는 것이 설득력있다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이러한 방법 중 하나로 상수 파라미터 (a constant step-size parameter) 를 사용하는 것이 유명하다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_5_1_constant_step_size_parameter.png&quot; alt=&quot;2_5_1_constant_step_size_parameter&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 수식을 weighted average 라고도 하는데 각 가중치(Q,R 앞 값) 의 합이 1이 되기 때문이다.&lt;/li&gt;
          &lt;li&gt;1-a 값이 1보다 작기 때문에 승수가 커질수록 (이전 step 의 값일수록) 가중치 값이 decay 됨&lt;/li&gt;
          &lt;li&gt;때로는 위 수식을 exponential recency-weighted average 라고도 함 (지수적 최근성 가중치 평균)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;때로는 step 별로 변동하는 step-size parameter 를 사용하는 것이 편할 때가 있음.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 1/n step-size parameter (sample-average method) 는 충분히 큰 step 을 진행할 경우 true action value 로 수렴하는 것을 보장한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;확률적 근사 이론은 확률 1로 수렴을 보장하는 데 필요한 조건을 제공한다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_5_2_conditions_required_to_convergence_with_prob1.png&quot; alt=&quot;2_5_2_conditions_required_to_convergence_with_prob1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;첫번째 조건은 초기 조건이나 무작위 변동을 극복할 수 있을 정도로 step 이 큰 것을 보장&lt;/li&gt;
          &lt;li&gt;두번째 조건은 step이 수렴을 확신할 정도로 충분히 작은 것을 보장&lt;/li&gt;
          &lt;li&gt;1/n 은 이 두 조건을 모두 만족하나, 상수 step 파라미터는 두번째 조건을 충족하지 않아 가장 최근의 보상값에 의해 완전히 수렴하지 못하게 됨.
            &lt;ul&gt;
              &lt;li&gt;이것은 비정상성 환경에서 필요한 내용이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;두 조건이 만족하더라도 매우 느리게 수렴하거나, 만족스러운 수렴율을 얻기 위해 파라미터를 튜닝해야 할 수도 있음.&lt;/li&gt;
          &lt;li&gt;위 이론은 이론적인 내용에는 자주 사용되나, 실제 적용 환경에서는 잘 사용되지 않음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic Initial Values
    &lt;ul&gt;
      &lt;li&gt;위에 언급한 모든 방법들은 initial action-value estimates (Q1(a)) 에 어느정도 영향을 받는다.&lt;/li&gt;
      &lt;li&gt;통계적 표현으로 이러한 방식들은 biased by their initial estimates (초기추정치에 의해 편향된다) 라고 한다.&lt;/li&gt;
      &lt;li&gt;예를 들어 sample-average methods 는 이 편향이 action 이 최초 선택될 때 사라진다면&lt;/li&gt;
      &lt;li&gt;상수 a 를 사용하는 경우 이 편향은 영구적이다. (시간의 흐름에 따라 점차 감소하지만)&lt;/li&gt;
      &lt;li&gt;장점 : 예상할 수 있는 보상 수준에 대한 사전지식을 제공하는 쉬운 방법&lt;/li&gt;
      &lt;li&gt;단점 : 모든 매개변수를 0 으로 설정하는 경우 사용자가 반드시 파라미터를 선택해야 한다.&lt;/li&gt;
      &lt;li&gt;간단한 탐색 장려의 방법 : 초기 값을 0 대신 +5 로 설정 (10-armed testbed 상황으로 가정)
        &lt;ul&gt;
          &lt;li&gt;초기값 +5 의 값은 매우 낙관적인 수치&lt;/li&gt;
          &lt;li&gt;특정 action 을 선택하고 받는 보상 값이 예측치보다 작음 (disappointed with the rewards)&lt;/li&gt;
          &lt;li&gt;학습자는 다른 action 을 선택하게 되고 이 상황을 몇번 반복됨. (greedy action 일지라도…)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_6_1_optimistic_greedy.png&quot; alt=&quot;2_6_1_optimistic_greedy&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;10-armed bandit testbed 에서 greedy method 를 초기값 Q1(a) +5 로 세팅한 결과&lt;/li&gt;
          &lt;li&gt;비교군은 e-greedy method 에 초기값 Q1(a) = 0&lt;/li&gt;
          &lt;li&gt;이 trick 은 stationary problem 에서 꽤 효과적이나, 탐색을 장려하는 일반적인 방법은 아님.&lt;/li&gt;
          &lt;li&gt;이러한 비판은 sample-average methods 에서도 통용된다.
            &lt;ul&gt;
              &lt;li&gt;초기 시점을 특수한 이벤트로 여긴다.&lt;/li&gt;
              &lt;li&gt;모든 보상을 똑같은 가중치로 평균을 구한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upper-Confidence-Bound Action Selection
    &lt;ul&gt;
      &lt;li&gt;action-value 추정값의 불확실성 때문에 탐험은 반드시 필요하다.&lt;/li&gt;
      &lt;li&gt;greedy actions 는 현재 시점에는 가장 최적의 선택이나 다른 action 이 실제로는 더 좋은 것일 수 있다.&lt;/li&gt;
      &lt;li&gt;e-greedy action selection 은 강제적으로 non-greedy action 을 선택하지만 선호도 없이 무차별적인 선택을 하여 greedy 한 선택을 하게 될 수도 있다.
        &lt;ul&gt;
          &lt;li&gt;강제적인 선택을 할 때는 non-greedy 한 선택을 하는 것이 좋음&lt;/li&gt;
          &lt;li&gt;추정치가 최대치에 얼마나 가까운지와 추정치의 불확실성을 모두 고려&lt;/li&gt;
          &lt;li&gt;실제로 최적일 가능성에 따라 탐욕스럽지 않은 action 을 선택하는 것이 좋음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_7_1_UCB.png&quot; alt=&quot;2_7_1_UCB&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;UCB (Upper-Confidence-Bound) Action Selection 은 그러한 효율적인 방식 중 하나이다.&lt;/li&gt;
          &lt;li&gt;Nt(a) 는 t step 이 진행되었을때 a action 이 선택된 횟수로, 많이 선택될 수록 우항의 피연산자 값이 작아진다.&lt;/li&gt;
          &lt;li&gt;자연로그 t 는 step 이 커짐에 따라 값이 무한대까지 증가 (수렴하지 않음) 하나 그 증가폭이 서서히 줄어든다&lt;/li&gt;
          &lt;li&gt;c는 탐험의 정도 (강도) 를 나타내는 수치이다.&lt;/li&gt;
          &lt;li&gt;즉 action이 많이 선택될수록 Qt(a) 의 값은 정확해지고, 우항의 피연산자 값은 작아진다.&lt;/li&gt;
          &lt;li&gt;action 이 한번도 선택되지 않을 경우 해당 a 를 maximizing action 으로 여긴다. (해당 a 에 대한 무조건적인 탐험)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_7_2_UCB.png&quot; alt=&quot;2_7_2_UCB&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;이러한 UCB 방식은 10-armed testbed 에서 e-greedy 보다 나은 성과를 보여주기도 한다.&lt;/li&gt;
          &lt;li&gt;단 몇가지 단점으로 인해 실용적이지 않은 방식이다.
            &lt;ul&gt;
              &lt;li&gt;bandits 문제에서 reinforcement learning 문제로 확장하기 어렵다.&lt;/li&gt;
              &lt;li&gt;nonstationary 한 문제들에는 더 어려운 방식의 action-value method 가 필요하다.&lt;/li&gt;
              &lt;li&gt;훨신 더 거대한 환경 (state space) 에서의 적용 (특히 뒤에 배울 function approximation 방식) 이 어렵다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-1-the-k-armed-bandit-problem&quot;&gt;Lesson 1: The K-Armed Bandit Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define reward&lt;/li&gt;
      &lt;li&gt;Understand the temporal nature of the bandit problem&lt;/li&gt;
      &lt;li&gt;Define k-armed bandit&lt;/li&gt;
      &lt;li&gt;Define action-values&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sequential Decision Making with Evaluative Feedback
    &lt;ul&gt;
      &lt;li&gt;불확실성 아래에서의 의사결정
        &lt;ul&gt;
          &lt;li&gt;의사가 3가지 처방약으로 환자에게 실험적 처방을 할 때…&lt;/li&gt;
          &lt;li&gt;몇번의 환자 반응을 보고 가장 잘 듣는 약을 고집할 경우
            &lt;ul&gt;
              &lt;li&gt;더이상 다른 약의 데이터를 모을 수 없음&lt;/li&gt;
              &lt;li&gt;나머지 두 약이 실제로 나음에도 몇몇 결과가 나쁘게 나왔는지 알 수 없음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;다른 약으로 계속 실험할 경우
            &lt;ul&gt;
              &lt;li&gt;나쁜 결과를 계속 초래할 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습에서의 기초적 컨셉 용어
        &lt;ul&gt;
          &lt;li&gt;에이전트 (Agent) : action을 선택하는 존재 - 의사&lt;/li&gt;
          &lt;li&gt;액션 (Action) : 선택지 - 3가지 약 중 하나를 선택하는 것&lt;/li&gt;
          &lt;li&gt;보상 (Rewards) : 액션에 대한 결과 - 환자의 상태&lt;/li&gt;
          &lt;li&gt;값 (Value Function - Action-Value (Function)) : 기대되는 보상 값 - 환자의 혈압 값
            &lt;ul&gt;
              &lt;li&gt;에이전트가 액션을 선택했을 때 그 값 (Action-Value = q*)이 최대화 할 경우 목적을 달성함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;q*(a) 값 구하기
        &lt;ul&gt;
          &lt;li&gt;각 약의 결과값이 서로 다른 확률분포를 가졌을 경우 q* 는 각 분포의 평균이 될 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bandits Problem 을 고려하는 이유
        &lt;ul&gt;
          &lt;li&gt;문제와 알고리즘 디자인 선택에 있어 가장 간단한 세팅&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-2-what-to-learn-estimating-action-values&quot;&gt;Lesson 2: What to Learn? Estimating Action Values&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define action-value estimation methods&lt;/li&gt;
      &lt;li&gt;Define exploration and exploitation&lt;/li&gt;
      &lt;li&gt;Select actions greedily using an action-value function&lt;/li&gt;
      &lt;li&gt;Define online learning&lt;/li&gt;
      &lt;li&gt;Understand a simple online sample-average action-value estimation method&lt;/li&gt;
      &lt;li&gt;Define the general online update equation&lt;/li&gt;
      &lt;li&gt;Understand why we might use a constant step-size in the case of non-stationarity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning Action Values
    &lt;ul&gt;
      &lt;li&gt;Estimate action values using the sample-average method
        &lt;ul&gt;
          &lt;li&gt;의사가 처방 후 환자가 나아질 경우 1, 그렇지 않은 경우 0 으로 표기하고 3개의 약을 처방해 평균을 구함&lt;/li&gt;
          &lt;li&gt;Step 이 많이 진행될 수록 평균 데이터는 더 정확해짐&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Describe greedy action selection
        &lt;ul&gt;
          &lt;li&gt;의시가 해당 시점에 가장 기대치가 큰 약을 처방할 경우 이 행동을 Greedy action 이라고 함&lt;/li&gt;
          &lt;li&gt;greedy action 을 선택하는 것을 현 지식을 활용한 이용 (exploitation) 이라고 함&lt;/li&gt;
          &lt;li&gt;당장의 기대되는 보상을 포기하고 다른 선택을 하는 것을 non-greedy action 이라고 하고 이를 탐험 (exploration) 이라고 함&lt;/li&gt;
          &lt;li&gt;탐험을 통해 기대되는 보상을 희생하고 non-greedy action 의 보상에 대한 더 많은 정보를 얻게 됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Introduce the exploration-exploitation dilemma
        &lt;ul&gt;
          &lt;li&gt;에이전트는 동시에 탐험과 이용을 할 수 없음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Estimating Action Values Incrementally
    &lt;ul&gt;
      &lt;li&gt;action value 의 점진적 표현법 (sample-average method 를 이용)
        &lt;ul&gt;
          &lt;li&gt;Incremental update rule&lt;/li&gt;
          &lt;li&gt;NewEstimate &amp;lt;- OldEstimate + StepSize(Target - OldEstimate)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;점진적 업데이트 룰(Incremental update rule)이 더 널리 쓰이는 이유
        &lt;ul&gt;
          &lt;li&gt;모든 이전 값들을 기억할 필요가 없다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보편적 업데이트 룰을 non-stationary bandit problem 에 적용하는 방법
        &lt;ul&gt;
          &lt;li&gt;Non-stationary Bandit Problem : 의사의 3가지 약중 특정 하나의 약이 겨울이 되면 효율이 높아진다.&lt;/li&gt;
          &lt;li&gt;보상의 분포가 시간에 따라 변하게 되는 경우를 Non-stationary 하다 라고 함.&lt;/li&gt;
          &lt;li&gt;StepSize 파라미터가 상수 (예:0.1) 일 경우 이전 Step 일수록 영향도가 작아지고, 최신 Step의 보상값을 더 반영함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-3-exploration-vs-exploitation-tradeoff&quot;&gt;Lesson 3: Exploration vs. Exploitation Tradeoff&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define epsilon-greedy&lt;/li&gt;
      &lt;li&gt;Compare the short-term benefits of exploitation and the long-term benefits of exploration&lt;/li&gt;
      &lt;li&gt;Understand optimistic initial values&lt;/li&gt;
      &lt;li&gt;Describe the benefits of optimistic initial values for early exploration&lt;/li&gt;
      &lt;li&gt;Explain the criticisms of optimistic initial values&lt;/li&gt;
      &lt;li&gt;Describe the upper confidence bound action selection method&lt;/li&gt;
      &lt;li&gt;Define optimism in the face of uncertainty&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What is the trade-off?
    &lt;ul&gt;
      &lt;li&gt;exploration-exploitation 의 등가교환
        &lt;ul&gt;
          &lt;li&gt;Exploration (탐색) : 장기적 이익을 위해 지식을 늘림&lt;/li&gt;
          &lt;li&gt;Exploitation (이용) : 단기적 이익을 위해 지식을 이용&lt;/li&gt;
          &lt;li&gt;탐색만 하면 단기적 보상이 작아지고, 이용만 하면 타 선택지의 true value 를 모르기 때문에 장기적으로 손해일 수 있음&lt;/li&gt;
          &lt;li&gt;한번의 선택에서 탐색 과 이용 둘 중 하나만 가능&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Epsilon-Greedy 방식 (탐색과 이용의 균형을 맞추는 쉬운 방법)
        &lt;ul&gt;
          &lt;li&gt;Epsilon-Greedy 는 대부분 Exploitation (Greedy method) 하고, 적은 확률로 Exploration (Random choice) 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;각 방식을 비교할 때 한 번의 진행으로는 노이즈가 많아 확인이 어려움.
        &lt;ul&gt;
          &lt;li&gt;1000 개의 에이전트로 보상 데이터를 모아 평균값으로 비교&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic Initial Values
    &lt;ul&gt;
      &lt;li&gt;Optimistic initial values 가 초기 탐색을 장려하는 이유
        &lt;ul&gt;
          &lt;li&gt;초기 예상 보상치를 높게 잡고 시작&lt;/li&gt;
          &lt;li&gt;첫 선택의 보상이 주어지더라도 평균값으로 인해 값이 떨어짐&lt;/li&gt;
          &lt;li&gt;에이전트는 첫 선택에 실망을 하고 선택되지 않은 다른 높은 초기치의 옵션 중 하나를 선택&lt;/li&gt;
          &lt;li&gt;초기 탐험을 통해 모든 Action 을 골고루 선택하게 됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Optimistic initial values 의 한계
        &lt;ul&gt;
          &lt;li&gt;초기 단계에서만 탐색을 진행한다. 이는 Non-stationary Problems 에서 문제가 됨.&lt;/li&gt;
          &lt;li&gt;Maximal Reward 를 시작하기 전 알 방법이 없기에 Optimistic Initial Value 를 어느정도로 설정해야 할지 모른다.&lt;/li&gt;
          &lt;li&gt;그럼에도 Optimistic initial values 는 휴리스틱 (충분한 정보 없이 빠르게 사용할 수 있는 직관적인) 한 방법으로 자주 활용됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upper-Confidence Bound(UCB) Action Selection
    &lt;ul&gt;
      &lt;li&gt;UCB action-selection 방식이 예측의 불확실성을 이용해 탐색을 유도하는 방법&lt;/li&gt;
      &lt;li&gt;UCB의 C는 confidence 를 뜻하며, 예측한 Q(a) 값의 오차범주 범위(c) 내에 실제 값이 들어올 것이라 확신하는 정도의 수치이다.
        &lt;ul&gt;
          &lt;li&gt;범위가 작을수록 결과에 더욱 확신한다는 뜻임&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;불확실성 (범위) 에 마주했을때 가장 높은 값 (Upper-Confidence Bound) 을 선택&lt;/li&gt;
      &lt;li&gt;수식에 대한 풀이
        &lt;ul&gt;
          &lt;li&gt;c : 사용자 정의 파라미터 (얼마나 탐험을 할 건지 컨트롤)&lt;/li&gt;
          &lt;li&gt;좌측 피연산자 : Exploit (이용)&lt;/li&gt;
          &lt;li&gt;우측 피연산자 : Explore (탐험)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Contextual Bandits for Real World Reinforcement Learning
    &lt;ul&gt;
      &lt;li&gt;Contextual Bandits 는 Reinforcement Learning 이 현실에 배포되는 방식임&lt;/li&gt;
      &lt;li&gt;Reinforcement 는 현실과 어떻게 다른가?
        &lt;ul&gt;
          &lt;li&gt;Reinforcement Learning 은 시뮬레이터로 동작한다.
            &lt;ul&gt;
              &lt;li&gt;시뮬레이터는 관측치를 제공&lt;/li&gt;
              &lt;li&gt;Learning 알고리즘은 어떤 Action을 선택할 지 정책을 가진다.&lt;/li&gt;
              &lt;li&gt;시뮬레이터는 실행하고 보상을 제공한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;현실이 제공하는 관측값은 시뮬레이터와 다르다.
            &lt;ul&gt;
              &lt;li&gt;같은 정책이라 해도 관측치가 다르기 때문에 시뮬레이터에서의 Action 과 다른 Action 을 수행하게 된다.&lt;/li&gt;
              &lt;li&gt;시뮬레이터와 현실은 보상도 다르다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉 현실과 시뮬레이터에는 갭이 존재한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Real World based Reinforcement Learning : 현실 기반 강화학습을 하려면 우선순위를 변경해야 함
        &lt;ul&gt;
          &lt;li&gt;Temporal Credit Assignment &amp;lt; Generalization
            &lt;ul&gt;
              &lt;li&gt;Temporal Credit Assignment Problem (CAP) : 시간적 기여도 할당문제. 일련의 행동이 모두 끝난 뒤 보상을 얻을 수 있는 환경에서 수행한 행동들 중 어떠한 행동이 기여도가 있고 어떠한 행동이 벌점을 줄 것인지 결정하는 문제.&lt;/li&gt;
              &lt;li&gt;Generalization : 다양한 관찰값을 통한 일반화&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Control environment &amp;lt; Environment controls
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션 환경에서는 자유자재로 한 스텝을 더 진행하거나 할 수 있다면 (컨트롤 가능)&lt;/li&gt;
              &lt;li&gt;현실에서는 환경이 지배함. (환경에 의해 컨트롤당함)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Computational efficiency &amp;lt; Statistical efficiency
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션 환경에서 계산효율이 중요하다면 (학습할 샘플이 많으므로)&lt;/li&gt;
              &lt;li&gt;현실에서는 통계적 효율이 중요함 (현실이 주는 샘플만 가질 수 있다.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;State &amp;lt; Features
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션에서는 상태를 생각 (상태값은 결정을 내리는 데 기반이 되는 요소)&lt;/li&gt;
              &lt;li&gt;현실에서는 매우 복잡한 관측값을 가지는 경우 (필요한 것보다 정보가 많음) 가 많아 어느 것이 핵심 요인인지가 중요&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Learning &amp;lt; Evaluation
            &lt;ul&gt;
              &lt;li&gt;현실에서는 “Off Policy Evaluation” 이 중요하다.
                &lt;ul&gt;
                  &lt;li&gt;Off Policy : 정책 업데이트에 어떤 데이터를 써도 상관이 없는 경우&lt;/li&gt;
                  &lt;li&gt;즉, 최신 업데이트 정책에서 수집된 데이터가 아니어도 사용가능&lt;/li&gt;
                  &lt;li&gt;학습 알고리즘이 학습 뿐만 아니라 정책 평가에 쓰일 수 있는 부산물 데이터 또한 제공&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Last policy &amp;lt; Every Policy
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션에서는 가장 최신의 정책이 중요&lt;/li&gt;
              &lt;li&gt;현실에서는 모든 포인트의 데이터가 세상과의 어느 정도 상호작용이 포함되어 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-summary-rlbook2018-pages-42-43&quot;&gt;Chapter Summary (RLbook2018 Pages 42-43)&lt;/h2&gt;

&lt;h2 id=&quot;weekly-assessment&quot;&gt;Weekly Assessment&lt;/h2&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">Course Introduction</summary></entry><entry><title type="html">Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020</title><link href="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/" rel="alternate" type="text/html" title="Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020" /><published>2023-01-26T11:00:00+09:00</published><updated>2023-01-26T11:00:00+09:00</updated><id>https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020</id><content type="html" xml:base="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/">&lt;h2 id=&quot;1-구매정보&quot;&gt;1. 구매정보&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;빈티지 : 2020&lt;/li&gt;
  &lt;li&gt;가격 : 40,000 원 (할인 및 페이백 후 추정가)&lt;/li&gt;
  &lt;li&gt;구매처 : 편의점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-이미지&quot;&gt;2. 이미지&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/Textbook_Napa_Cabernet_Sauvignon.jpg&quot; alt=&quot;Textbook_Napa_Cabernet_Sauvignon.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;about:blank&quot; target=&quot;_blank&quot;&gt;링크없음&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-감상&quot;&gt;3. 감상&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;1일차
    &lt;ul&gt;
      &lt;li&gt;레드 와인&lt;/li&gt;
      &lt;li&gt;향이 강하지 않음&lt;/li&gt;
      &lt;li&gt;맛이 굉장히 가벼운 느낌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-총점&quot;&gt;3. 총점&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;3/5
    &lt;ul&gt;
      &lt;li&gt;많은 기대를 한 와인이었으나, 기대에 못미침&lt;/li&gt;
      &lt;li&gt;가볍고 경쾌한 느낌이어서 많이 마실 수 있었음&lt;/li&gt;
      &lt;li&gt;하지만 이번 와인을 계기로 난 와인보다 양주나 정종이 더 맞다는 결론을 내림.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Wine" /><category term="와인" /><summary type="html">1. 구매정보 빈티지 : 2020 가격 : 40,000 원 (할인 및 페이백 후 추정가) 구매처 : 편의점</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 분석 학습을 위한 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;강화학습의 기반 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 04. Real-world time series data</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 04. Real-world time series data" /><published>2022-11-07T14:00:00+09:00</published><updated>2022-11-07T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/">&lt;h1 id=&quot;real-world-time-series-data&quot;&gt;Real-world time series data&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;실사용 데이터 (태양의 흑점활동) 이용하기&lt;/li&gt;
  &lt;li&gt;Conv1D, LSTM, DNN 을 결합한 모델을 활용할 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions&quot;&gt;Convolutions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, // 32개의 필터를 학습할 1D Conv
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9)

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bi-directional-lstms&quot;&gt;Bi-directional LSTMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), // LSTM 의 입력값을 재구성하는 Lambda 레이어를 없앰
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500) // epoch 늘리기
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;windowed_dataset 헬퍼 함수&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):

          ds = tf.data.Dataset.from_tensor_slices(series)
          ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
          ds = ds.flat_map(lambda w: w.batch(window_size + 1))
          ds = ds.shuffle(shuffle_buffer)
          ds = ds.map(lambda w: (w[:-1], w[-1]))

          return ds.batch(batch_size).prefetch(1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bi-directional LSTM&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), 
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), // 양방향
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;결과수치는 긍정적이나 검증세트에 예측을 플로팅 해보면 과적합이 보여 일부 파라미터에 변화를 줄 필요가 있음&lt;/li&gt;
      &lt;li&gt;MAE 로 손실을 플로팅하면 문제점을 확인할 수 있음
        &lt;ul&gt;
          &lt;li&gt;스파이크 현상은 배치 크기가 작아서 무작위 노이즈가 많기 때문임&lt;/li&gt;
          &lt;li&gt;배치사이즈를 줄이거나 늘이는 것에 따라 학습과 예측이 달라짐&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-batch-sizing&quot;&gt;More on batch sizing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;신경망을 더 빠르게 학습하도록 하는 최적화 알고리즘
    &lt;ul&gt;
      &lt;li&gt;머신러닝 : 잘 작동되는 모델을 찾기 위해 많은 훈련을 거쳐야 하는 반복적인 과정
        &lt;ul&gt;
          &lt;li&gt;모델을 빠르게 훈련시키는 것이 매우 중요함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;딥러닝은 빅데이터에서 가장 잘 작동됨 -&amp;gt; 훈련이 어려움 (큰 데이터 세트에서 훈련하는 것은 매우 느린과정)&lt;/li&gt;
      &lt;li&gt;좋은 최적화 알고리즘을 찾는 것은 효율성을 향상시켜준다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;미니배치 경사 하강법
    &lt;ul&gt;
      &lt;li&gt;벡터화 : m개의 샘플에 대한 계산을 효율적으로 만들어줌. 명시적인 반복문 없이도 훈련 세트를 진행할 수 있도록 함.&lt;/li&gt;
      &lt;li&gt;훈련 샘플을 받아서 큰 벡터에 저장함
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^m]&lt;/code&gt; : shape : (n_x,m)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = [y^1, y^2, ..., y^m]&lt;/code&gt; : shape : (1,m)&lt;/li&gt;
          &lt;li&gt;하지만 m 의 수치가 크면 여전히 학습은 느리다.&lt;/li&gt;
          &lt;li&gt;예를들어 m 의 수치가 500만 이라면?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;전체 훈련 세트에 대한 경사 하강법을 구현하면 경사 하강법의 작은 한 단계를 밟기 전에 모든 훈련 세트를 처리해야 함
        &lt;ul&gt;
          &lt;li&gt;즉, 경사하강법의 다음 단계를 밟기 전에 500만 개의 전체 훈련 샘플을 처리해야 함&lt;/li&gt;
          &lt;li&gt;500만 개의 전체 훈련 샘플을 모두 훈련하기 전에 경사 하강법이 진행되도록 하면 더 빠른 알고리즘을 얻을 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;훈련 세트를 더 작은 훈련세트 (미니배치) 로 나눔
        &lt;ul&gt;
          &lt;li&gt;mini-batch 가 1000개의 샘플을 갖는다고 가정&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^1000 | x^1001, ..., x^2000 | ... | ... x^m]&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = X^{1},                   X^{2}, ... ,               X^{5000}&lt;/code&gt; : shape : (n_x, 1000)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = Y^{1},                   Y^{2}, ... ,               Y^{5000}&lt;/code&gt; : shape : (1, 1000)&lt;/li&gt;
          &lt;li&gt;Mini-batch t : X^{t}, Y^{t}&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;표기법 정의
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x^(i)&lt;/code&gt; : i 번째 훈련 샘플&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z^[l]&lt;/code&gt; : l 번째 신경망의 z 값&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X^{t}&lt;/code&gt; : t 번째 미니배치 X&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Batch Gradient Descent : 일반적인 경사하강법, 모든 훈련 세트를 동시에 훈련시킴, 훈련 샘플의 모든 배치를 진행시킨다는 관점&lt;/li&gt;
      &lt;li&gt;미니배치 : 전체 훈련 세트 X,Y 를 한번에 진행시키지 않고, 하나의 미니배치 X^{t}, Y^{t} 를 동시에 진행시키는 알고리즘
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for t=1 ,..., 5000 : 총 미니배치의 수 (5000개)
      // 1 step of gradient descent using X^{t}, Y^{t}
      // (as if m=1000)
      // 모든 1000개의 샘플에 대해 명시적인 반복문을 갖는 것보다 벡터화를 사용해 모든 1000개의 샘플을 동시에 진행함
      Forward prop on X^{t}
          Z^[1] = W^[1] * X^{t} + b^[1] : Vectorized Implementation (1000 examples)
          A^[1] = g^[1] * (Z^[1]) : Vectorized Implementation (1000 examples)
          ...
          A^[l] = g^[l] * (Z^[l]) : Vectorized Implementation (1000 examples)
      Compute cost J^{t} = 1/1000 * Sum (( i = 1 to l) Loss(expect(y^(i)), y^(i)))
          + 정규화 항
      Backprop to compute Gradients cost J^(t) using X^{t}, Y^{t}
          W^[l] = W^[l] - adW^[l], b^[l] = b^[l] - adb^[l]
      ...
      1 epoch : pass through training set (5000 개의 경사하강단계)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm-notebook&quot;&gt;Convolutions with LSTM notebook&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm&quot;&gt;Convolutions with LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;real-data---sunspots&quot;&gt;Real data - sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;소스 내려받기 : 케글에서 내려받거나, 이번학습을 위한 데이터 제공 저장소를 사용 (후자)&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv -O /tmp/sunspots.csv&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CSV 읽기&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      import csv
      time_step = []
      sunspots = []

      with open(&apos;/tmp/sunspots.csv&apos;) as csvfile:
          reader = csv.reader(csvfile, delimiter=&apos;,&apos;)
          next(reader)
          for row in reader :
              sunspots.append(float(row[2]))
              time_step.append(int(row[0]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;numpy 배열로 전환&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      series = np.array(sunspots) // numpy 에 항목을 추가할 때마다 목록을 복제하는데,
      time = np.array(time_step) // 메모리 관리 과정이 많이 진행되기 때문에 데이터 양이 많으면 느려질 수 있음
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;시계열을 훈련 및 검증 데이터 세트로 분할&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      split_time = 1000
      time_train = time[:split_time]
      x_train = series[:split_time]
      time_valid = time[split_time:]
      x_valid = series[split_time:]

      window_size = 20
      batch_size = 32
      shuffle_buffer_size = 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이전 windowed_dataset 함수와 동일 코드를 사용&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
          dataset = tf.data.Dataset.from_tensor_slices(series)
          dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
          dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
          dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
          dataset = dataset.batch(batch_size).prefetch(1)
          return dataset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-and-tune-the-model&quot;&gt;Train and tune the model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;이전 수업에서 배웠던 모델로 예측하여 플로팅하면, 결과는 괜찮아보이나 MAE 가 매우 큼&lt;/li&gt;
  &lt;li&gt;이는 이전 window_size 가 20 (여기서는 약 2년이 안되는 시간) 이나, 사실 흑점 데이터의 주기는 11년 혹은 22년으로 추정됨&lt;/li&gt;
  &lt;li&gt;window_size 를 11년에 해당하는 132로 두고 다시 훈련을 하면 차트는 더 잘 나오나 MAE는 더 커짐
    &lt;ul&gt;
      &lt;li&gt;데이터를 되돌아보면 11년 주기의 계절성을 갖지만 창 안에 계절 전체가 있어야 할 필요는 없음&lt;/li&gt;
      &lt;li&gt;플롯을 확대해보면 전형적인 시계열 형태 데이터임&lt;/li&gt;
      &lt;li&gt;나중에 오는 값이 앞선 값과 연관이 있지만 노이즈가 많음&lt;/li&gt;
      &lt;li&gt;그래서 훈련 시에 창 크기가 클 필요는 없을 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;data 의 분할을 1000을 훈련, 2500을 검증으로 설정하였는데 이는 좋지 못한 분할임
    &lt;ul&gt;
      &lt;li&gt;3500과 500으로 지정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망 설계와 파라미터의 크기 변경
    &lt;ul&gt;
      &lt;li&gt;10, 10, 1 레이어 를 30, 15, 1 로 값을 바꿔서 훈련 (입력 Shape 값이 30)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예측
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.predict(series[3205:3235][np.newaxis])&lt;/code&gt; : 7.077 개의 흑점 예상 (실 데이터 8.7 개)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;설정 변경 : MAE 13.7&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  split_time = 3000
  window_size = 60
	
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(20, input_shape=[window_size], activation=&quot;relu&quot;),
      tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
      tf.keras.layers.Dense(1)
  ])
	
  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;8.13 예측 (실제값 8.7)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots-notebooks-lab-2--lab-3&quot;&gt;Sunspots notebooks (Lab 2 &amp;amp; Lab 3)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 으로 대체&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots&quot;&gt;Sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;입력 창의 크기를 60으로 함&lt;/li&gt;
  &lt;li&gt;DNN 을 Dense 20, 10, 1 로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;combining-our-tools-for-analysis&quot;&gt;Combining our tools for analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      window_size = 60
      batch_size = 64
      train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=&quot;casual&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(30, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 400)
      ])

      lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch / 20))
      optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-8, momentum=0.9)
      model.compile(loss=tf.keras.losses.Huber(), optimizer = optimizer, metrics=[&quot;mae&quot;])
      history = model.fit(train-set, epochs=100, callbacks=[lr_schedule])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;batch_size 의 변경 (256) : loss 에 노이즈가 생길 경우 고려해볼 파라미터&lt;/li&gt;
  &lt;li&gt;하이퍼파라미터를 다양하게 실험해 봐야 함&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Real-world time series data</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 03. Recurrent Neural Networks for Time Series</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 03. Recurrent Neural Networks for Time Series" /><published>2022-10-25T14:00:00+09:00</published><updated>2022-10-25T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series/">&lt;h1 id=&quot;recurrent-neural-networks-for-time-series&quot;&gt;Recurrent Neural Networks for Time Series&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Recurrent Neural Networks 와 Long Short Term Memory Networks 는 시계열 데이터의 예측과 분류에 매우 유용함&lt;/li&gt;
  &lt;li&gt;Lambda Layer : 신경망 내 임의의 코드를 레이어로 활용할 수 있음 (전처리와 후처리)
    &lt;ul&gt;
      &lt;li&gt;명시적인 전처리 단계로 데이터를 스케일링한 다음 신경망에 넣는 게 아니라 Lambda 레이어를 사용할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conceptual-overview&quot;&gt;Conceptual overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN : 순환 레이어를 포함한 신경망
    &lt;ul&gt;
      &lt;li&gt;시퀀스 입력값을 순차적으로 처리하도록 설계&lt;/li&gt;
      &lt;li&gt;입력값의 형태 : 배치 사이즈, 타임스탬프 (윈도우사이즈), 컬럼디멘전 (다변량) = 3차원
        &lt;ul&gt;
          &lt;li&gt;지금까지 사용한 입력값 형태 : 배치 사이즈, 입력값 특징 수 (윈도우 사이즈)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN Cell
    &lt;ul&gt;
      &lt;li&gt;겉으로 보기에는 셀이 많은 것 같지만, 셀은 하나 뿐이고 이를 반복적으로 사용하여 출력값을 산출&lt;/li&gt;
      &lt;li&gt;입력값이 2개 (X 값과 상태벡터 H 값) - 상태벡터값을 이용해 이전 입력값의 잔존 데이터를 전달받음&lt;/li&gt;
      &lt;li&gt;입력차원 (예: 타임스탬프가 30개) 만큼 반복&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rnn-notebook&quot;&gt;RNN Notebook&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shape-of-the-inputs-to-the-rnn&quot;&gt;Shape of the inputs to the RNN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터의 형태, 데이터를 분할한 배치
    &lt;ul&gt;
      &lt;li&gt;예시
        &lt;ul&gt;
          &lt;li&gt;Window size 가 30 : 시간 단계가 30&lt;/li&gt;
          &lt;li&gt;4개로 일괄 처리 : 배치값 4&lt;/li&gt;
          &lt;li&gt;입력 형태는 4 * 30 * 1&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;셀의 관점
        &lt;ul&gt;
          &lt;li&gt;하나의 셀은 고정된 시간 단계에서 (Batch Size : 4 * 1) 의 입력을 받음&lt;/li&gt;
          &lt;li&gt;레이어 내 메모리셀이 3개의 뉴런으로 구성된다면&lt;/li&gt;
          &lt;li&gt;출력값 행렬은 4 * 3&lt;/li&gt;
          &lt;li&gt;출력 형태는 4(Batch Size) * 30(Window Size) * 3(Unit Size)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;단순 RNN 에서의 상태 출력값 H 는 출력값 행렬 Y 와 동일함&lt;/li&gt;
      &lt;li&gt;일부 경우에는 시퀀스를 입력하되, 출력값의 경우 배치 내 각 인스턴스에 대한 단일 벡터를 얻고 싶은 경우가 있음
        &lt;ul&gt;
          &lt;li&gt;마지막 (마지막 시퀀스 스텝-Window) 을 제외하고 모든 출력값을 무시&lt;/li&gt;
          &lt;li&gt;시퀀스 출력값을 도출하려면 레이어를 생성할 때 return_sequences 를 True 로 지정해야 함
            &lt;ul&gt;
              &lt;li&gt;하나의 RNN 레이어를 다른 레이어 위에 스태킹 할때 이 작업이 반드시 필요&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outputting-a-sequence&quot;&gt;Outputting a sequence&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;적층 예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  model = tf.keras.models.Sequential([
	tf.keras.layers.SimpleRNN(40, return_sequences=True, input_shape = [None,1]),
	tf.keras.layers.SimpleRNN(40),
	tf.keras.layers.Dense(1),
  ])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;타 RNN 레이어에 입력으로 들어가야 하는 RNN 레이어에 return_sequences 를 True 로 설정&lt;/li&gt;
      &lt;li&gt;Dense 레이어에 입력으로 들어가야 하는 RNN 레이어는 마지막 시퀀스 단계의 결과값만을 출력&lt;/li&gt;
      &lt;li&gt;input_shape (배치 사이즈) 를 설정하지 않음 : 어떤 크기든 상관이 없으니 정의할 필요가 없음&lt;/li&gt;
      &lt;li&gt;Timestamp 값을 None 으로 설정 : 시퀀스 길이와 관계 없이 입력값을 받음&lt;/li&gt;
      &lt;li&gt;마지막 차원이 1로 되어있는 이유 : 일변량 시계열을 다루기 때문&lt;/li&gt;
      &lt;li&gt;두번째 층 RNN 레이어에 return_sequences 값을 True 로 설정할 경우
        &lt;ul&gt;
          &lt;li&gt;시퀀스 값이 출력됨&lt;/li&gt;
          &lt;li&gt;Keras 는 각 시간 단계별로 동일한 Dense 레이어를 독립적으로 활용함&lt;/li&gt;
          &lt;li&gt;입력값이 시퀀스이고 출력값 또한 시퀀스일 경우 : 시퀀스 to 시퀀스 RNN&lt;/li&gt;
          &lt;li&gt;차원의 값은 RNN 레이어의 유닛 값에 따라 변동될 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lambda-layers&quot;&gt;Lambda layers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  model = tf.keras.models.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),
                        input_shape=[window_size]),
    tf.keras.layers.SimpleRNN(40, return_sequences=True),
    tf.keras.layers.SimpleRNN(40),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;첫 lambda 레이어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.expand_dims(x, axis=-1)&lt;/code&gt; : 기존 window 생성 function 을 그대로 활용하기 위해 차원을 하나 늘림 (2차원-&amp;gt;3차원)&lt;/li&gt;
      &lt;li&gt;마지막 lambda 레이어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lambda x: x * 100.0&lt;/code&gt; : RNN 의 기본 활성함수 tanh 의 출력값 -1 ~ 1 &amp;gt; 시계열 값은 10개 단위로 구성되고, 비슷한 값으로 출력값을 올리면 학습에 도움이 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adjusting-the-learning-rate-dynamically&quot;&gt;Adjusting the learning rate dynamically&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  train_set = windowed_dataset(x_train, window_size, batch_size=128,
      shuffle_buffer=shuffle_buffer_size)

  model = tf.keras.models.Sequential([
          tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1), input_shape=[None]),
          tf.keras.layers.SimpleRNN(40, return_sequences=True),
          tf.keras.layers.SimpleRNN(40),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 100.0)
      ])

  lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))

  optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)

  model.compile(loss=tf.kears.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

  history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;callback 함수를 활용, epoch 진행 별로 학습률을 약간 변경&lt;/li&gt;
      &lt;li&gt;Huber 손실함수 : 이상치에 덜 민감하게 반응하는 손실함수, 데이터에 노이즈가 많이 섞여있을 때 시도해볼만 함
        &lt;ul&gt;
          &lt;li&gt;squared error loss 보다 이상치에 덜 민감함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lstm&quot;&gt;LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN
    &lt;ul&gt;
      &lt;li&gt;X 가 셀에 투입되면 Y 결과값과 H 상태벡터가 출력되고, 이는 다음 셀에 영향을 줌&lt;/li&gt;
      &lt;li&gt;Step 이 진행되면서 초기 H 상태벡터의 영향도는 점점 작아짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM
    &lt;ul&gt;
      &lt;li&gt;전체 훈련 기간 동안 상태를 유지해주는 셀 상태를 추가함&lt;/li&gt;
      &lt;li&gt;상태 값이 셀 간에 이동을 하고 Step 사이를 이동하면서 더 잘 유지될 수 있게함 - 앞 단계에 있던 데이터가 전체 추정치에 더 큰 영향을 줌&lt;/li&gt;
      &lt;li&gt;상태는 양방향으로 움직일 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;coding-lstms&quot;&gt;Coding LSTMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.backend.clear_session()&lt;/code&gt; : 내부 변수를 초기화. 이후 버전에 영향을 주지 않고 여러 모델을 시험해 볼 수 있음&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))&lt;/code&gt; : 32개 셀의 단일 LSTM 레이어 추가. 예측에 미치는 영향을 파악할 수 있도록 양방향으로 만듦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))&lt;/code&gt; : LSTM 레이어를 한층 더 쌓음, retrun_sequences 를 True 로 설정해야만 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Recurrent Neural Networks for Time Series</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 02. Deep Neural Networks for Time Series</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 02. Deep Neural Networks for Time Series" /><published>2022-10-21T14:00:00+09:00</published><updated>2022-10-21T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series/">&lt;h1 id=&quot;deep_neural_networks_for_time_series&quot;&gt;Deep_Neural_Networks_for_Time_Series&lt;/h1&gt;

&lt;h2 id=&quot;preparing-features-and-labels&quot;&gt;Preparing features and labels&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;시계열에서의 Input 값과 Label 값
    &lt;ul&gt;
      &lt;li&gt;Features : 입력되는 값
        &lt;ul&gt;
          &lt;li&gt;예 : 이전 30개의 시점의 값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Labels : 예측할 값 (정답)
        &lt;ul&gt;
          &lt;li&gt;예 : 미래 시점의 값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)
	dataset = tf.data.Dataset.range(10)

	# Window the data but only take those with the specified size
	dataset = dataset.window(5, shift=1, drop_remainder=True)

	# Flatten the windows by putting its elements in a single batch
	dataset = dataset.flat_map(lambda window: window.batch(5))

	# Create tuples with features (first four elements of the window) and labels (last element)
	dataset = dataset.map(lambda window: (window[:-1], window[-1]))

	# Shuffle the windows
	dataset = dataset.shuffle(buffer_size=10)

	# Create batches of windows ( 한번에 여러 데이터 처리를 위함 )
	dataset = dataset.batch(2).prefetch(1)

	# Print the results
	for x,y in dataset:
	  print(&quot;x = &quot;, x.numpy())
	  print(&quot;y = &quot;, y.numpy())
	  print()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;feeding-windowed-dataset-into-neural-network&quot;&gt;Feeding windowed dataset into neural network&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
	    &quot;&quot;&quot;Generates dataset windows

	    Args:
	      series (array of float) - contains the values of the time series
	      window_size (int) - the number of time steps to include in the feature
	      batch_size (int) - the batch size
	      shuffle_buffer(int) - buffer size to use for the shuffle method

	    Returns:
	      dataset (TF Dataset) - TF Dataset containing time windows
	    &quot;&quot;&quot;
	  
	    # Generate a TF Dataset from the series values
	    dataset = tf.data.Dataset.from_tensor_slices(series)
	    
	    # Window the data but only take those with the specified size
	    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
	    
	    # Flatten the windows by putting its elements in a single batch
	    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))

	    # Create tuples with features and labels 
	    dataset = dataset.map(lambda window: (window[:-1], window[-1]))

	    # Shuffle the windows 
	    dataset = dataset.shuffle(shuffle_buffer) // shuffle buffer 의 크기만큼 이동하면서 buffer 내에서 무작위로 하나씩 선택 (선택속도증가)
	    
	    # Create batches of windows
	    dataset = dataset.batch(batch_size).prefetch(1)
	    
	    return dataset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;single-layer-neural-network&quot;&gt;Single layer neural network&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	# Generate the dataset windows
	dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

	# Build the single layer neural network
	l0 = tf.keras.layers.Dense(1, input_shape=[window_size])
	model = tf.keras.models.Sequential([l0])

	# Print the initial layer weights
	print(&quot;Layer weights: \n {} \n&quot;.format(l0.get_weights()))

	# Print the model summary
	model.summary()

	# Set the training parameters
	model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))

	# Train the model
	model.fit(dataset,epochs=100)

	# Print the layer weights
	print(&quot;Layer weights {}&quot;.format(l0.get_weights()))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;machine-learning-on-time-windows&quot;&gt;Machine learning on time windows&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-single-layer-neural-network&quot;&gt;More on single layer neural network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-neural-network-training-tuning-and-prediction&quot;&gt;Deep neural network training, tuning and prediction&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
	dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
	model = tf.keras.models.Sequential([
		tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
		tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
		tf.keras.layers.Dense(1)
	])

	model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
	model.fit(dataset, epochs=100, verbose=0)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;우리가 선택한 학습률이 아니라 최적의 학습률을 선택할 수 있다면 더 좋은 결과가 나올 것
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;callback 을 활용하는 기법&lt;/p&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;		
      dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
      model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
          tf.keras.layers.Dense(1)
      ])
		
      // 각 epoch 종료 시마다 callback 에서 호출, epoch 숫자값을 기준으로 학습률을 값으로 변경
      // 
      lr_schedule = tf.keras.callbacks.LearningRateScheduler(
          lambda epoch: 1e-8 * 10**(epoch / 20))
		
      optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)
		
      model.compile(loss=&quot;mse&quot;, optimizer=optimizer)
			
      history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])
		
      // 트레이닝을 마치고 나면 epoch 당 학습률에 대한 epoch 당 오차를 플로팅
      // x 축은 learning rate, y축은 epoch 의 손실
      lrs = 1e-8 (10 ** (np.arange(100) / 20))
      plt.semilogx(lrs, history.history[&quot;loss&quot;])
      plt.axis([1e-8, 1e-3, 0, 300])
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;위에서 구한 learning_rate (7e-6) 로 재훈련&lt;/p&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
      window_size = 30
      dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
      model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
          tf.keras.layers.Dense(1)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_rate=7e-6, momentum=0.9)
      model.compile(loss=&quot;mse&quot;, optimizer=optimizer)
      model.fit(dataset, epochs=500)

      // 훈련 도중 산출한 손실 플로팅 코드
      loss = history.history[&apos;loss&apos;]
      epochs = range(len(loss))
      plt.plot(epochs, loss, &apos;b&apos;, label=&apos;Training Loss&apos;)
      plt.show()

      // 초기손실 (왜곡값) 자르기
      loss = history.history[&apos;loss&apos;]
      epochs = range(10, len(loss))
      plot_loss = loss[10:]
      print(plot_loss)
      plt.plot(epochs, plot_loss, &apos;b&apos;, label=&apos;Training Loss&apos;)
      plt.show()

      // mean absolute error 값 확인
      tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-neural-network&quot;&gt;Deep neural network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 참조&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Deep_Neural_Networks_for_Time_Series</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 01. Sequences and Prediction</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 01. Sequences and Prediction" /><published>2022-10-20T14:00:00+09:00</published><updated>2022-10-20T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;순차 시계열 데이터 (Sequential time series data)
    &lt;ul&gt;
      &lt;li&gt;값이 시간에 따라 변하는 것&lt;/li&gt;
      &lt;li&gt;예
        &lt;ul&gt;
          &lt;li&gt;주식거래의 종가&lt;/li&gt;
          &lt;li&gt;특정 일의 기온&lt;/li&gt;
          &lt;li&gt;웹사이트의 방문자 수&lt;/li&gt;
          &lt;li&gt;스프레드시트에 기록할 수 있는 데이터&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다룰 내용
    &lt;ul&gt;
      &lt;li&gt;미래 시점의 값 예측을 위한 다양한 방법론&lt;/li&gt;
      &lt;li&gt;위의 내용의 구현법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-a-conversation-with-andrew-ng&quot;&gt;Introduction, A conversation with Andrew Ng&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터의 합성 시퀀스를 만들기&lt;/li&gt;
  &lt;li&gt;데이터 시계열에서 공통 속성을 살펴보기
    &lt;ul&gt;
      &lt;li&gt;계절성 : 날씨의 경우 6월이 1월보다 따듯하고, 11월은 10월보다 습할 수 있음&lt;/li&gt;
      &lt;li&gt;경향성 : 주식의 종가처럼 시간이 가면서 상승, 혹은 하강&lt;/li&gt;
      &lt;li&gt;노이즈 : 무작위 요소&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;구현 : 흑점 활동 모니터
    &lt;ul&gt;
      &lt;li&gt;흑점 활동
        &lt;ul&gt;
          &lt;li&gt;11년, 혹은 22년의 주기 (계절성)&lt;/li&gt;
          &lt;li&gt;노이즈&lt;/li&gt;
          &lt;li&gt;250년 전부터 측정해온 데이터 활용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;sequences-and-prediction&quot;&gt;Sequences and Prediction&lt;/h1&gt;

&lt;h2 id=&quot;time-series-examples&quot;&gt;Time series examples&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 (Time Series) 이란 무엇인가?
    &lt;ul&gt;
      &lt;li&gt;오랜 시간에 걸쳐 균등한 간격으로 순서가 지정된 시퀀스로 나타나는 값&lt;/li&gt;
      &lt;li&gt;다변량 시계열 : 각 시점에서 복수 개의 값이 표시된 경우
        &lt;ul&gt;
          &lt;li&gt;데이터에 추가값을 더하여 상관관계를 파악할 수 있음
            &lt;ul&gt;
              &lt;li&gt;시간의 흐름에 따른 기온과 이산화탄소 배출량의 상관관계&lt;/li&gt;
              &lt;li&gt;자동차의 이동경로 (동시간의 간격 (속도), 위도, 경도 등)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;machine-learning-applied-to-time-series&quot;&gt;Machine learning applied to time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;머신러닝으로 가능한 시계열 관련 작업
    &lt;ul&gt;
      &lt;li&gt;데이터를 기반으로 한 예측 작업&lt;/li&gt;
      &lt;li&gt;이미 가지고 있는 데이터보다 이전 시점의 데이터를 예측&lt;/li&gt;
      &lt;li&gt;실질적으로 존재하지 않는 데이터의 시점의 데이터를 예측&lt;/li&gt;
      &lt;li&gt;변칙 감지에 활용&lt;/li&gt;
      &lt;li&gt;패턴의 발견 (예: 음파를 인식)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;common-patterns-in-time-series&quot;&gt;Common patterns in time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;흔하게 나타나는 시계열 패턴 (눈으로 보고 인식하는데 유용)
    &lt;ul&gt;
      &lt;li&gt;추세 : 특정한 방향으로 움직이는 경우 (예: 무어의 법칙)&lt;/li&gt;
      &lt;li&gt;계절성 : 패턴이 예측 가능한 간격으로 반복됨 (예: 쇼핑사이트 방문자수 (주말에 올라감))&lt;/li&gt;
      &lt;li&gt;노이즈 : 전혀 예측이 불가능한 임의의 값들로 구성된 세트&lt;/li&gt;
      &lt;li&gt;자기상관관계 : 시간의 흐름에 따라 과거 혹은 현재의 값이 미래에 영향을 주는 것&lt;/li&gt;
      &lt;li&gt;복합적으로 나타나는 경우&lt;/li&gt;
      &lt;li&gt;비정상시계열 : 명확한 패턴을 보이다가 큰 이벤트로 인해 패턴이 깨지는 경우
        &lt;ul&gt;
          &lt;li&gt;특정 경향성을 보이는 경우에 특정 구간만 학습&lt;/li&gt;
          &lt;li&gt;하지만 현실의 데이터는 단순하지 않음 (패턴이 깨지며 경향이 나타났으나 다시 과거 패턴으로 회귀)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-time-series&quot;&gt;Introduction to time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Colab 파일&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-validation-and-test-sets&quot;&gt;Train, validation and test sets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;예측 모델의 성능 측정
    &lt;ul&gt;
      &lt;li&gt;Fixed Partitioning (고정 파티셔닝) : 시계열을 훈련기간, 검증 기간, 테스트 기간으로 분할
        &lt;ul&gt;
          &lt;li&gt;시계열이 계절성이 있는 경우 각각의 기간에 계절 전체를 포함하고 싶음&lt;/li&gt;
          &lt;li&gt;시간이 지남에 따라 검증 기간의 데이터를 훈련에 사용, 테스트 기간의 데이터로 검증을 하고, 새로운 테스트 기간으로 테스트를 함&lt;/li&gt;
          &lt;li&gt;테스트 기간은 현재 데이터에 가장 영향을 많이 줄 수 있는 데이터. 따라서 테스트 세트를 포기하는 경우가 흔함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;롤 포워드 파티셔닝
        &lt;ul&gt;
          &lt;li&gt;짧은 훈련기간을 가지고, 점점 증가시켜 (한번에 하루, 한번에 한 주) 반복수행&lt;/li&gt;
          &lt;li&gt;검증기간에는 다음 달이나 다음 주를 예측&lt;/li&gt;
          &lt;li&gt;고정 파티셔닝을 여러 번 시행하고 모델을 계속 다듬는 과정&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;metrics-for-evaluating-performance&quot;&gt;Metrics for evaluating performance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;성능을 계산할 지표
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      errors = forcasts - actual // 오차산출
      mse = np.square(errors).mean() // 평균제곱오차 :  가장 일반적인 지표 (음수제거를 하여 에러간 상쇄가 없도록 함)
      rmse = np.sqrt(mse) // 평균제곱근오차 : 원래 에러 규모와 동일한 규모를 만들기 위해 제곱근 계
      mae = np.abs(errors).mean() // 평균절대오(편)차 :  제곱 대신 절대값을 사용
      mape = np.abs(errors / x valid).mean() // 평균절대백분율오차 : 절대 오차와 절대값의 평균 비율 (값 대비 오차의 크기를 파악)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;작은 오차보다 큰 오차가 생겼을 경우 비용이 훨씬 크다면 MSE&lt;/li&gt;
      &lt;li&gt;손익이 단순 오차의 크기에 비례한다면 MAE
        &lt;ul&gt;
          &lt;li&gt;케라스에서의 구현
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keras.metrics.mean_absolute_error(x_valid, native_forecast).numpy()&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;moving-average-and-differencing&quot;&gt;Moving average and differencing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;이동 평균을 계산 : 일반적이고 간단한 예측 방법
    &lt;ul&gt;
      &lt;li&gt;노이즈를 제거하고, 원본 시계열을 대략적으로 유추하는 곡선이 도출&lt;/li&gt;
      &lt;li&gt;추세나 계절성을 예측하지는 않음 : 현재 시점에서 미래를 예측하고자 하는 기간 이후에는 단순 예측보다 결과가 저조할 수 있음&lt;/li&gt;
      &lt;li&gt;차분은 이를 피하는 방법 중 하나 : 시계열에서 추세와 계절성을 제거
        &lt;ul&gt;
          &lt;li&gt;즉, 시계열 자체를 연구하는게 아니라 T 시점의 값과 이전 기간의 값 사이의 차이를 연구&lt;/li&gt;
          &lt;li&gt;차분에 이동평균선을 예측하면 이는 차분에 대한 예측일 뿐이고, 원본 시계열에 대한 것은 아님
            &lt;ul&gt;
              &lt;li&gt;뺀 값 (이전 기간의 값) 을 다시 더해줘야 함&lt;/li&gt;
              &lt;li&gt;하지만 이전의 값을 더해줄 때 노이즈도 같이 생기게 됨. &amp;gt; 이동 평균을 이용하여 과거의 노이즈를 제거&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trailing-versus-centered-windows&quot;&gt;Trailing versus centered windows&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Trailing Window (현재 값의 이동 평균을 산출할때)
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t1000 = (t970 + t971 + ... + t999) / 30&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Centered Window (과거 값의 이동 평균을 산출할때) : 정확도가 Trailing Window 보다 높음
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t635 = (t630 + t631 + ... + t640) / 11&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forecasting&quot;&gt;Forecasting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;week-1-working-with-time-series&quot;&gt;Week 1: Working with time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Introduction</summary></entry><entry><title type="html">Jenkins Pipeline - Declarative and IaC approaches for DevOps</title><link href="https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps/" rel="alternate" type="text/html" title="Jenkins Pipeline - Declarative and IaC approaches for DevOps" /><published>2022-09-30T14:00:00+09:00</published><updated>2022-09-30T14:00:00+09:00</updated><id>https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps</id><content type="html" xml:base="https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps/">&lt;h1 id=&quot;후기&quot;&gt;후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_JenkinsPipeline-DeclarativeandIaCapproachesforDevOps_course_info.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;해당 강좌는 영상플레이 시간 2시간 (하지만 영어 강좌여서 3시간 이상) 소요 되는 강좌입니다.&lt;/li&gt;
  &lt;li&gt;장점은 부담되는 양의 자료 (책, 강의 등) 를 보기 전에 짧은 시간 훑어보기에 좋은 강의라는 점입니다.&lt;/li&gt;
  &lt;li&gt;대부분 Declarative Script 로 Jenkinsfile 을 작성하여 활용할 것으로 생각하는 바, 기초에 좋은 강의일듯 싶습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;강좌&quot;&gt;강좌&lt;/h1&gt;

&lt;h2 id=&quot;파이프라인이란-무엇인가&quot;&gt;파이프라인이란 무엇인가?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;정의와 기능
    &lt;ul&gt;
      &lt;li&gt;SCM(Source Code Management) 의 Continuous Delivery 절차를 위한 플러그인 집합&lt;/li&gt;
      &lt;li&gt;제품 개발 라이프사이클 (Submitting Code -&amp;gt; Testing -&amp;gt; Staging -&amp;gt; Deployment …) 에 연관&lt;/li&gt;
      &lt;li&gt;각 단계의 성공 / 실패 여부 제공&lt;/li&gt;
      &lt;li&gt;다양한 타 환경에서의 운영 지원&lt;/li&gt;
      &lt;li&gt;저장소 단계에서 실 환경 배포까지의 자동화&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;파이프라인 작성법
    &lt;ul&gt;
      &lt;li&gt;Pipeline script 를 Jenkins UI 에서 작성&lt;/li&gt;
      &lt;li&gt;Jenkins file 을 통한 작성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;파이프라인 언어
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;declarative&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;scripted&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;jenkinsfile 의 장점
    &lt;ul&gt;
      &lt;li&gt;IaC (Infrastructure as Code)
        &lt;ul&gt;
          &lt;li&gt;application code 와 마찬가지로 취급되어 저장소에 committed 됨&lt;/li&gt;
          &lt;li&gt;저장소의 이점을 누리며, 동시에 어떤 구조로 되어있는지 구성원들이 시각적으로 확인할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;파이프라인-스크립트의-형태와-전역변수&quot;&gt;파이프라인 스크립트의 형태와 전역변수&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;젠킨스 서버 구동
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;java -jar jenkins.war httpPort=8080&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;브라우저에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:8080&lt;/code&gt; 으로 접속&lt;/li&gt;
      &lt;li&gt;기본 계정 로그인 : admin / admin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;new items
    &lt;ul&gt;
      &lt;li&gt;네이밍&lt;/li&gt;
      &lt;li&gt;Pipeline 생성&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pipeline 섹션으로 이동&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Jenkinsfile (Declarative Pipeline)
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  pipeline {
      agent any // Execute this Pipeline or any of its stages, on any available agent.
      stages {
          // stage : conceptually distinct subset or tasks performed throughout the entire pipeline
          stage(&apos;Build&apos;) { // Defines the &quot;Build&quot; stage. (Keyword is flexible)
              // Steps represents a single task.
              // It tells Jenkins what to do at a particular point in time of a particular step in the process.
              steps {
                  //  Perform some steps related to the &quot;Build&quot; stage.
              }
          }
          stage(&apos;Test&apos;) { // Defines the &quot;Test&quot; stage.
              steps {
                  // Perform some steps related to the &apos;&quot;Test&quot; stage.
              }
          }
          stage(&apos;Deploy&apos;) { Defines the &quot;Deploy&quot; stage. 
              steps {
                  // Perform some steps related to the &quot;Deploy&quot; stage.
              }
          }
      }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Environment Variables (Global Variables)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8080/pipeline-syntax/globals#env&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;첫-파이프라인-스크립트-작성과-환경변수-삽입&quot;&gt;첫 파이프라인 스크립트 작성과 환경변수 삽입&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;pipeline 섹션에 코드 작성&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          stages {
              stage(&apos;stage 1&apos;){
                  steps {
                      echo &quot;hello world&quot;
                      echo BUILD_ID // Global Variable
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;Build Now&lt;/li&gt;
      &lt;li&gt;Console Output 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;environment 변수 삽입&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;environment 변수는 최상단에 위치할 수도, stage 안에 존재할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          environment {
              mainenv = &apos;dev&apos;
          }
          stages {
              stage(&apos;stage 1&apos;){
                  steps {
                      echo &quot;hello world&quot;
                      echo BUILD_ID
                      echo &quot;&quot;&quot;mainenv = ${mainenv}&quot;&quot;&quot; // enject template, literals, variables
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          environment {
              mainenv = &apos;dev&apos;
          }
          stages {
              stage(&apos;stage 1&apos;){
                  environment{
                      subenv = &quot;prod&quot;
                  }
                  steps {
                      echo &quot;&quot;&quot;inside stage 1: mainenv = ${mainenv}&quot;&quot;&quot; // enject template, literals, variables
                      echo &quot;&quot;&quot;inside stage 1: subenv = ${subenv}&quot;&quot;&quot;
                  }
              }
              stage(&apos;stage 2&apos;){
                  steps {
                      echo &quot;&quot;&quot;inside stage 2: mainenv = ${mainenv}&quot;&quot;&quot;
                      echo &quot;&quot;&quot;inside stage 2: subenv = ${subenv}&quot;&quot;&quot; // causing error, because of scope.
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;실제-github-저장소를-사용한-파이프라인-스크립트-작성과-build-steps&quot;&gt;실제 Github 저장소를 사용한 파이프라인 스크립트 작성과 build steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;대시보드에서 new item 선택
    &lt;ul&gt;
      &lt;li&gt;Pipeline 생성
        &lt;ul&gt;
          &lt;li&gt;우측 드롭다운 메뉴에서 샘플 코드를 불러올 수 있음&lt;/li&gt;
          &lt;li&gt;Pipeline Syntax 에서 Snippet Generator 활용하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most common MVN build phases&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: left&quot;&gt;Build Phase&lt;/th&gt;
          &lt;th style=&quot;text-align: left&quot;&gt;Description&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;validate&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Validates that the project is correct and all necessary information is available. This also makes sure the dependencies are downloaded.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;compile&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Compiles the source code of the project.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;test&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Runs the tests against the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;package&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Packs the compiled code in its distributable format. such as a JAR.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;install&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Install the package into the local repository, for use as a dependency in other projects locally.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;deploy&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Copies the final package to the remote repository for sharing with other developers and projects.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;작성한 소스코드&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
      	agent any
      	stages {
          	stage(&apos;Setup&apos;){
              	steps {
                   	// this will delete folder whatever the OS platform is.
                  	dir(&apos;jenkins-spring&apos;){
                      	deleteDir()               
                  	}
              	}
          	}
          	stage(&apos;Build&apos;){
              	steps {
                  	// sh : for Linux
                  	// for Windows (copy github source)
                  	bat &apos;git clone https://github.com/rudihinds/jenkins-spring.git&apos;
                  	bat &apos;mvn clean -f jenkins-spring&apos;
              	}
          	}
          	stage(&apos;Test&apos;){
              	steps {
                  	bat &apos;mvn clean test -f jenkins-spring&apos;
              	}
          	}
          	stage(&apos;Deploy&apos;){
              	steps {
                  	bat &apos;mvn clean package -f jenkins-spring&apos;
              	}
          	}
      	}
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;jenkinsfile-을-이용하여-scm-에-파이프라인-연결&quot;&gt;Jenkinsfile 을 이용하여 SCM 에 파이프라인 연결&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;SCM 을 통해 어떻게 파이프라인을 가져올 수 있는지?&lt;/li&gt;
  &lt;li&gt;복잡한 Microarchitecture 구조가 아니라면 보편적으로 Jenkinsfile 은 프로젝트의 root 디렉토리에 있음&lt;/li&gt;
  &lt;li&gt;Github 에 있는 Jenkinsfile 에 작성된 declarative script 와 지금까지 작성한 스크립트의 차이점
    &lt;ul&gt;
      &lt;li&gt;clone 스테이지가 없음 : Github의 Jenkinsfile을 사용한다는 것은 이미 Jenkins 에게 SCM 에서 소스코드를 가져오라고 지시한 것임&lt;/li&gt;
      &lt;li&gt;setup 스테이지가 없음 : 해당 프로세스는 이미 통합되어 있음 (Jenkins가 핸들링)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실습
    &lt;ul&gt;
      &lt;li&gt;새 파이프라인을 만들고, 파이프라인의 정의를 Pipeline script from SCM 으로 설정
        &lt;ul&gt;
          &lt;li&gt;SCM 종류 Git으로 설정&lt;/li&gt;
          &lt;li&gt;Repository URL 설정&lt;/li&gt;
          &lt;li&gt;브랜치 설정&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Build Now
        &lt;ul&gt;
          &lt;li&gt;Declarative: Checkout SCM 스테이지가 자동생성된 것을 확인할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Jenkins" /><category term="Coursera" /><category term="Rudi Hinds" /><category term="Jenkins" /><category term="Declarative and IaC approaches for DevOps" /><category term="젠킨스" /><category term="CI/CD" /><summary type="html">후기</summary></entry><entry><title type="html">Jenkins - Automating your delivery pipeline</title><link href="https://bluesplatter.com/jenkins/Jenkins_Automating_your_dilivery_pipeline/" rel="alternate" type="text/html" title="Jenkins - Automating your delivery pipeline" /><published>2022-09-29T16:00:00+09:00</published><updated>2022-09-29T16:00:00+09:00</updated><id>https://bluesplatter.com/jenkins/Jenkins_Automating_your_dilivery_pipeline</id><content type="html" xml:base="https://bluesplatter.com/jenkins/Jenkins_Automating_your_dilivery_pipeline/">&lt;h1 id=&quot;후기&quot;&gt;후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_Automatingyourdeliverypipeline_course_info.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;해당 강좌는 영상플레이 시간 1시간 (하지만 영어 강좌여서 2시간 이상) 소요 되는 강좌입니다.&lt;/li&gt;
  &lt;li&gt;개인적으로 추천하고 싶지는 않은데, 내용을 떠나 구축된 Cloud 환경의 젠킨스가 구 버전이어서 플러그인과 호환이 되질 않습니다.&lt;/li&gt;
  &lt;li&gt;실습의 절반정도는 영상을 보는 것으로만 하였습니다.&lt;/li&gt;
  &lt;li&gt;장점은 부담되는 양의 자료 (책, 강의 등) 를 보기 전에 짧은 시간 훑어보기에 좋은 강의라는 점입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;jenkins-란&quot;&gt;Jenkins 란?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;DevOps 환경에서 Continuous integration and Delivery 를 가능하게 하는 툴&lt;/li&gt;
  &lt;li&gt;Continuous Delivery
    &lt;ul&gt;
      &lt;li&gt;개발자가 개발한 새 소스코드를 즉시 소스코드 저장소에 반영&lt;/li&gt;
      &lt;li&gt;빌딩, 테스팅, 패키징이 등이 일어나 배포 가능 버전이 생성됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이점
    &lt;ul&gt;
      &lt;li&gt;배포속도가 빨라짐&lt;/li&gt;
      &lt;li&gt;피드백을 빨리 받을 수 있음&lt;/li&gt;
      &lt;li&gt;초기 단게에서 결함을 발견할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;실습&quot;&gt;실습&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;젠킨스 서버에서 사용하기 위한 Maven 설정
    &lt;ul&gt;
      &lt;li&gt;DashBoard 에서 Global Tool Configuration 설정&lt;/li&gt;
      &lt;li&gt;하단의 Maven 설치 (네이밍 포함)&lt;/li&gt;
      &lt;li&gt;Maven : 자바용 프로젝트 빌드, 관리에 사용되는 도구&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;라이브러리의 추가, 라이브러리 버전 동기화의 어려움을 해소&lt;/li&gt;
      &lt;li&gt;프로젝트 생성, 테스트 빌드, 배포 등의 작업을 위함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Spring pet clinic 어플리케이션의 첫번째 Job 생성
    &lt;ul&gt;
      &lt;li&gt;Spring pet clinic 의 깃헙 소스코드 fork 하기&lt;/li&gt;
      &lt;li&gt;Jenkins 의 파이프라인 첫 단계는 compile stage 혹은 build stage 가 될 것&lt;/li&gt;
      &lt;li&gt;New Item&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Freestyle project : 어떠한 제약, 제한조건 없는 프로젝트
        &lt;ul&gt;
          &lt;li&gt;네이밍하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;섹션들
        &lt;ul&gt;
          &lt;li&gt;General&lt;/li&gt;
          &lt;li&gt;Source Code Management&lt;/li&gt;
          &lt;li&gt;Build Triggers&lt;/li&gt;
          &lt;li&gt;Build Environment&lt;/li&gt;
          &lt;li&gt;Build&lt;/li&gt;
          &lt;li&gt;Post-build Actions&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Source Code Management 설정
        &lt;ul&gt;
          &lt;li&gt;Git URL 값 입력&lt;/li&gt;
          &lt;li&gt;Branch 입력 (소스코드가 위치하는 경로)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Build 설정
        &lt;ul&gt;
          &lt;li&gt;Add build step -&amp;gt; Invoke top-level Maven targets : Maven 을 통한 build task 수행&lt;/li&gt;
          &lt;li&gt;Maven 버전 설정 : Global Tool Configuration 에서 네이밍한 Maven 선택&lt;/li&gt;
          &lt;li&gt;Goals 설정 : compile 입력&lt;/li&gt;
          &lt;li&gt;Save
            &lt;ul&gt;
              &lt;li&gt;Build Now&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;#1 빌드 수행&lt;/li&gt;
      &lt;li&gt;버튼 색에 따른 상태
        &lt;ul&gt;
          &lt;li&gt;Blue : 성공(완료)&lt;/li&gt;
          &lt;li&gt;Blinking : 현재 실행중&lt;/li&gt;
          &lt;li&gt;Rend : 실패&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;버튼 옆 arrow -&amp;gt; Console Output 으로 결과 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Delivery Pipeline 생성하기 (Build, Test Stage 생성하기)
    &lt;ul&gt;
      &lt;li&gt;Jenkins -&amp;gt; New Item&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Freestyle project
        &lt;ul&gt;
          &lt;li&gt;네이밍하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Source Code Management 설정
        &lt;ul&gt;
          &lt;li&gt;Git URL 값 입력&lt;/li&gt;
          &lt;li&gt;Branch 입력 (소스코드가 위치하는 경로)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Build 설정
        &lt;ul&gt;
          &lt;li&gt;Add build step -&amp;gt; Invoke top-level Maven targets : Maven 을 통한 build task 수행&lt;/li&gt;
          &lt;li&gt;Maven 버전 설정 : Global Tool Configuration 에서 네이밍한 Maven 선택&lt;/li&gt;
          &lt;li&gt;Goals 설정 : test 입력&lt;/li&gt;
          &lt;li&gt;Save
            &lt;ul&gt;
              &lt;li&gt;Build Now&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;#1 빌드 수행&lt;/li&gt;
      &lt;li&gt;Console Output 에서 테스트 케이스에 대한 성공/실패여부를 볼 수 있음
    - Test Job 의 Configure (Build 수행 후 Test 를 진행하고 싶다)&lt;/li&gt;
      &lt;li&gt;Build Triggers : 어떻게 특정 Job 을 Trigger 할지 설정
        &lt;ul&gt;
          &lt;li&gt;Build after other projects are built
            &lt;ul&gt;
              &lt;li&gt;Projects to watch : Build Job 입력&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Save
            &lt;ul&gt;
              &lt;li&gt;Pipeline 전체를 살펴보기 위해 Plugin 설치하기&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Jenkins -&amp;gt; Manage Jenkins -&amp;gt; Manage Plugins
        &lt;ul&gt;
          &lt;li&gt;Available tab -&amp;gt; search Build pipeline -&amp;gt; install&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Jenkins Dashboard 의 작은 + 버튼 클릭 -&amp;gt; Build Pipeline View
        &lt;ul&gt;
          &lt;li&gt;네이밍&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Pipeline Flow -&amp;gt; Upstream / downstream config
        &lt;ul&gt;
          &lt;li&gt;Initial Job (첫 시작 Job) 설정&lt;/li&gt;
          &lt;li&gt;초록색 Job : 이미 실행된 Job&lt;/li&gt;
          &lt;li&gt;파란색 Job : 아직 실행되지 않은 Job&lt;/li&gt;
          &lt;li&gt;노란색 Job : 실행중인 Job&lt;/li&gt;
          &lt;li&gt;빨간색 Job : Job 실패&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Run&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Codify Pipeline
    &lt;ul&gt;
      &lt;li&gt;Build pipeline 의 코드화 필요성&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;위 활동에서의 Configuration Setup 을 직접 수행할 경우 신뢰성이 떨어짐&lt;/li&gt;
      &lt;li&gt;Jenkins 는 Groovy Script 를 사용함
    - Pipeline Maven integration plugin 설치&lt;/li&gt;
      &lt;li&gt;메이븐과 젠킨스의 통합을 쉽게 해줌&lt;/li&gt;
      &lt;li&gt;scripted pipeline 에서 사용할 많은 Wrapper methods 를 제공
    - New item&lt;/li&gt;
      &lt;li&gt;Pipeline
        &lt;ul&gt;
          &lt;li&gt;Pipeline Tab&lt;/li&gt;
        &lt;/ul&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  node{
      stage(&apos;Checkout&apos;){
          git branch: &apos;main&apos;, url:&apos;https://github.com/HY03/SpringPetClinic.git&apos;
      }
      stage(&apos;Build&apos;){
          withMaven(maven: &apos;M3&apos;){
              sh &apos;mvn compile&apos;
          }
      }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  - node : where you run your job
      + master : where jenkins installed
      + slave : for the distributed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;declarative pipeline
    &lt;ul&gt;
      &lt;li&gt;필요성&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Jenkins 와 엮여있는 Groovy Script 의 어려움
    - New item&lt;/li&gt;
      &lt;li&gt;Pipeline
        &lt;ul&gt;
          &lt;li&gt;Pipeline Tab&lt;/li&gt;
        &lt;/ul&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  pipeline{
      agent{label &apos;master&apos;}
      tools{maven &apos;M3&apos;}
      stages{
          stage(&apos;Checkout&apos;){
              steps{
                  git branch: &apos;main&apos;, url:&apos;https://github.com/HY03/SpringPetClinic.git&apos;&apos;
              }
          }
          stage(&apos;Build&apos;){
              steps{
                  sh &apos;mvn compile&apos;   
              }
          }
          stage(&apos;Test&apos;){
              steps{
                  sh &apos;mvn test&apos;
              }
          }
          stage(&apos;Package&apos;){
              steps{
                  sh &apos;mvn package&apos;
              }
          }
          stage(&apos;Deploy&apos;){
              steps{
                  sh &apos;java -jar /var/lib/jenkins/workspace/PetClinicDeclarativePipeline/target/*.jar&apos;
              }
          }
      }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  - `sh &apos;mvn [goal]&apos;` : 실행할 mvn goal 설정
  - 메이븐은 여러 플러그인으로 구성되어 있으며, 각각의 플러그인은 하나 이상의 goal(명령, 작업)을 포함하고 있다. 
  - Goal은 Maven의 실행 단위이다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Declarative Pipeline 을 Jenkins File 로 저장시켜 저장소에 저장하기
    &lt;ul&gt;
      &lt;li&gt;소스코드 저장소에 Jenkins Pipeline 스크립트를 복사&lt;/li&gt;
      &lt;li&gt;Github 에서 파일 생성 : Jenkinsfile (파일명은 고정)&lt;/li&gt;
      &lt;li&gt;Job 의 Pipeline 설정에서 Pipeline script 대신 Pipeline script from SCM 선택&lt;/li&gt;
      &lt;li&gt;Git 경로 및 Branch 설정&lt;/li&gt;
      &lt;li&gt;Script Path : jenkinsfile 위치 설정&lt;/li&gt;
      &lt;li&gt;Lightweight Checkout 설정 : jenkinsfile 먼저 checkout 한 뒤, jenkinsfile 의 모든 스테이지를 수행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Jenkins" /><category term="Coursera" /><category term="Anju M Dominic" /><category term="Jenkins" /><category term="Automating your delivery pipeline" /><category term="젠킨스" /><category term="CI/CD" /><summary type="html">후기</summary></entry></feed>