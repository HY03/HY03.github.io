<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-12-15T11:24:58+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Bluesplatter</title><subtitle>전문적이지 않은 정보들, 감상, 즉흥적인 내용들</subtitle><author><name>HY03</name><email>hyunik03@gmail.com</email></author><entry><title type="html">Prediction and Control with Function Approximation - 01. Week 1. On-policy Prediction with Approximation</title><link href="http://localhost:4000/ai/reinforcement%20learning/Prediction_and_Control_with_Function_approximation_01_Week1/" rel="alternate" type="text/html" title="Prediction and Control with Function Approximation - 01. Week 1. On-policy Prediction with Approximation" /><published>2023-08-31T10:00:00+09:00</published><updated>2023-08-31T10:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Prediction_and_Control_with_Function_approximation_01_Week1</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Prediction_and_Control_with_Function_approximation_01_Week1/"><![CDATA[<h2 id="강의-개요-과정-로드맵">강의 개요 (과정 로드맵)</h2>

<ul>
  <li>가치함수를 테이블로 표현할 수 있는 경우</li>
</ul>

<p><img src="/assets/images/posts/categorize_reinforcement_learning_1.png" alt="categorize_reinforcement_learning_1" /></p>

<ul>
  <li>가치함수를 테이블로 표현할 수 없는 경우</li>
</ul>

<p><img src="/assets/images/posts/categorize_reinforcement_learning_2.png" alt="categorize_reinforcement_learning_2" /></p>

<h2 id="관련-자료-rlbook-pages-197-209">관련 자료 (RLbook Pages 197-209)</h2>
<ul>
  <li>On-policy Prediction with Approximation (개요)
    <ul>
      <li>이 장에서는 강화학습의 함수 근사의 학습을 다룬다.
        <ul>
          <li>함수 근사는 On-policy 데이터의 상태가치함수를 추정하기 위해 사용</li>
          <li>즉, 알고있는 정책 $\pi$ 로 생성한 경험으로 $v_\pi$ 를 근사하고자 한다.</li>
        </ul>
      </li>
      <li>여기에서 눈여겨볼 점은 가치 함수 근사는 테이블로 나타내지 않고, 파라미터화된 함수의 형태 (벡터 $\textbf{w}$ $\in \mathbb{R}^d$) 를 사용한다는 점이다.</li>
      <li>$\hat{v}(s,$ $\textbf{w}) \approx v_\pi(s)$
        <ul>
          <li>상태 $s$ 와 주어진 가중치 벡터 $\textbf{w}$ 의 근사된 값을 나타냄</li>
          <li>$\hat{v}$ 는 상태의 요소에 대한 선형함수일 수 있고, $\textbf{w}$ 는 요소에 대한 가중치 벡터일 수 있음.</li>
          <li>더 일반적으로 $\hat{v}$ 는 다중 인공 신경망 (multi-layer artificial neural network) 에 의해 계산되는 함수일 수 있고, $\textbf{w}$ 는 모든 층에 연결된 가중치일 수 있음.</li>
          <li>혹은 $\hat{v}$ 는 의사결정나무 (decision tree) 에 의해 계산되는 함수일수 있고, $\textbf{w}$ 는 모든 분기점과 트리의 리프 값을 정의하는 모든 숫자일 수 있음.</li>
        </ul>
      </li>
      <li>일반적으로 가중치의 수 ($\textbf{w}$ 의 차원 수) 는 상태의 수보다 훨씬 작고 ($d \ll | \mathcal{S} |$), 하나의 가중치의 변화는 다수의 상태의 예측값을 바꾸게 된다.
        <ul>
          <li>따라서 하나의 상태가 업데이트되면 일반화되어, 다른 다수의 상태의 값에 영향을 주게 된다.</li>
          <li>이러한 일반화 (generalization) 는 학습을 더욱 강력하게 하는 동시에 이해와 관리를 어렵게 한다.</li>
        </ul>
      </li>
      <li>놀랍게도 강화학습을 함수 근사로 확장하면 전체 상태를 사용할 수 없는 부분적으로 관찰이 가능한 문제에도 적용할 수 있다.
        <ul>
          <li>$\hat{v}$ 에 대한 파라미터화된 함수 형식이 추정값이 상태의 특정 요소에 의존하는 것을 허용하지 않으면, 이는 해당 측면을 관찰할 수 없는 것과 같다.</li>
          <li>실제로 이 책의 이 부분에 제시된 함수 근사를 사용하는 이론적 결과물은 부분적 관찰의 케이스에도 잘 적용된다.</li>
        </ul>
      </li>
      <li>그러나 함수 근사가 할 수 없는 것은 과거 관찰에 대한 기억으로 상태의 표현을 강화하는 것이다.
        <ul>
          <li>이러한 부분의 일부는 섹션 17.3 에서 간략하게 논의하기로 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Value-function Approximation
    <ul>
      <li>이 책의 모든 예측 방법 (prediction methods) 은 특정 상태의 값을 backed-up value 혹은 update target 으로 이동시키는, 추정 가치 함수 (estimated value function) 의 업데이트이다.</li>
      <li>개개의 업데이트를 표기법 $s \mapsto u$ 로 나타내도록 한다.
        <ul>
          <li>$s$ 는 상태 업데이트를 뜻하고, $u$ 는 $s$ 의 추정값이 향하는 update target 을 뜻한다.</li>
          <li>Monte Carlo update : $S_t \mapsto G_t$</li>
          <li>TD update : $S_t \mapsto R_{t+1} + \gamma \hat{v} (S_{t+1},$ ${\textbf{w}}_t)$</li>
          <li>n-step TD update : $S_t \mapsto G_{t:t+n}$</li>
          <li>DP policy-evaluation update : $s \mapsto$ $E_{\pi}$ $[ R_{t+1} + \gamma \hat{v} (S_{t+1},$ ${\textbf{w}}_t)$ $| S_t = s ]$
            <ul>
              <li>$s$ 는 임의의 상태를 뜻하며, $S_t$ 는 실제 경험에서 마주하게 된 상태를 의미한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>지금까지의 실제 업데이트는 $s$ 의 추정 값에 대한 테이블 항목이 $u$ 방향으로 약간 이동되며, 다른 모든 상태의 추정값은 변경되지 않은 채로 남아 있었음.</li>
      <li>이제는 업데이트를 구현하기 위해 임의로 복잡하고 정교한 방법을 허용, $s$ 에서의 업데이트는 다른 많은 상태의 추정 값도 변경되도록 일반화한다.
        <ul>
          <li>이러한 방식으로 입력-출력 예제를 모방하는 방법을 학습하는 기계 학습 방법을 지도 학습 방법 (Supervised learning) 이라고 한다.</li>
          <li>출력이 $u$ 와 같은 숫자인 경우 이 프로세스를 종종 함수 근사라고 한다.</li>
          <li>우리는 각 업데이트의 $s \mapsto g$ 를 훈련 예제로 전달하여 값의 예측을 위해 이러한 방법을 사용한다.</li>
          <li>그런 다음 그들이 생성하는 근사 함수를 가치 추정 함수로 본다.</li>
        </ul>
      </li>
      <li>이러한 방식으로 각 업데이트를 전통적인 학습 예제로 보면, 가치 예측을 위한 광범위한 함수 근사 방법을 사용할 수 있다.
        <ul>
          <li>원칙적으로, 인공신경망, 의사결정트리, 다양한 종류의 다변량 회귀 등 다양한 지도학습의 학습 방법을 모두 사용 가능</li>
          <li>그러나 모든 함수 근사 방법이 강화 학습에 사용하기 적합한 것은 아님.
            <ul>
              <li>정교한 인공신경망과 통계 방법은 다중 패스가 이루어지는 정적 훈련 세트를 가정한다.</li>
              <li>그러나 강화학습에서는 에이전트가 환경 혹은 환경 모델과 상호작용하는 동안 학습이 온라인으로 이루어질수 있다는 것이 중요</li>
              <li>이를 위해 점진적으로 획득된 데이터로부터 효율적으로 학습할 수 있는 방법이 필요함.</li>
            </ul>
          </li>
          <li>또한 강화학습에서는 일반적으로 비정상성 (nonstationary target) (시간에 따라 변하는 목표) 를 처리할 수 있는 함수 근사 방법이 필요함.
            <ul>
              <li>예를 들어 GPI (generalized policy iteration) 기반 방법에서는 $\pi$ 가 변경되는 동안 $q_\pi$ 를 학습하려고 하는 경우가 많음.</li>
              <li>정책이 동일하게 유지되더라도 훈련 예제의 목표 값이 부트스트래핑 방법 (DP, TD 등) 으로 생성된 경우 비정상성임.</li>
              <li>이러한 비정상성을 쉽게 처리할 수 없는 방법은 강화학습에 적합하지 않다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The Prediction Objective ($\overline{VE}$)
    <ul>
      <li>예측을 위한 명시적인 목표
        <ul>
          <li>Tabular case 의 경우
            <ul>
              <li>예측을 위한 명시적인 목표를 지정하지 않았음.</li>
              <li>학습된 가치함수가 실 가치함수와 정확하게 동일해질 수 있기 때문에 예측 품질에 대한 지속적인 측정이 필요하지 않음.</li>
              <li>각 상태에서 학습된 값들은 서로 분리되어있었음 (한 상태의 업데이트가 다른 상태에 영향을 주지 않음)</li>
            </ul>
          </li>
          <li>Function Approximation 의 경우
            <ul>
              <li>근사를 사용하면 한 상태의 업데이트가 다른 많은 상태에 영향을 미치므로 모든 상태의 값을 정확하게 얻는 것은 불가능</li>
              <li>가중치보다 훨씬 더 많은 상태가 있으므로, 한 상태의 추정치를 정확하게 만드는 것은 다른 상태의 추정치를 덜 정확하게 만드는 것을 의미</li>
              <li>이는 어떤 상태에 집중할 지를 정해야 함을 의미</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Mean Squared Value Error, $\overline{VE}$
        <ul>
          <li>즉, $\mu (s)$ 를 정의해야 함.
            <ul>
              <li>상태의 분포 $\mu (s) \geq 0, \sum_s \mu(s) = 1$</li>
              <li>각 상태 $s$ 의 오차에 대해 얼마나 신경 써야할지를 의미</li>
            </ul>
          </li>
          <li>상태 $s$ 에서의 오차는 근사값 $\hat{v} (s,$ $\textbf{w})$ 와 참 값 $v_\pi(s)$ 의 차의 제곱을 의미한다.
            <ul>
              <li>이를 각 상태별 $\mu$ 를 이용해 가중치를 부여하면 아래의 식이 도출됨.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/9_2_1_VE.png" alt="9_2_1_VE" /></p>

        <ul>
          <li>$\overline{VE}($$\textbf{w}) \doteq \sum_{s \in S} \mu(s)$ $[ v_\pi(s) - \hat{v} (s,$ $\textbf{w}) ]^2$
            <ul>
              <li>이 측정값의 제곱근, root $\overline{VE}$ 는 근사값과 실제 값이 얼마나 다른지에 대한 대략적 측정값을 제공하며 도표에서 자주 사용됨.</li>
              <li>위 식에 이미 상태의 분포 값이 들어가 있기에, MSE 값과 연관이 있다고 할 수 있음.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$\mu (s)$ 에 대해
        <ul>
          <li>종종 $\mu (s)$ 는 상태 $s$ 에서 보낸 시간의 비율값이 선택됨.</li>
          <li>on-policy 학습에서 이는 on-policy distribution(분포) 라고 하며, 이 장에서는 이 케이스에 초점을 맞춤</li>
          <li>Continuing tasks 에서 on-policy distribution 은 $\pi$ 아래에서 정상성 분포이다.
            <ul>
              <li>정상성 분포를 가진다고 가정하는 것.</li>
              <li>에피소드에서 무한한 상태를 수집할 수는 없으니, 일정 정도를 수집하여 분포를 구하고 이 분포가 정상 분포인 것을 가정하는 것이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>The on-policy distribution in episodic tasks</p>

        <p><img src="/assets/images/posts/9_2_2_on_policy_distribution.png" alt="9_2_2_on_policy_distribution" /></p>

        <ul>
          <li>에피소드 작업에서 on-policy distribution 은 에피소드의 초기 상태가 선택되는 방식에 따라 약간 다르다.</li>
          <li>$\eta(s) = h(s) + \sum_{\bar{s}} \eta(\bar{s}) \sum_a \pi (a | \bar{s} ) p(s | \bar{s},a)$, for all $s \in S$. (9.2)
            <ul>
              <li>$h(s)$ : 각 상태 s 가 첫 에피소드 시작일 확률</li>
              <li>$\eta(s)$ : 에피소드에서 상태 $s$ 에 머무는 비율 (s에 방문하는 time step 수)</li>
              <li>에피소드가 $s$ 에서 시작하거나 이전 상태 $\bar{s}$ 에서 $s$ 로 전이가 일어난 경우</li>
              <li>위 식은 이전 상태의 time-step 비율을 이용해 현 상태의 비율을 구하는 식이다.</li>
            </ul>
          </li>
          <li>$\eta(s)$ 를 정규화 (on-policy distribution)
            <ul>
              <li>$\mu(s) = \frac{\eta(s)}{\sum_{s’} \eta(s’)}$, for all $s \in S$. (9.3)
                <ul>
                  <li>$s’$ : 상태 $s$ 를 포함한 다른 모든 상태를 의미</li>
                </ul>
              </li>
              <li>이것은 할인이 없는 경우에 대한 것임.
                <ul>
                  <li>할인이 있는 경우 9.2의 식의 두번째 항에 $\gamma$ 를 적용해야 함.</li>
                  <li>여기에서 할인의 역할은 에피소드의 확률적인 시작과 종료를 다루기위한 요소로 사용되는 것임.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>연속적인 케이스와 에피소드 케이스 두 가지의 경우 비슷하게 동작하나, 근사하는 경우 완전히 분리해서 생각해야 한다. (향후 언급)</li>
      <li>위의 $\overline{VE}$ 로 학습목표 지정이 완료
        <ul>
          <li>그러나 $\overline{VE}$ 가 강화학습에 적합한 성능의 목표인지 명확하지 않음.</li>
          <li>가치함수를 학습하는 궁극적인 이유는 더 나은 정책을 찾는 것임.
            <ul>
              <li>이 목적을 위한 최고의 가치함수가 반드시 $\overline{VE}$ 를 최소화하는데 최적인 것은 아님.</li>
            </ul>
          </li>
          <li>지금은 $\overline{VE}$ 에 중점을 둔다.</li>
        </ul>
      </li>
      <li>$\overline{VE}$ 학습목표의 이상적인 목표와 현실적 목표
        <ul>
          <li>이상적인 목표 : 가능한 모든 $\textbf{w}$ 에 대해 $\overline{VE}(w^{*}) \leq \overline{VE}(w)$ 인 전역 최적의 가중치 벡터 $\textbf{w*}$ 를 찾는 것</li>
          <li>위 목표를 찾는 것은 선형과 같은 간단한 함수 근사에서는 가능하나, 인공신경망이나 의사결정트리와 같은 복잡한 함수근사에서는 거의 불가능하다.
            <ul>
              <li>복잡한 함수근사에서는 이에 미치지 못하는, 지역 최적, $\textbf{w*}$ 와 유사한 수렴값을 찾게 된다.</li>
              <li>즉, 모든 상태에 대해 오차를 구해 합한 값 $\overline{VE}$ 의 변수 $\textbf{w}$ 에 대해, 전역최소값을 가지는 $\textbf{w}$ 는 구하기 힘들다는 이야기이다.</li>
            </ul>
          </li>
          <li>이러한 수렴의 보장은 약간만 안심할 수 있지만, 일반적으로 비선형 함수 근사의 가능한 최선으로 여겨지고, 대부분의 경우 이정도로 충분하다.
            <ul>
              <li>강화학습의 관심있는 많은 케이스의 경우 최적에 대한 수렴을 보장하지 않고, 또한 최적치에 대한 지정된 범위 내의 수렴 또한 보장하지 않는다.</li>
              <li>어떠한 방법의 경우 $\overline{VE}$ 로의 무한한 접근이 한계치를 넘어 발산하기도 한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>마지막 두 섹션은 광범위한 가치 추정을 위한 강화학습 방법에 광범위한 함수 근사 방법을 결합하는 프레임워크이다.
        <ul>
          <li>전자의 업데이트를 사용하여 후자에 대한 훈련 예제를 생성한다.</li>
          <li>또한 $\overline{VE}$ 성능 측정을 통한 최소화의 촉진에 대해서 살펴본다.</li>
          <li>가능한 함수 근사 방식의 범위는 모두 다루기에 광범위하며, 대부분의 경우 신뢰할 수 있는 평가나 권장사항을 만들기에 알려진 정보가 너무 적다.
            <ul>
              <li>우리는 몇 가지 가능성에 대해서만 살펴본다.</li>
            </ul>
          </li>
          <li>이 챕터의 나머지 부분에서 경사 원리를 기본으로 한 함수 근사 방법에 집중하며, 이 중에서도 선형 경사 하강법에 중점을 둔다.
            <ul>
              <li>우리는 이러한 방법이 특히 유망하다고 생각하며, 핵심적인 이론적 문제를 드러낸다고 생각한다. (또한 단순하기도 하다.)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Stochastic-gradient and Semi-gradient Methods
    <ul>
      <li>Stochastic Gradient Descent (SGD) : 확률적 경사하강법
        <ul>
          <li>모든 함수 근사화 방법 중 가장 널리 사용되는 방법으로 온라인 강화 학습에 특히 적합함</li>
          <li>확률적이라는 뜻은, 모든 표본을 구해 경사하강법을 진행하는 것이 아닌, 일부 표본 데이터(상태)에 대해서만 적용한다는 의미임.</li>
          <li>경사 하강법에서 가중치 벡터는 고정된 실수 값을 구성요소로 가진 열 벡터이다.
            <ul>
              <li>$\textbf{w} \doteq (w_1, w_2, …, w_d)^\top$
                <ul>
                  <li>여기서 $\top$ 는 전치행렬을 의미한다. 수평의 row 벡터를 수직의 column 벡터로 전환하는 것을 의미</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>근사가치함수 $\hat{v} (s, \textbf{w} )$ 는 모든 $s \in S$ 에 대한 $\textbf{w}$ 의 미분 가능한 함수이다.</li>
          <li>우리는 $\textbf{w}$ 를 매 이산 time-step 의 시리즈 $t = 0,1,2,3,…$ 각각에 업데이트를 할 것이므로, $\textbf{w}_t$ 라는 표기법이 필요하다.</li>
          <li>지금부터 우리는 각 스텝에서 새 예시 $S_t \mapsto v_\pi (S_t)$ (아마도 무작위로 선택된 $S_t$ 와 정책 하의 참 값) 를 관측한다고 가정한다.
            <ul>
              <li>위의 상태 값은 환경과의 상호작용을 통한 연속적인 상태가 되겠지만, 여기에서는 그렇게 가정하지 않는다.</li>
            </ul>
          </li>
          <li>비록 각 상태 $S_t$ 에 정확하게 옳은 참 값 $v_\pi (S_t)$ 이 제공된다 해도 여전히 어려운 문제가 있다.
            <ul>
              <li>우리의 함수 근사기는 한정된 자원과 그로 인한 한정된 해결책을 가지고 있다.</li>
              <li>특히 일반적으로 모든 상태를 가지거나, 정확히 옳은 샘플들을 가지는 $\textbf{w}$ 는 존재하지 않는다.</li>
              <li>추가적으로 우리는 예제에 보여지지 않은 다른 모든 상태에 대해서도 일반화를 해야 한다.</li>
            </ul>
          </li>
          <li>우리는 (9.1) 에 주어진 $\overline{VE}$ 를 최소화 하는동안 각각의 상태가 동일 분포 $\mu$ 로 나타난다고 가정한다.
            <ul>
              <li>이러한 케이스에서 좋은 전략은 관측된 예제에서 에러값을 최소화하는 것을 시도해보는 것이다.</li>
            </ul>
          </li>
          <li>
            <p>확률적 경사 하강법 (SGD, Stochastic Gradient Descent) 은 각 예제 후에 해당 예제에서의 에러를 가장 많이 줄이는 방향으로 가중치 벡터를 소량 조정함으로서 위 방법을 수행한다.</p>

            <p><img src="/assets/images/posts/9_3_1_sgd_vector_adjust_1.png" alt="9_3_1_sgd_vector_adjust_1" /></p>

            <ul>
              <li>$\alpha$ : 양수의 step-size 파라미터</li>
              <li>$w_{t+1} \doteq w_t - \frac{1}{2} \alpha \nabla [  v_\pi(S_t) - \hat{v}(S_t, w_t) ]^2$
                <ul>
                  <li>$v_\pi (S_t)$ : 상태 $S_t$ 에서의 실 가치값</li>
                  <li>$\hat{v} (S_t, w_t)$ : 함수 근사를 통해 예측된 가치</li>
                  <li>손실함수 $\textbf{L}$
                    <ul>
                      <li>$\frac{1}{2} [ v_\pi(S_t) - \hat{v}(S_t, w_t) ]^2$</li>
                      <li>분산 : 편차의 제곱 평균</li>
                    </ul>
                  </li>
                  <li>즉, 위 식은 가중치 벡터를 학습률을 반영한 손실함수의 경사만큼 뺀 값
                    <ul>
                      <li>손실함수의 값을 최소화 하는 방향으로 이동 (기울기의 반대방향)</li>
                    </ul>
                  </li>
                  <li>즉, 위 식에서 구하고자 하는 값은 $w_t -\alpha \nabla \textbf{L}$ 이 된다.</li>
                </ul>
              </li>
              <li>$= w_t + \alpha [ v_\pi (S_t) - \hat{v} (S_t, w_t) ] \nabla \hat{v} (S_t, w_t)$
                <ul>
                  <li>위 식 중 $\nabla \textbf{L}$ 을 구한다는 것의 의미는 손실함수에 대한 $w_t$ 의 기울기 함수를 구한다는 의미 (편미분)</li>
                  <li>$\textbf{L}$ 은 $v_\pi (S_t)$ 와 $\hat{v}(S_t,w_t)$ 의 함수이고, 후자는 $w_t$ 에 의존한다.</li>
                  <li>$\frac{\partial L}{\partial w_t} = \frac{\partial L}{\partial \hat{v}(S_t,w_t)} \cdot \frac{\partial \hat{v}(S_t,w_t)}{\partial w_t}$</li>
                  <li>각 항에 대한 미분
                    <ul>
                      <li>$\frac{\partial L}{\partial \hat{v}(S_t,w_t)} = 2 \cdot \frac{1}{2} [ v_\pi (S_t) - \hat{v}(S_t,w_t) ] \cdot (-1) = \hat{v}(S_t,w_t)-v_\pi (S_t)$</li>
                      <li>$\frac{\partial \hat{v}(S_t,w_t)}{\partial w_t}$</li>
                    </ul>
                  </li>
                  <li>즉, $[ \hat{v} (S_t, w_t) - v_\pi (S_t) ] \nabla \hat{v} (S_t, w_t)$ 이 된다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/9_3_2_sgd_vector_adjust_2.png" alt="9_3_2_sgd_vector_adjust_2" /></p>

            <ul>
              <li>$f( \textbf{w} )$ : 벡터 w 의 함수.</li>
              <li>$\nabla f( \textbf{w} )$ : 벡터의 구성요소에 대한 편미분
                <ul>
                  <li>위 식은 벡터 $\textbf{w}$ 에 대한 함수 $f( \textbf{w} )$ 를 미분하려면 벡터의 구성요소들로 편미분 해야 함을 나타낸다.</li>
                </ul>
              </li>
              <li>이 파생 벡터는 $\textbf{w}$ 에 대한 $f$ 의 경사이다.</li>
            </ul>
          </li>
          <li>SGD 방식은 경사 하강 (gradient descent) 방식이라 하는데, $w_t$ 의 전반적인 단계가 예시의 제곱 에러 (9.4) 의 음의 기울기 값에 비례하기 때문
            <ul>
              <li>이는 오차가 가장 빠르게 감소하는 방향이다.</li>
            </ul>
          </li>
          <li>확률적으로 선택되었을 수 있는 단일 예에서 업데이트가 완료되면 경사 하강법을 확률적이라고 한다.
            <ul>
              <li>많은 예를 통해 작은 단계만큼 수행하다 보면, 전반적인 효과는 $\overline{VE}$ 와 같은 평균 측정값을 최소화할 수 있다.</li>
              <li>한번에 큰 단계로 움직이거나 경사 방향으로 완전히 이동해 해당 예시에서의 에러를 완전히 없앨 수는 없다.
                <ul>
                  <li>모든 상태에 대해 에러가 없을 수 없고, 각각의 다른 상태에서 오직 에러의 균형을 근사화할 수밖에 없기 때문.</li>
                </ul>
              </li>
              <li>$\alpha$ 의 값이 충분히 작다면, SGD 방식은 지역 최적값으로 수렴하는 것을 보장한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Stochastic Gradient Descent (SGD) 의 Bootstrapping 개념에 대해</p>

        <ul>
          <li>t 번째 학습 예제의 목표값을 $U_t \in \mathbb{R}$ 로 정의하면, $S_t \mapsto U_t$ 가 되며, 이때 $v_\pi (S_t)$ 의 참 값은 될 수 없지만, 임의의 근사치일 수는 있다.
            <ul>
              <li>예를 들어 $U_t$ 는 $v_\pi (S_t)$ 의 노이즈 버전일 수 있고, 혹은 $\hat{v}$ 의 부트스트래핑된 목표일 수 있다.</li>
              <li>우리는 이 경우 (9.5) 의 정확한 업데이트를 수행할 수 없으나, $v_\pi (S_t)$ 를 $U_t$ 로 대체함으로써 근사 업데이트를 진행할 수 있다.</li>
            </ul>

            <p><img src="/assets/images/posts/9_3_3_sgd_u_t_update.png" alt="9_3_3_sgd_u_t_update" /></p>

            <ul>
              <li>만약 $U_t$ 가 편향되지 않은 추정값이라면, 만약 각각의 $t$ 에 $\mathbb{E} [U_t | S_t=s ] = v_\pi (S_t)$ 라면 $w_t$ 는 지역 최적에 수렴함을 보장한다. ($\alpha$ 가 충분히 작을 경우)
                <ul>
                  <li>예를 들어 예시의 상태가 정책 $\pi$ 하에 환경과 상호작용 (혹은 시뮬레이션) 된 것이라면, 상태의 참 값의 정의는 리턴 값의 추측 값이기에, 몬테카를로 목표 $U_t \doteq G_t$ 는 편향이 없는 $v_\pi (S_t)$ 의 추정치이다.</li>
                </ul>
              </li>
              <li>따라서 몬테카를로 상태값 예측의 경사하강법 버전은 국소적으로 최적의 솔루션을 찾는 것이 보장된다.</li>
            </ul>
          </li>
          <li>
            <p>gradient monte carlo algorithm 의 psuedo code</p>

            <p><img src="/assets/images/posts/9_3_4_gradient_monte_carlo_algorithm_psuedo_code.png" alt="9_3_4_gradient_monte_carlo_algorithm_psuedo_code" /></p>

            <ul>
              <li>$\hat{v} : S \times \mathbb{R}^d \to \mathbb{R}$
                <ul>
                  <li>$S$ : 상태 공간을 의미</li>
                  <li>$\mathbb{R}^d$ : 실수의 $d$ 차원 벡터 공간을 의미</li>
                  <li>$\mathbb{R}$ : 실수를 의미</li>
                  <li>따라서 $\hat{v}$ 함수는 상태 $s \in S$ 와 실수 벡터 $w \in \mathbb{R}^d$ 의 입력을 받아 실수를 출력한다는 의미이다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Semi-gradient (bootstrapping) methods
            <ul>
              <li>$v_\pi (S_t)$ 의 부트스트래핑 추정 (9.7) 에서 목표가 $U_t$ 로 사용되는 경우 동일한 보장을 얻지 못한다.
                <ul>
                  <li>부트스트래핑 목표 n-step 반환값 $G_{t:t+n}$ 또는 DP 목표 $\sum_{a,s’,r} \pi (a|S_t) p(s’,r | S_t,a) [ r+\gamma \hat{v} (s’,w_t) ]$ 모두 가중치 벡터 $w_t$ 의 현재 값에 따라 달라짐. 이는 해당 대상이 편향될 것이며 실제 경사하강법을 생성하지 않음을 의미</li>
                  <li>이는 (9.4), (9.5) 의 식에서 주요 단계가 $w_t$ 와 독립적인 대상에 의존하는 것에서 알 수 있음.</li>
                  <li>따라서 $v_\pi (S_t)$ 대신 부트스트래핑 추정이 사용된 경우 이 단계는 유효하지 않게 됨.</li>
                  <li>가중치 벡터 $w_t$ 를 변경하면 추정치에 미치는 영향을 고려하지만, 목표에 미치는 영향은 무시함.</li>
                </ul>
              </li>
              <li>따라서 부트스트래핑 방법은 실제로 진정한 경사하강법 사례가 아니며, 경사의 일부만 포함되므로 Semi-gradient 방법이라고 한다.</li>
            </ul>
          </li>
          <li>Semi-gradient (bootstrapping) methods 의 장점
            <ul>
              <li>Semi-gradient (bootstrapping) 방법은 gradient 방법만큼 견고하게 수렴하지는 않지만, 다음 섹션에서 설명하는 선형 사례와 같은 중요한 경우 안정적으로 수렴한다.</li>
              <li>6장과 7장에서 살펴본 것처럼 일반적으로 훨씬 더 빠른 학습을 가능하게 한다.</li>
              <li>에피소드가 끝날 때까지 기다리지 않고도 학습이 지속적이고 온라인으로 이루어질 수 있다.</li>
            </ul>
          </li>
          <li>Semi-gradient TD(0) 의 psuedo code
            <ul>
              <li>목표값 : $U_t \doteq R_{t+1} + \gamma \hat{v} (S_{t+1},w)$</li>
            </ul>

            <p><img src="/assets/images/posts/9_3_5_semi_gradient_td_0_psuedo_code.png" alt="9_3_5_semi_gradient_td_0_psuedo_code" /></p>
          </li>
        </ul>
      </li>
      <li>State Aggregation (Chat GPT 검색내용)
        <ul>
          <li>강화학습에서 Function Approximation을 사용하는 방법 중 하나로 “State Aggregation”이라는 기법이 있다.
            <ul>
              <li>이 방법은 상태 공간(state space)을 더 작은 그룹으로 나누는 것을 의미</li>
              <li>각 그룹은 하나의 상태로 취급되며, 이 방법은 연속적인 상태 공간을 이산적으로 변환하여 함수 근사를 적용할 때 유용</li>
            </ul>
          </li>
          <li>주요 특징과 원리
            <ul>
              <li>상태 그룹화 (State Aggregation): State Aggregation은 상태 공간을 작은 그룹 또는 상태 집합으로 나누는 프로세스. 연속적인 상태를 몇 가지 이산적인 상태로 집계하는 것과 유사</li>
              <li>상태 유사성: 유사한 특성 또는 관찰을 갖는 상태들이 동일한 그룹으로 집계. 이러한 유사성은 도메인 지식 또는 경험에 근거하여 결정</li>
              <li>함수 근사 적용: 이제 상태 그룹화를 통해 작은 상태 집합을 얻었으므로, 각 그룹에 대한 가치 함수를 추정할 수 있음.
                <ul>
                  <li>Function Approximation 기법 중 하나인 선형 함수 근사(linear function approximation) 또는 비선형 함수 근사를 사용하여 각 그룹의 가치 함수를 근사</li>
                </ul>
              </li>
              <li>업데이트 규칙: 상태 그룹화를 사용하면 상태 공간이 단순화되므로 각 그룹의 가치 함수 업데이트가 빨라질 수 있음. 에이전트는 각 상태 그룹의 가치 함수를 개별적으로 업데이트함.</li>
              <li>함수 근사 오류: 그러나 이런 단순화는 정보 손실을 의미하기도 한다. 각 상태 그룹 내에서 상태의 차이나 다양성을 잃을 수 있음. 따라서, State Aggregation을 사용할 때 어떤 정보 손실이 발생할 수 있음을 염두에 두어야 한다.</li>
            </ul>
          </li>
          <li>주의점
            <ul>
              <li>State Aggregation은 강화학습에서 상태 공간이 커서 함수 근사를 적용하기 어려운 경우 유용한 방법 중 하나이다.</li>
              <li>그러나 주의해야 할 점은 그룹화 과정에서 상태 유사성을 올바르게 판단하고, 정보 손실을 최소화하도록 설계해야 한다는 것이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>예제 9.1: State Aggregation on the 1000-state Random Walk
        <ul>
          <li>문제의 가정
            <ul>
              <li>1000 개의 상태를 가진 random walk (예제 6.2, 예제 7.1 참고) 문제를 가정한다.</li>
              <li>상태 : 왼쪽에서 오른쪽으로 1 부터 1000까지 넘버링된 상태, 모든 에피소드는 중앙 (상태 500) 에서 시작</li>
              <li>상태 전이 : 현 상태 값에서 좌측 100 개, 우측 100개의 상태 내의 값 (전이 확률은 모두 동일)</li>
              <li>현 상태가 끝쪽에 가깝다면, 100 개의 이웃 상태보다 더 적은 상태가 있을 것이고, 이 경우 종료 상태에 확률이 합산된다.
                <ul>
                  <li>상태 1에서 왼쪽으로 이동해 종료될 확률 (상태 0) : 0.5</li>
                  <li>상태 950 에서 오른쪽이로 이동해 종료될 확률 (상태 1001) : 0.25</li>
                </ul>
              </li>
              <li>왼쪽 상태 (0) 에서 끝날 경우 보상은 -1, 오른쪽 상태 (1001) 에서 끝날 경우 보상 +1, 기타 다른 상태에서의 보상은 0</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/9_3_6_figure_9_1_1000_state_random_walk_state_aggregation.png" alt="9_3_6_figure_9_1_1000_state_random_walk_state_aggregation" /></p>

        <ul>
          <li>결과의 해석
            <ul>
              <li>Figure 9.1 은 이 작업의 참 가치 함수 $v_\pi$ 를 보여준다.
                <ul>
                  <li>이것은 거의 직선이지만, 종료상태 옆 마지막 100 개의 상태에서 수평으로 굽는 형상이다.</li>
                </ul>
              </li>
              <li>또한 state aggregation 을 한, gradient Monte-Carlo 알고리즘을 통한 최종 근사 가치함수를 보여준다.
                <ul>
                  <li>100000 에피소드, step size $\alpha = 2 \times 10^{-5}$</li>
                  <li>state aggregation 은 1000 개의 상태에서 100개의 상태 묶음 10개 그룹으로 진행 (1-100, 101-200, …)</li>
                </ul>
              </li>
              <li>계단 형태의 결과는 전형적인 state aggregation 의 효과이다.
                <ul>
                  <li>각 그룹에서 근사 가치는 상수이며, 하나의 그룹에서 다음 그룹으로 이동 시 이 상수 값이 급격히 변한다.</li>
                  <li>이 근사 값은 $\overline{VE}$ 의 전역 최소값 (9.1) 과 가깝다.</li>
                </ul>
              </li>
              <li>근사값의 일부는 상태 분포 $\mu$ 에 영향을 받게 된다.
                <ul>
                  <li>예를 들어 가장 극단의 값 (1) 보다 가장 왼쪽 그룹 중 상태 (100) 이 3배 이상 더 강한 가중치가 부여된다.</li>
                  <li>따라서 그룹에 대한 추정치는 상태 (1) 의 참 값보다 상태 (100) 의 참 값 쪽으로 편향되게 된다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Linear Methods
    <ul>
      <li>가치함수가 가중치에 대해 선형인 경우
        <ul>
          <li>함수 근사의 가장 중요한 특수 사례 중 하나는 근사함수 $\hat{v} (\cdot , \textbf{w} )$ 가 가중치 벡터 $\textbf{w}$ 에 대해 선형함수인 경우이다.</li>
          <li>모든 상태에 대응하는, $\textbf{w}$ 와 동일한 수의 구성요소를 가진, 실수 값 벡터 $\textbf{x} (s) \doteq (x_1(s), x_2(s), …, x_d(s))^\top$ 가 있다.</li>
          <li>이때, 선형 근사 상태-가치 함수는 $\textbf{w}$ 와 $\textbf{x} (s)$ 의 내적으로 구한다.</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_1_linear_approximate_inner_product_between_w_and_x.png" alt="9_4_1_linear_approximate_inner_product_between_w_and_x" /></p>

        <ul>
          <li>이러한 경우, 근사가치함수는 가중치에 대해 선형이다, 혹은 선형이다 라고 표현한다.</li>
        </ul>
      </li>
      <li>feature vector
        <ul>
          <li>벡터 $\textbf{x} (s)$ 는 상태 s의 feature vector 라 한다.
            <ul>
              <li>$\textbf{x} (s)$ 의 각각의 구성요소 $x_i (s)$ 는 $x_i : \mathcal{S} \to \mathbb{R}$ 이다.</li>
            </ul>
          </li>
          <li>선형 방법에서 feature 는 근사함수 세트에서 선형 기반을 형성하므로, basis function (기저함수) 가 된다.
            <ul>
              <li>즉, 각각의 feature 가 각각의 차원을 이룬다.</li>
              <li><em>d</em>-dimensional feature vector 를 생성하는 것은 <em>d</em> basis function 을 선택하는 것과 같다.</li>
              <li>features 들은 많은 서로 다른 방식으로 정의되며, 우리는 다음 장에서 몇 가지 가능성에 대해 다룬다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>선형 가치근사함수와 SGD
        <ul>
          <li>선형 가치근사에서 SGD 업데이트를 사용하는 것은 자연스럽다.
            <ul>
              <li>$\nabla \hat{v}(s,\textbf{w}) = \textbf{x}(s)$</li>
            </ul>
          </li>
          <li>이는 선형 근사 상태-가치 함수가 $\textbf{w}$ 와 $\textbf{x} (s)$ 의 내적으로 구하기 때문임.</li>
          <li>그러므로 선형 케이스의 경우 SGD 업데이트 (9.7) 가 간단한 형태로 바뀌게 된다.
            <ul>
              <li>$w_{t+1} \doteq w_t + \alpha [ U_t - \hat{v}(S_t,w_t) ] \textbf{x} (S_t)$</li>
            </ul>
          </li>
          <li>매우 간단하기 때문에 선형 SGD 사례는 수학적 분석에 가장 유리한 사례 중 하나이다.</li>
          <li>모든 종류의 학습 시스템에 대한 거의 모든 유용한 수렴 결과는 선형(또는 더 간단한) 함수 근사 방법에 대한 것이다.
            <ul>
              <li>(이 주장은 선형 가치 함수 근사가 간단하면서도 많은 문제에서 효과적으로 동작한다는 아이디어를 강조한 것입니다. 하지만 모든 문제에 대해 선형 근사가 항상 최적인 것은 아닙니다. 일부 문제에서는 더 복잡한 함수 근사가 필요할 수 있습니다.)</li>
            </ul>
          </li>
          <li>선형의 경우 단 하나의 최적값 (혹은 최적값의 한 세트) 만 있으므로 지역최적값이나 전역최적값에 수렴하는 것이 자동적으로 보장된다.
            <ul>
              <li>즉, 근사함수가 선형인 경우 $\frac{1}{2} [ v_\pi(s) - \hat{v}(s,w) ]^2$ 오차 값이 볼록한 형태를 가지고 하나의 최적값이 존재하게 된다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>선형 가치함수근사와 Gradient Monte Carlo 알고리즘의 경우
        <ul>
          <li>예를 들어 이전 섹션에서 제시된 gradient Monte Carlo 알고리즘은 시간이 지남에 따라 $\alpha$ 가 감소하면 선형함수근사 하에 $\overline{VE}$ 의 전역최적으로 수렴한다.</li>
        </ul>
      </li>
      <li>선형 가치함수근사와 semi-gradient TD(0) 알고리즘의 경우
        <ul>
          <li>마찬가지로 앞서 다룬 semi-gradient TD(0) 도 선형함수근사 하에 수렴하나, SGD 의 일반적 결과와는 다르다.
            <ul>
              <li>가중치 벡터는 전역최적이 아닌 지역최적에 수렴한다. (특히 연속적인 케이스에서 중요)</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/9_4_2_update_semi_gradient_td_0.png" alt="9_4_2_update_semi_gradient_td_0" /></p>

        <ul>
          <li>여기에서 우리는 축약 표기 $\textbf{x}_t = \textbf{x}(S_t)$ 를 사용한다.</li>
          <li>시스템이 정상 상태에 도달하면 어떠한 $\textbf{w}_t$ 에 대해서도 다음 가중치 벡터를 아래와 같이 구할 수 있다.</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_3_semi_gradient_td_0_td_fixed_point_1.png" alt="9_4_3_semi_gradient_td_0_td_fixed_point_1" />
  <img src="/assets/images/posts/9_4_4_semi_gradient_td_0_td_fixed_point_2.png" alt="9_4_4_semi_gradient_td_0_td_fixed_point_2" /></p>

        <ul>
          <li>위의 값을 TD fixed point 라고 한다.
            <ul>
              <li>선형 semi-gradient TD(0) 는 이 지점으로 수렴한다.</li>
            </ul>
          </li>
          <li>위 수식에 대한 이해 (Chat GPT 로 정리한 내 생각)
            <ul>
              <li>즉 Semi-gradient 방식은 업데이트 식의 타겟에 학습 중인 가중치 벡터 $w$ 가 포함되어 수렴이 늦어지나</li>
              <li>근사함수가 선형일 경우 현재 목표 $w_{TD}$ 를 아래의 요소로 계산하여 구할 수 있으므로
                <ul>
                  <li>$\textbf{x}_t$</li>
                  <li>$\textbf{x}_{t+1}$</li>
                  <li>$R_{t+1}$</li>
                </ul>
              </li>
              <li>위 요소에 따라 $w_{TD}$ 의 값이 바뀔 수 있으나 학습해야 할 파라미터 $w$ 가 목표에 없으므로 더 빠르고 안정적인 수렴이 가능하다는 이야기.</li>
            </ul>
          </li>
          <li>Proof of Convergence of Linear TD(0)
            <ul>
              <li>아래의 내용을 이해해보려 하였으나, 너무 복잡하여 본문 내용을 붙여넣는 것으로 갈음한다.</li>
            </ul>

            <p><img src="/assets/images/posts/9_4_5_proof_of_convergence_of_linear_td_0_1.png" alt="9_4_5_proof_of_convergence_of_linear_td_0_1" />
  <img src="/assets/images/posts/9_4_5_proof_of_convergence_of_linear_td_0_2.png" alt="9_4_5_proof_of_convergence_of_linear_td_0_2" /></p>

            <ul>
              <li>마지막으로, 이것은 TD(0) 알고리즘이 수렴할 때 가중치 벡터 $w_{TD}$ 에서의 에러가 전체 상태 공간에서의 최적 에러 $w^*$ 에서의 에러에 대한 한계로 나타나며, $\gamma$ 가 충분히 작을 때 $w_{TD}$ 의 에러가 최적 에러에 근접함을 보여준다.
                <ul>
                  <li>$\overline{VE}(w_{TD}) \leq \frac{1}{1-\gamma} \min_w \overline{VE}(w)$</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>가능한 가장 작은 에러 값에 대한 $\overline{VE}$ 의 경계치</p>

            <p><img src="/assets/images/posts/9_4_6_td_fixed_point_ve_error_bounded.png" alt="9_4_6_td_fixed_point_ve_error_bounded" /></p>

            <ul>
              <li>Chat GPT 로 내가 확인한 내용
                <ul>
                  <li>$w_{TD}$ 는 부트스트래핑으로 얻은 특정 상태에 대한 추정값으로, 실제 값과는 오차가 있을 수 있다.</li>
                  <li>$w$ 는 $w_{TD}$ 와는 별개로, 전체 상태에 대한 가중치 값을 학습하는 것으로, 각 상태의 $w_{TD}$ 로부터 추정된 값과 $w$ 로 계산된 값 사이의 오차 합을 최소화하려고 노력한다.</li>
                  <li>부등식 $\overline{VE}(w_{TD}) \leq \frac{1}{1-\gamma} \min_w \overline{VE}(w)$ 는 $\gamma$ 값이 충분히 작을 때 $w_{TD}$ 의 각 상태에 대한 평균 에러가 전역최적 $w$ 의 평균 에러 값으로 계산된 경계 영역보다 작음을 보장한다는 내용이다.</li>
                </ul>
              </li>
              <li>즉, TD 방법은 $\gamma$ 값이 1에 가까워지면, 점근 성능에 상당한 잠재적 손실이 있을 수 있으나, 종종 몬테카를로 방법에 비해 분산이 크게 줄어들어 더 빠르게 수렴할 수 있다.</li>
              <li>9.14 와 유사한 경계는 다른 정책 부트스트래핑 방법에도 작용되며, Sarsa(0) 와 같은 방법이나 에피소드 작업과 같은 방식도 유사한 경계값이 있음.
                <ul>
                  <li>보상, 기능, step-size 매개변수 감소에 대한 몇 가지 기술적 조건이 있으나 여기에서는 생략함.</li>
                </ul>
              </li>
              <li>이러한 수렴 결과에서 중요한 것은 상태가 정책 분포에 따라 업데이트된다는 것이다.
                <ul>
                  <li>다른 업데이트 분포의 경우 함수 근사를 사용하는 부트스트래핑 방법은 무한대로 발산할 수 있다.</li>
                  <li>이에 대한 예와 가능한 해결 방법은 11장에서 다룬다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>예제 9.2 : Bootstrapping on the 1000-state Random Walk
        <ul>
          <li>이 장에서 관찰한 내용 중 일부를 설명하기 위해 1000개 상태의 랜덤 워크의 예제를 사용 (State aggregation + 선형함수근사)</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_7_figure_9_2_bootstrapping_with_state_aggregation_1000_step_random_walk_example_result.png" alt="9_4_7_figure_9_2_bootstrapping_with_state_aggregation_1000_step_random_walk_example_result" /></p>

        <ul>
          <li>왼쪽 패널은 예제 9.1 과 동일한 State aggregation 을 사용, Semi-gradient TD(0) 에 의한 학습된 최종 값 함수를 보여줌
            <ul>
              <li>TD 근사가 몬테카를로 근사보다 실제 값에서 더 멀다는 것을 알 수 있음</li>
            </ul>
          </li>
          <li>오른쪽 패널은 그림 7.2 (n-step semi-gradient TD + state aggregation) 와 유사한 결과를 보여준다.
            <ul>
              <li>함수근사를 한 것과, tabular 계산을 한 것의 유사성을 보여줌</li>
            </ul>
          </li>
          <li>semi-gradient n-step TD 알고리즘은 tabular n-step TD 알고리즘에서 semi-gradient 함수 근사를 확장한 것으로 psuedocode 는 아래와 같다.</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_8_n_step_semi_gradient_TD_psuedo_code.png" alt="9_4_8_n_step_semi_gradient_TD_psuedo_code" /></p>

        <p><img src="/assets/images/posts/9_4_9_key_equation_n_step_semi_gradient_TD.png" alt="9_4_9_key_equation_n_step_semi_gradient_TD" /></p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="estimating-value-functions-as-supervised-learning">Estimating Value Functions as Supervised Learning</h2>

<h3 id="moving-to-parameterized-functions">Moving to Parameterized Functions</h3>

<ul>
  <li>Tabular Methods
    <ul>
      <li>모든 가능 상태를 표현하는 테이블 형태의 저장공간에 각각의 학습 값을 저장하는 형태</li>
      <li>하지만 실제 세계의 문제들의 경우 이 테이블 저장공간이 추적불가능할 정도로 커지게 된다.
        <ul>
          <li>로봇이 카메라를 통해 세계를 관찰하는 경우, 모든 가능한 이미지를 저장할 수는 없음.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>학습목표
    <ul>
      <li>파라미터화된 함수를 사용하여 근사값을 구하는 법 이해하기</li>
      <li>선형 가치함수근사의 의미를 설명하기</li>
      <li>tabular 케이스 또한 선형 가치함수근사의 특별한 케이스임을 이해하기</li>
      <li>가치함수근사를 파라미터화 하는 많은 방법이 있음에 대해 이해하기</li>
    </ul>
  </li>
  <li>
    <p>다양한 형태로 표현가능한 가치 함수</p>

    <p><img src="/assets/images/posts/value_function_tabular_and_linear_approximation.png" alt="value_function_tabular_and_linear_approximation" /></p>

    <ul>
      <li>좌측은 tabular 형태로 각각의 가치함수값을 가지고 있는 형태
        <ul>
          <li>각각의 상태에 따라 독립된 값을 테이블에 저장하는 형태 (지금까지 학습한 방식)</li>
          <li>학습이 진행됨에 따라 테이블에 저장된 값을 업데이트한다.</li>
        </ul>
      </li>
      <li>우측은 X 와 Y 좌표 값에 따라 X + Y 가치함수를 가진 형태
        <ul>
          <li>이론적으로 우리는 상태를 제공받아 실수를 출력하는 어떠한 함수도 사용할 수 있다.</li>
          <li>하지만 이러한 형태를 가치함수로 사용하기를 원치 않음.
            <ul>
              <li>이 예측치를 수정할 방법이 없음 (학습할 방법이 없음)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Parameterized function
    <ul>
      <li>$\hat{v}(s,\textbf{w}) \approx v_\pi (s)$
        <ul>
          <li>$\textbf{w}$ : weights (가중치) - 함수에 변화를 주기 위한 조정이 가능해짐.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/example_parameterized_value_function.png" alt="example_parameterized_value_function" /></p>

    <ul>
      <li>$\hat{v}$ : 참 가치함수 값을 근사하는 함수의 의미</li>
      <li>$\textbf{w}$ : 함수근사에 대하여 모든 가중치를 파라미터화 한 벡터 값</li>
      <li>여기에서 우리는 모든 상태에 대한 가치함수 값을 저장하는 것이 아닌, 2개의 가중치만을 저장하게 된다.</li>
    </ul>
  </li>
  <li>
    <p>가중치(Weight) 의 변화가 가치함수에 주는 영향</p>

    <p><img src="/assets/images/posts/weight_impact_value_function.png" alt="weight_impact_value_function" /></p>

    <ul>
      <li>tabular case 의 경우 하나의 상태값에 영향을 주지만</li>
      <li>Parameterized function 의 경우 가중치 하나를 변경할 경우 복수 개의 상태에 변화를 준다.</li>
    </ul>
  </li>
  <li>
    <p>Linear Value Function Approximation</p>

    <p><img src="/assets/images/posts/linear_value_function_approximation.png" alt="linear_value_function_approximation" /></p>

    <ul>
      <li>가중치와 어떠한 고정된 속성 (feature) 간의 곱의 합</li>
      <li>위의 식일 간단하게 weight vector $\textbf{w}$ 와 feature vector $\textbf{x} (s)$ 간의 내적 (inner product) 으로 표현한다.</li>
    </ul>
  </li>
  <li>
    <p>Limitations of Linear Value Function Approximation</p>

    <p><img src="/assets/images/posts/limitations_of_linear_value_function_approximation.png" alt="limitations_of_linear_value_function_approximation" /></p>

    <ul>
      <li>위의 linear value function approximation 을 살펴보면 X, Y feature 에 대해서 선형적인 표현만 할 수 있다.</li>
      <li>만약 참 값이 위의 그림과 같다면 X, Y feature 에 대한 선형 함수로는 표현할 방법이 없다.
        <ul>
          <li>외각의 0 을 표현하기 위해서는 $W_1$ 과 $W_2$ 가 0이 되어야만 한다.</li>
          <li>그렇게 된다면 내부의 5를 표현할 방법이 없다.</li>
        </ul>
      </li>
      <li>그러나 우리가 반드시 X, Y 값을 features 로 사용할 필요는 없다.
        <ul>
          <li>즉, Linear value function 은 좋은 특징(features) 값을 가지는 것이 중요하다.
            <ul>
              <li>특징(features) 을 정의하는 데 다양한 효과적인 방법들이 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Tabular case 를 linear function 으로 표현하는 방법</p>

    <p><img src="/assets/images/posts/tabular_value_function_is_linear.png" alt="tabular_value_function_is_linear" /></p>

    <ul>
      <li>각각의 상태를 feature 로 정의한다.</li>
      <li>이에 대응하는 가중치 값과의 내적을 구하면, 왼쪽의 Tabular case 와 동일한 연산이 된다.</li>
    </ul>
  </li>
  <li>
    <p>Nonlinear Function Approximation</p>

    <p><img src="/assets/images/posts/nonlinear_function_approximation_NN.png" alt="nonlinear_function_approximation_NN" /></p>

    <ul>
      <li>신경망 방식 또한 비선형 가치근사 방법 중 하나이다.</li>
      <li>이 또한 parameterized function 중 하나이다.</li>
      <li>상태 S 가 실제 가중치 값들 $\textbf{w}$ 를 통과하며 $\hat{v}(s, \textbf{w})$ 의 연산을 하게 된다.</li>
    </ul>
  </li>
</ul>

<h3 id="generalization-and-discrimination">Generalization and Discrimination</h3>

<ul>
  <li>Generalization 에 대해
    <ul>
      <li>함수 근사에서 가장 중요한 고려사항은 어떻게 상태들을 일반화 (Generalize) 할 것인지 이다.</li>
      <li>Generalization 의 예
        <ul>
          <li>어떤 사람이 특정한 자동차를 운전하는 방법을 배울 경우, 다른 자동차의 운전 방법을 배울 때 처음부터 배우지 않는다.</li>
          <li>혹은 다른 도로에서 운전하거나, 비 오는 도로에서 운전한다고 처음부터 배우지 않는다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>학습목표
    <ul>
      <li>generalization (일반화) 와 discrimination (차별) 의 의미 이해하기</li>
      <li>generalization (일반화) 의 혜택 이해하기</li>
      <li>가치 근사에서 왜 generalization (일반화) 와 discrimination (차별) 모두가 필요한지 설명하기</li>
    </ul>
  </li>
  <li>Generalization
    <ul>
      <li>직관적 의미 : 특정한 상황에서의 지식을 적용하여 광범위한 상황에서의 결론을 도출하는 것</li>
      <li>정책 평가에서의 의미 : 하나의 상태에서 추정값의 업데이트가 다른 상태의 값에도 영향을 주는 것</li>
    </ul>

    <p><img src="/assets/images/posts/generalization_updates_one_state_affect_other_state.png" alt="generalization_updates_one_state_affect_other_state" /></p>

    <ul>
      <li>위의 그림처럼 가령 비슷한 시간이 소요되는, 비슷한 거리의 캔을 수거하러 가는 경우 센서에 의해 다른 값이 읽히더라도 비슷한 값이 도출될 수 있다.
        <ul>
          <li>이러한 경우 위의 두 상태에 대해 가치함수의 일반화를 하길 원한다.</li>
        </ul>
      </li>
      <li>일반화를 통해 더 빠른 학습의 진행이 가능하다. (아직 방문하지 않은 상태에 대해서도 업데이트가 가능함)</li>
    </ul>
  </li>
  <li>Discrimination
    <ul>
      <li>두 개의 상태를 구분하여 두 개의 상태가 다르도록 만드는 능력</li>
    </ul>

    <p><img src="/assets/images/posts/discrimination_make_two_states_different.png" alt="discrimination_make_two_states_different" /></p>

    <ul>
      <li>거리가 같더라도 벽 뒤에 있는지, 벽이 없는지에 따라 상태를 구분해야 한다.</li>
      <li>따라서 비슷한 거리에 있는 캔에 대해 상태를 일반화하는 것도 중요하나, 다른 정보에 따라 상태를 구분하는 것도 중요하다.</li>
    </ul>
  </li>
  <li>
    <p>일반화와 구분에 따른 카테고리화</p>

    <ul>
      <li>Tabular Methods : 구분은 뛰어나나 일반화는 전혀 못함. 각 상태에 대해 독립적인 학습을 진행함.</li>
      <li>Aggregate All States : 모든 상태를 똑같은 상태로 판단. 상태에 대한 학습이 불가함</li>
      <li>$*$ (●:현실적인 목표) : 좋은 구분과 좋은 일반화를 달성한 상태로, 비슷한 상태끼리 학습을 같이 진행하여 빠른 학습을 하고, 상태간 구분을 하여 정확히 근사를 하는 상태
        <ul>
          <li>예를 들어 비슷한 상태그룹을 나타내는 feature 를 표기한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Generalize 방법에 대해</p>

    <p><img src="/assets/images/posts/generalize_chess_how.png" alt="generalize_chess_how" /></p>

    <ul>
      <li>좌측은 극단적인 Generalize 로 모든 체스게임 상태를 동일한 상태로 보고, 승률을 0.5 로 책정한 경우 (안좋은 예측값)</li>
      <li>우측은 Tabular Case 의 경우로 모든 경우의 수에 대해 승률을 책정한 경우 (경우의 수가 너무 많아 불가능)</li>
      <li>우리는 이 사이의 무언가를 원한다.
        <ul>
          <li>비슷한 승률 (비슷한 상태) 끼리의 그룹화는 어려운 질문이다.</li>
          <li>이것은 우리의 알고리즘 성능에 지대한 영향을 미치며, 머신러닝과 강화학습의 중심 화제이다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="framing-value-estimation-as-supervised-learning">Framing Value Estimation as Supervised Learning</h3>

<ul>
  <li>학습목표
    <ul>
      <li>어떻게 가치 측정이 지도학습 문제에 포함될 수 있는지 이해하기</li>
      <li>모든 가치근사방식이 강화학습에 잘 적용될 수 없다는 점을 식별하기</li>
    </ul>
  </li>
  <li>
    <p>Supervised learning</p>

    <p><img src="/assets/images/posts/function_approximation_similarity_supervised_learning.png" alt="function_approximation_similarity_supervised_learning" /></p>

    <ul>
      <li>지도학습도 입력 값과 목표 값을 이용해 함수를 근사시키는 과정은 동일하다.</li>
      <li>학습 세트에 없는 입력값에 대해서, 일반화를 통해 실 가치와 유사한 값을 얻길 원한다.</li>
      <li>이러한 parameterized function 은 여러 형태로 표현될 수 있는데, 그 중 하나가 신경망이다.</li>
    </ul>
  </li>
  <li>
    <p>Monte Carlo 방식과 Supervised Learning 의 유사성</p>

    <p><img src="/assets/images/posts/function_approximation_similarity_MC.png" alt="function_approximation_similarity_MC" /></p>

    <ul>
      <li>Policy Evaluation 에 있어서, Monte Carlo 는 샘플의 리턴값을 이용하여 가치 함수를 추정하는 방식이다.
        <ul>
          <li>이 또한 입력값이 상태, 목표 값이 리턴 값인 지도학습의 일환으로 볼 수 있다.</li>
          <li>또한 모든 상태에서 함수가 참 값과 유사한 예측값을 출력하기를 원한다.</li>
          <li>이는 TD 또한 마찬가지이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>모든 가치근사 방식이 강화학습에 잘 적용될 수 없는 이유</p>

    <ul>
      <li>온라인 업데이트
        <ul>
          <li>에이전트가 환경과 상호작용을 하면서 계속 새 데이터를 만드는 경우 (즉, 처음부터 전체데이터에 접근 가능한 Offline Learning 과 차이가 있음)</li>
          <li>함수 근사를 사용할 때, 해당 방식이 온라인 환경에서 잘 적용될 수 있는지를 생각해 봐야 한다.</li>
          <li>어떠한 근사 방법은 고정된 배치 데이터를 사용해야 하거나, 시간적으로 상관된 데이터 (강화학습은 언제나 상관되어 있다.) 에 맞지 않는 경우가 있다.</li>
        </ul>
      </li>
      <li>부트스트래핑
        <ul>
          <li>타겟 값이 현재의 추측값과 연관이 있을 경우</li>
          <li>계속적으로 타겟 값이 변동되는 경우</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="the-objective-for-on-policy-prediction">The Objective for On-policy Prediction</h2>

<h3 id="the-value-error-objective">The Value Error Objective</h3>

<ul>
  <li>학습목표
    <ul>
      <li>정책 평가를 위한 평균제곱오차 (mean squared value error) 의 목표 이해</li>
      <li>목표에서 상태 분포 (state distribution) 의 역할을 설명하기</li>
    </ul>
  </li>
  <li>An Idealized Scenario
    <ul>
      <li>예를 들어 모든 상태에 대한 참 값을 알 수 있는 상태라고 가정하자.
        <ul>
          <li>${(S_1, v_\pi(S_1)), (S_2, v_\pi(S_2)), (S_3, v_\pi(S_3)), …}$</li>
        </ul>
      </li>
      <li>우리는 이 참 값과 최대한 유사한 값을 출력할 수 있는 근사 함수를 찾아야 한다.
        <ul>
          <li>$\hat{v}(s,\textbf{w}) \approx v_\pi(s)$</li>
        </ul>
      </li>
      <li>하지만 위 근사함수가 모든 상태에서 참 값과 동일한 값을 출력할 수는 없다.
        <ul>
          <li>우리는 가중치 $\textbf{w}$ 값을 조절하여 최대한 좋은 결과를 얻고자 한다.</li>
          <li>우리는 어떠한 측정치를 이용하여 우리의 예측을 보다 정확하게 조절할 필요가 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The Mean Squared Value Error Objective
    <ul>
      <li>문제의 가정
        <ul>
          <li>선형 가치근사함수 $\hat{v}$</li>
          <li>상태는 1차원에 연속적이라고 가정</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/mean_squared_value_error_objective.png" alt="mean_squared_value_error_objective" /></p>

    <ul>
      <li>Squared Value Error
        <ul>
          <li>$[v_\pi(s) - \hat{v}(s, \textbf{w} )]^2$</li>
          <li>참 값과 추정 값 간의 오차를 측정할 수 있는 전통적인 방법</li>
          <li>문제는 하나의 상태에서 오차가 준다면, 다른 상태에서 오차가 늘어날 수도 있다는 점이다.</li>
        </ul>
      </li>
      <li>Sum of Squared Value Error
        <ul>
          <li>$\sum_s [v_\pi(s) - \hat{v}(s, \textbf{w}) ]^2$</li>
          <li>모든 상태에서의 오차 합을 측정</li>
          <li>하지만 과연 모든 상태가 서로 같은 중요도를 가진다고 볼 수 있을까?</li>
        </ul>
      </li>
      <li>Sum of Mean Squared Value Error
        <ul>
          <li>$\sum_s \mu(s) [v_\pi(s) - \hat{v}(s, \textbf{w}) ]^2$</li>
          <li>$\mu(s)$ : 해당 정책 하에 s 상태에 방문한 빈도 수를 전체 빈도수 대비 분수로 나타낸 것
            <ul>
              <li>많이 방문한 상태의 에러 값에 더 많이 집중하고, 드물게 방문한 상태의 에러값에 덜 신경 쓰는 것</li>
              <li>해당 값은 확률분포 값이다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Adapting the Weights to Minimize the Mean Squared Value Error Objective
    <ul>
      <li>$\overline{VE} = \sum_s \mu(s) [v_\pi(s) - \hat{v}(s, \textbf{w})]^2$</li>
      <li>우리는 가중치 $\textbf{w}$ 를 조정하여 Mean Squared Value Error 값을 최대한 작게 만드는 것이 목적이다.</li>
      <li>이 Objective 를 VE bar 라고 한다.</li>
      <li>함수 근사를 이용한 정책의 평가는 특정한 목표값을 정의해야 한다.</li>
      <li>Mean Squared Value Error 는 이러한 목표값 중 하나이다.</li>
    </ul>
  </li>
</ul>

<h3 id="introducing-gradient-descent">Introducing Gradient Descent</h3>

<ul>
  <li>학습목표
    <ul>
      <li>경사하강법의 개념을 이해한다.</li>
      <li>경사하강법이 고정된 한 지점으로 수렴하는 것을 이해한다.</li>
    </ul>
  </li>
  <li>
    <p>Recap : Learning Parameterized Value Functions</p>

    <p><img src="/assets/images/posts/recap_learning_parameterized_value_functions.png" alt="recap_learning_parameterized_value_functions" /></p>

    <ul>
      <li>가중치 $\textbf{w}$ 는 실제 실수 값들로 이루어져 있다.</li>
      <li>위의 연산을 보면, 가중치의 변화는 많은 상태에 영향을 줄 수 있다.</li>
      <li>우리의 목표는 전체 에러값 (Overall value error) 의 최소화이다.</li>
    </ul>
  </li>
  <li>
    <p>Understanding Derivatives (미분 이해하기)</p>

    <p><img src="/assets/images/posts/understanding_derivatives.png" alt="understanding_derivatives" /></p>

    <ul>
      <li>$f$ : function, 여기에서는 위 value error 로 이해</li>
      <li>$W$ : 가중치의 스칼라 파라미터로 이해</li>
      <li>여기에서 미분 값으로 $W$ 값의 지역적 변화에 대해 $f$ 값을 증가시킬지 감소시킬지를 알 수 있다.
        <ul>
          <li>미분 값의 음수, 양수의 여부로 $W$ 포인트에서 $f$ 증가, 감소에 대해 판단</li>
          <li>미분 값의 절대값 크기로 $W$ 포인트에서의 경사 (얼마나 급변하는지) 에 대해 판단</li>
          <li>여기에서 미분 값의 기울기 방향으로 $W$ 를 이동시키는 것은 $f$ 의 값을 증가시키는 방향으로 이동하는 것임</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/gradient_derivatives_in_multiple_dimensions.png" alt="gradient_derivatives_in_multiple_dimensions" /></p>

    <ul>
      <li>$\textbf{w}$ 의 벡터 요소의 수 (차원) 에 따라 여러 차원의 미분값이 존재한다.
        <ul>
          <li>이 각각의 차원들에 대해서도 위의 규칙이 통용된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Example : Gradient of a Linear Value Function</p>

    <p><img src="/assets/images/posts/example_gradient_of_a_linear_value_function_1.png" alt="example_gradient_of_a_linear_value_function_1" /></p>

    <ul>
      <li>이전에 다뤘듯, 선형가치근사함수에서의 가치함수값은 단순 가중치와 상태 feature vector의 내적이었다.</li>
      <li>이 때, feature vector 는 가중치와는 독립적인 값이므로, 미분 값이 해당 상태의 feature vector 그 자체가 되게 된다.</li>
    </ul>

    <p><img src="/assets/images/posts/example_gradient_of_a_linear_value_function_2.png" alt="example_gradient_of_a_linear_value_function_2" /></p>

    <ul>
      <li>목표 값은 $\textbf{w}$ 에 대한 함수이다.
        <ul>
          <li>$\hat{v}$ 가 $\textbf{w}$ 에 대한 함수이기 때문</li>
          <li>우리의 목표는 이 함수 값을 최소화 하는 것이다.</li>
        </ul>
      </li>
      <li>$\alpha$ : 얼마나 움직일지 (step-size) 를 정의. 미분 값은 지역적인 영역에서의 증감만을 보장하기 때문</li>
      <li>적은 양으로 가중치를 조절하다 보면 Gradient 값이 0이 되는 부분이 있는데 이 부분을 지역최소값 (local minumim) 이라 한다.
        <ul>
          <li>해당 가중치 $\textbf{w}$ 가 당장의 근처 값보다 낫다는 것을 보여줌. (하지만 최적의 값은 아닐 수 있음)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Global Minima and Solution Quality
    <ul>
      <li>전역최적값 (global minimum) $\textbf{w}_*$ 에서의 $\hat{v}$ 값이 반드시 참 값일 필요는 없음. (충족하지 않음)
        <ul>
          <li>$\hat{v} \ne v_\pi$</li>
          <li>이것은 function parameterization 의 한계이기도 하고, 목표값(objective)의 설정에도 영향을 받는다고 볼 수 있음</li>
        </ul>
      </li>
      <li>만약 feature vector 값이 상태와 무관하게 언제나 1이라면, Mean Squared Value Error 목표값을 최소화 하는 근사가치함수 (모든 상태에 대해 평균 값을 제공) 를 찾을 수는 있겠지만 이것이 좋은 가치함수라고 볼 수는 없다.</li>
    </ul>
  </li>
</ul>

<h3 id="gradient-monte-for-policy-evaluation">Gradient Monte for Policy Evaluation</h3>

<ul>
  <li>학습목표
    <ul>
      <li>경사하강법과 확률적 경사하강법을 사용하여 오차값을 최소화 하는 방법 이해하기</li>
      <li>가치 추정을 위한 Gradient Monte Carlo 알고리즘의 이해</li>
    </ul>
  </li>
  <li>
    <p>Gradient of the Mean Squared Value Error Objective</p>

    <p><img src="/assets/images/posts/gradient_of_the_mean_squared_value_error_objective.png" alt="gradient_of_the_mean_squared_value_error_objective" /></p>

    <ul>
      <li>첫번째로 목표값 (Objective) 의 Gradient 를 찾아야 한다.
        <ul>
          <li>위의 경우 Mean Squared Value Error 의 Gradient 를 찾아야 한다.</li>
          <li>Mean Squared Value Error : A weighted sum of the squared error over all states.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/from_gradient_descent_to_stochastic_gradient_descent_1.png" alt="from_gradient_descent_to_stochastic_gradient_descent_1" /></p>

    <ul>
      <li>Mean Squared Value Error 에 대한 Gradient 를 계산하는 것은 모든 상태에 대한 합, 모든 상태의 확률분포에 대한 계산을 의미
        <ul>
          <li>일반적으로 실현이 불가능하다.</li>
          <li>대부분의 경우 분포값 $\mu$ 를 알지 못한다.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/from_gradient_descent_to_stochastic_gradient_descent_2.png" alt="from_gradient_descent_to_stochastic_gradient_descent_2" /></p>

    <ul>
      <li>이상적인 설정 - $v_\pi$ 에 접근이 가능한 경우
        <ul>
          <li>명시적으로 $\mu$ 가 없더라도, 정책을 따름으로서 상태를 샘플링할 수 있다.</li>
          <li>정책을 따르면서 얻은 상태에 대한 가중치의 즉각적인 업데이트가 가능하다.</li>
          <li>하나의 차원으로 볼 때, 상태 샘플에 따라 에러값이 늘어날 수도 있지만 점진적으로 개선되어간다.</li>
        </ul>
      </li>
      <li>위와 같은 업데이트 접근법을 확률적 경사하강법 (Stochastic Gradient Descent) 이라 한다.
        <ul>
          <li>즉, 확률적이란 모든 상태에 대한 업데이트가 아닌, 정책을 따라 얻은 샘플링된 상태에 대한 업데이트를 한다는 뜻</li>
          <li>이 확률적 경사하강법은 경사에 대한 노이즈가 있는 근사라고 볼 수 있다.
            <ul>
              <li>계산비용이 훨씬 저렴함</li>
              <li>최소값까지 꾸준한 발전을 이룰 수 있음</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Gradient Monte Carlo</p>

    <p><img src="/assets/images/posts/gradient_monte_carlo.png" alt="gradient_monte_carlo" /></p>

    <ul>
      <li>위의 Stochastic Gradient Descent 에는 한계점이 있다.
        <ul>
          <li>우리는 $v_\pi$ 에 접근할 수 없다.</li>
          <li>이 $v_\pi$ 값을 정책을 따라 얻은 리턴값으로 대체한다. (Monte Carlo 방식)</li>
          <li>생성된 에피소드 샘플에 대하여 가중치 업데이트를 진행한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="state-aggregation-with-monte-carlo">State Aggregation with Monte Carlo</h3>

<ul>
  <li>학습목표
    <ul>
      <li>가치함수의 근사를 위한 state aggregation (상태 집합) 기법 사용법 이해</li>
      <li>state aggregation (상태 집합) 과 함께 Gradient Monte Carlo 방식 적용</li>
    </ul>
  </li>
  <li>
    <p>Random Walk Example</p>

    <p><img src="/assets/images/posts/random_walk_example.png" alt="random_walk_example" /></p>

    <ul>
      <li>문제의 정의
        <ul>
          <li>좌, 우측에 종료상태, 그리고 1 부터 1000 까지의 상태가 있다.</li>
          <li>좌측 종료상태에서 보상 -1, 우측 종료상태에서 보상 +1, 그 외의 상태는 보상 0</li>
          <li>동작은 좌측 혹은 우측으로 100칸까지 이동 가능하며 좌,우 1~100 칸 이동 확률은 uniform random policy 를 따른다.</li>
          <li>첫 시작 지점은 상태 500에서 시작한다.</li>
          <li>discount gamma 값은 1이다. (할인 없음)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>State Aggregation</p>

    <p><img src="/assets/images/posts/state_aggregation.png" alt="state_aggregation" /></p>

    <ul>
      <li>몇몇 상태를 같은 상태로 취급하는 기법</li>
      <li>위의 예시에서 상태가 8개 있는데, 4개의 상태를 같은 상태로 묶어 2개의 상태로 취급하는 기법임.
        <ul>
          <li>즉, 위의 묶음 중 아무 상태가 업데이트 되어도 나머지 3개의 상태가 같이 업데이트된다.</li>
        </ul>
      </li>
      <li>State Aggregation 은 linear function approximation 의 일종이다.</li>
      <li>상태가 많은 경우 학습의 속도가 느려질 수 있는데, 위 기법을 통해 빠르게 학습할 수 있음.</li>
    </ul>
  </li>
  <li>
    <p>How to Compute the Gradient for Monte Carlo with State Aggregation</p>

    <p><img src="/assets/images/posts/compute_gradient_monte_carlo_with_state_aggregation.png" alt="compute_gradient_monte_carlo_with_state_aggregation" /></p>
  </li>
  <li>
    <p>Constructing a State Aggregation for the Random Walk</p>

    <p><img src="/assets/images/posts/monte_carlo_updates_for_a_single_episode.png" alt="monte_carlo_updates_for_a_single_episode" /></p>

    <ul>
      <li>어떻게 집합으로 묶을 것인가?
        <ul>
          <li>State Aggregation 은 상태를 동일 그룹 군으로 묶어 같은 가치 추정을 하도록 만든다.</li>
          <li>즉, 우리는 상태를 묶을 때 그들의 값이 유사할 것이라고 생각되는 상태들을 그룹군으로 묶어야 한다.</li>
          <li>그룹이 작다면 보다 더 정확한 결과를 얻을 것이나, 학습 시간이 더 오래 걸린다.</li>
          <li>Random Walk 문제에서는 1부터 1000까지의 상태를 100개 단위의 그룹 군으로 묶어본다.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/random_walk_one_episode_result.png" alt="random_walk_one_episode_result" /></p>

    <ul>
      <li>첫 에피소드에 대한 해석
        <ul>
          <li>첫 번째 에피소드는 종료 결과 보상 1을 얻었고, 할인이 없기 때문에 모든 상태에 대한 리턴값은 1이 된다.</li>
          <li>속하는 그룹의 가중치 값이 모두 업데이트 된다.</li>
          <li>여러 상태를 오간 뒤 첫번째 에피소드에 대한 가치 추정의 결과는 위 그림과 같다.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/random_walk_final_value_estimates.png" alt="random_walk_final_value_estimates" /></p>

    <ul>
      <li>최종 에피소드 이후 가치 추정 결과
        <ul>
          <li>각 상태그룹에 따라 동일한 근사 값을 가지는 것을 볼 수 있음</li>
          <li>참 값이 근사 값의 중앙을 관통하는 것은, 상태의 확률분포 (극단지점의 상태보다 그렇지 않은 상태의 분포가 더 크다) 의 영향이다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="the-objective-for-td">The Objective for TD</h2>

<h3 id="semi-gradient-td-for-policy-evaluation">Semi-Gradient TD for Policy Evaluation</h3>

<ul>
  <li>학습목표
    <ul>
      <li>함수 근사를 위한 TD update 의 이해</li>
      <li>가치 추정을 위한 Semi-gradient TD(0) 알고리즘의 개요</li>
    </ul>
  </li>
  <li>
    <p>Gradient Monte Carlo 와의 비교</p>

    <p><img src="/assets/images/posts/td_update_for_function_approximation.png" alt="td_update_for_function_approximation" /></p>

    <ul>
      <li>Gradient Monte Carlo 에서는 리턴값 $G_t$ 를 사용하며 이는 편향되지 않은 값이기에 가중치가 지역 최적값에 수렴한다.
        <ul>
          <li>꼭 리턴값이 아니더라도 다른 타겟을 사용할 수 있으며, 이 값이 편향되지 않다면 수렴을 보장한다.</li>
        </ul>
      </li>
      <li>TD 방식에서는 현재 가치 추정값을 타겟으로 하기에 값이 편향된다.
        <ul>
          <li>추정값이기에 참 가치함수와는 값이 다름.</li>
          <li>그렇기에 해당 알고리즘은 에러 값이 지역 최소값에 수렴한다고 보장할 수 없다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>TD target 의 이점
    <ul>
      <li>샘플의 리턴 값보다 분산이 작아 더 빠르게 수렴한다.</li>
    </ul>
  </li>
  <li>TD target 의 이점 (Chat GPT)
    <ul>
      <li>계산 효율성: 함수 근사를 사용한 TD 업데이트는 매우 큰 상태 공간에서도 적용할 수 있다. 대규모의 상태 공간을 전체적으로 계산하는 것보다 훨씬 효율적임.</li>
      <li>활용 가능한 데이터: 실제 상황에서는 종종 완벽한 정보가 제공되지 않는다. 편향된 추정값이라도 현재 사용 가능한 정보를 기반으로 한 업데이트는 여전히 유용할 수 있다.</li>
      <li>탐색적인 측면: 편향된 추정값을 사용하는 것은 다양한 상황을 탐색하고 경험하는 데 도움을 줄 수 있다. 이는 종종 실제 환경에서 더 나은 행동을 선택하는 데 도움이 될 수 있다.</li>
      <li>일반화 가능성: 함수 근사를 사용한 TD 업데이트는 일반화 가능성을 가질 수 있다. 이는 일부 편향된 추정값이라도 일반적인 상황에서 적용 가능한 모델을 생성할 수 있다는 것을 의미함.</li>
    </ul>
  </li>
  <li>
    <p>TD is a semi-gradient method</p>

    <p><img src="/assets/images/posts/td_is_a_semi_gradient_method.png" alt="td_is_a_semi_gradient_method" /></p>

    <ul>
      <li>TD 의 경우 업데이트의 목표 타겟값이 TD target ($R_{t+1} + \gamma \hat{v} (S_{t+1}, \textbf{w})$) 이다.</li>
      <li>목표 타겟값에 가중치 $\textbf{w}$ 가  포함되어 있어, 미분식이 기존의 TD Update 식과 다르게 된다.
        <ul>
          <li>The TD Update : $-(U_t - \hat{v}(S_t, \textbf{w})) \nabla \hat{v}(S_t, \textbf{w})$</li>
          <li>여기에서 TD Update 란 시간차 학습에 의한 실제값과 기대값의 차이를 줄이기 위한 방법을 의미한다.</li>
          <li>함수 근사에서 사용될 경우 위와같은 형태가 됨.</li>
        </ul>
      </li>
      <li>즉, 실제 값과 기대값의 차이가 아닌 기대값과 기대값 사이 TD 오차에 비례하는 위의 식은 gradient descent 방법과는 다르다.</li>
      <li>위의 차이에도 불구하고, TD 는 많은 케이스에서 수렴한다.</li>
    </ul>

    <p><img src="/assets/images/posts/semi_gradient_td_0_psuedocode.png" alt="semi_gradient_td_0_psuedocode" /></p>

    <ul>
      <li>Semi-Gradient TD(0) 의 psuedocode
        <ul>
          <li>TD(0) 는 에피소드가 끝날 때 까지 기다리지 않고, 매 스텝마다 업데이트를 진행한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="comparing-td-and-monte-carlo-with-state-aggregation">Comparing TD and Monte Carlo with State Aggregation</h3>

<ul>
  <li>학습목표
    <ul>
      <li>TD 가 편항된 가치 추정으로 수렴하는 점을 이해</li>
      <li>TD 가 Gradient Monte Carlo 보다 훨씬 빠르게 수렴하는 점을 이해</li>
    </ul>
  </li>
  <li>
    <p>Gradient Monte Carlo 의 경우</p>

    <p><img src="/assets/images/posts/gradient_monte_carlo_vonberge_local_minimum.png" alt="gradient_monte_carlo_converge_local_minimum" /></p>

    <ul>
      <li>더 많은 샘플들로 최적화 할 수록 Mean Squared Value Error 의 지역최소값에 수렴한다.</li>
      <li>이는 value error의 경사로 편향되지 않은 추정값을 사용하기 때문이다.</li>
      <li>이론대로라면, 우리는 이 알고리즘을 긴 시간동안, step-size 파라미터를 decay 하며 진행해야 수렴값을 얻을 수 있다.
        <ul>
          <li>예제에서 상수 step-size 를 사용하여, 지역 최소값에서 계속 진동하는 것을 볼 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Semi-Gradient TD 의 경우</p>

    <p><img src="/assets/images/posts/semi_gradient_td_not_converge_to_a_local_minimum.png" alt="semi_gradient_td_not_converge_to_a_local_minimum" /></p>

    <ul>
      <li>Target 값이 예측값 (정확하지 않은 값) 이므로, 업데이트 값에 편향이 생길 수 있다.</li>
      <li>우리의 가치 근사가 경계값 내에서도 완벽할 수 없으므로, Target 은 편향된 상태로 남게 된다.</li>
      <li>따라서 Semi-Gradient TD 의 Mean Squared Value Error 가 지역 최소값으로 수렴한다는 것을 보장할 수 없다.</li>
      <li>물론 이 편향은 추정이 개선될 수록 줄게 된다.</li>
    </ul>
  </li>
  <li>
    <p>State Aggregation 을 이용한 1000 State Random Walk 문제에서 MC 방식과 TD 방식의 결과 비교</p>

    <p><img src="/assets/images/posts/1000_state_random_walk_semi_gradient_td.png" alt="1000_state_random_walk_semi_gradient_td" /></p>

    <ul>
      <li>1000 State Random Walk 를 값이 수렴할 때까지 진행 (1000 Episodes)</li>
      <li>Value Estimate 값의 변화가 멈추었을 때의 결과를 도식화함.</li>
      <li>Monte Carlo 와 비교하여 값이 정확하지 않다. (편향값 때문)</li>
    </ul>

    <p><img src="/assets/images/posts/1000_state_random_walk_30_episodes_td_mc.png" alt="1000_state_random_walk_30_episodes_td_mc" /></p>

    <ul>
      <li>위 문제를 30 Episodes 만 진행</li>
      <li>TD 와 MC 의 $\alpha$ 값에 큰 차이가 있으므로, 0과 1 사이의 100개 구간으로 시험하여 가장 좋은 결과의 $\alpha$ 를 선택
        <ul>
          <li>TD : 0.22</li>
          <li>MC : 0.01</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="linear-td">Linear TD</h2>

<ul>
  <li>학습목표
    <ul>
      <li>선형 함수 근사 를 사용하여 TD-update 도출</li>
      <li>tabular TD90) 가 linear semi-gradient TD(0) 의 특별한 케이스인 것을 이해하기</li>
      <li>선형 가치함수 근사가 비선형에 대해 가지는 이점 강조</li>
      <li>linear TD 학습의 고정점 (fixed point) 이해</li>
      <li>TD 고정점에서 평균 제곱 오차의 이론적 보증에 대한 설명</li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="On-policy Prediction with Approximation" /><summary type="html"><![CDATA[강의 개요 (과정 로드맵)]]></summary></entry><entry><title type="html">Sample-based Learning Methods - 04. Week 4. Planning, Learning &amp;amp; Acting</title><link href="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_04_Week4/" rel="alternate" type="text/html" title="Sample-based Learning Methods - 04. Week 4. Planning, Learning &amp;amp; Acting" /><published>2023-07-16T10:00:00+09:00</published><updated>2023-07-16T10:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_04_Week4</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_04_Week4/"><![CDATA[<h2 id="관련-자료-rlbook-pages-159-166">관련 자료 (RLbook Pages 159-166)</h2>

<ul>
  <li>Planning and Learning with Tabular Methods : 개요
    <ul>
      <li>이 장에서는 동적 프로그래밍과 휴리스틱 탐색과 같이 환경모델이 필요한 강화학습 방법과, 몬테카를로와 시간차 학습과 같이 환경모델 없이 사용할 수 있는 강화학습 방법에 대한 통합된 관점을 개발한다.</li>
      <li>이러한 방법들은 각각 모델기반(model-based) 과 모델프리(model-free) 강화학습 방법으로 불린다.</li>
      <li>모델기반(model-based) 방법은 주로 계획(planning)에 의존하며, 모델프리(model-free) 방법은 주로 학습(learning)에 의존한다.</li>
      <li>이 두가지 방법 사이에는 실제로 차이가 있지만, 큰 유사점도 있다.
        <ul>
          <li>특히, 두 가지 방법 모두 가치함수를 계산하는 것이 핵심임.</li>
          <li>또한 모든 방법들은 미래 이벤트를 예측하고 backed-up value 를 계산하며 이를 근사 가치함수의 업데이트 대상으로 사용한다.</li>
        </ul>
      </li>
      <li>이전 장에서 몬테카를로 학습과 시간차 학습을 서로 다른 대안으로 소개한 후, n-step 방법으로 통합하는 방법을 보여줌.</li>
      <li>이 장에서는 모델기반 방법과 모델프리 방법의 통합이며, 우선 이들을 구분하여 설명한 후 서로 어떻게 조합될 수 있는지를 탐구한다.</li>
    </ul>
  </li>
  <li>Models and Planning
    <ul>
      <li>모델 (models) 에 대해서
        <ul>
          <li>환경의 모델 : 에이전트가 에이전트의 행동에 환경이 어떻게 응답할 것인지 예측하는 데 사용할 수 있는 것
            <ul>
              <li>특정 상태와 행동이 주어지면, 모델은 결과적인 다음 상태와 다음 보상에 대한 예측을 생성한다.</li>
              <li>즉, 에이전트가 환경과 상호작용하여 얻은 경험을 바탕으로 환경의 동작을 예측하는 수단</li>
            </ul>
          </li>
          <li>분포 모델 (distribution models)
            <ul>
              <li>모델이 확률적인 경우, 가능한 여러 다음 상태와 다음 보상들이 각각 발생할 확률과 함께 존재함</li>
              <li>몇몇 모델들은 모등 가능성과 그 확률에 대한 설명을 생성하며, 이러한 모델들을 분포 모델이라고 한다.</li>
              <li>예를 들어, 주사위 12개의 합을 모델링할 때 분포모델은 모든 가능한 합과 그들이 발생할 확률을 생성한다.</li>
              <li>동적 프로그래밍에서 가정하는 모델의 종류인 MDP 의 역학 $p(s’,r | s,a)$ 는 분포모델이다.</li>
            </ul>
          </li>
          <li>샘플 모델 (sample models)
            <ul>
              <li>어떤 모델들은 확률에 따라 샘플링된 하나의 가능성만 생성하며, 이러한 모델을 샘플 모델이라고 한다.</li>
              <li>예를 들어, 주사위 12개의 합을 모델링할 때 샘플 모델은 이 확률 분포에 따라 샘플링된 개별 합을 생성한다.</li>
              <li>Chapter 5 의 블랙잭 예시에서 사용되는 모델은 샘플 모델이다.</li>
            </ul>
          </li>
          <li>분포 모델과 샘플 모델간의 비교
            <ul>
              <li>분포 모델은 항상 샘플을 생성할 수 있어, 샘플 모델보다 강력하다.</li>
              <li>그러나 많은 경우 샘플 모델을 얻는 것이 분포 모델을 얻는 것 보다 훨씬 쉽다.
                <ul>
                  <li>주사위 12개의 경우가 간단한 예시가 될 수 있는데, 주사위를 굴린 결과를 시뮬레이션하고 합을 반환하는 컴퓨터 프로그램은 쉽게 작성할 수 있지만, 모든 가능한 합과 그들의 확률을 찾는 것은 어렵고 오류가 발생할 수 있다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>모델의 활용
        <ul>
          <li>모델은 경험을 모방하거나 시뮬레이션 하는 데 사용될 수 있다.</li>
          <li>시작 상태와 행동이 주어지면, 샘플 모델은 가능한 전이를 생성하고, 분포 모델은 그들의 확률에 따라 가중치가 주어진 모든 가능한 전이를 생성한다.</li>
          <li>시작 상태와 정책이 주어지면, 샘플 모델은 전체 에피소드를 생성하고, 분포 모델은 모든 가능한 에피소드와 그들의 확률을 생성한다.</li>
          <li>두 경우 모두, 모델은 환경을 시뮬레이션하고 모의 경험을 생성하는데 사용된다고 말한다.</li>
        </ul>
      </li>
      <li>계획 (planning) 에 대해서
        <ul>
          <li>Planning 이라는 용어는 다른 분야에서 여러 가지 방식으로 사용됨.
            <ul>
              <li>강화학습에서는 모델을 입력으로 받아 환경과 상호작용하기 위한 정책을 생성하거나 개선하는 계산 과정을 가리키는 데 사용한다.
                <ul>
                  <li>즉, 모델을 사용하여 최적의 정책을 찾는 과정</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/8_1_1_planning_meaning_1.png" alt="8_1_1_planning_meaning_1" /></p>
          </li>
          <li>상태-공간 계획 (State-space planning)
            <ul>
              <li>최적 정책 또는 목표로의 최적 경로를 찾기 위해 상태 공간을 탐색하는 것</li>
              <li>행동은 상태에서 상태로의 전이를 야기하며, 가치 함수는 상태 위에서 계산됨</li>
              <li>강화학습에서 일반적으로 많이 사용되는 계획 방식으로, 주어진 상태에 가능한 모든 행동을 시뮬레이션하고 그에 따른 예상 보상을 계산하여 좋은 행동을 선택하는 것을 반복
                <ul>
                  <li>상태 공간이 작을 때 효과적, 모델 기반 강화학습 알고리즘 중 일반적으로 사용되는 것</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>계획-공간 계획 (Plan-space planning)
            <ul>
              <li>최적 정책 또는 목표로의 최적 경로를 찾기 위해 계획 공간을 탐색하는 것</li>
              <li>연산자들은 한 계획을 다른 계획으로 변환하며, 가치함수 (있는 경우) 는 계획 공간 위에 정의됨</li>
              <li>계획-공간 계획에는 진화적 방법과 부분순서계획 (partial-order planning) 이 포함됨.
                <ul>
                  <li>이는 계획의 단계별 순서가 완전히 결정되지 않은 인공지능에서의 일반적인 계획 방법임</li>
                  <li>계획 공간 방법은 강화학습의 초점인 확률적 순차 결정 문제에 효율적으로 적용하기 어렵기 때문에, 이에 대해 더 이상 고려하지 않음.</li>
                </ul>
              </li>
              <li>에이전트가 가능한 다양한 계획들을 생성하고 평가함
                <ul>
                  <li>위 방식으로 계획을 다른 계획으로 변환하거나 합성하여 최적의 전략을 도출</li>
                  <li>계획들 사이의 관계를 탐색하고 변환하는 과정을 거쳐 최적의 계획을 찾는데 사용
                    <ul>
                      <li>계획들의 집합과 관계를 모델링하고 분석하는 것이 필요하기 때문에 복잡한 문제에 유용하게 적용</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>모든 상태-공간 계획 방법에 대한 통합된 관점
            <ul>
              <li>상태-공간 계획 방법은 공통된 구조를 가지고 있음
                <ul>
                  <li>모든 상태-공간 계획 방법은 정책 개선을 위한 핵심 중간단계로 가치함수를 계산하는 것을 포함한다.</li>
                  <li>이들은 가치 함수를 시뮬레이션된 경험에 적용하는 업데이트 또는 백업 연산으로 계산한다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/8_1_2_planning_meaning_2.png" alt="8_1_2_planning_meaning_2" /></p>

            <ul>
              <li>동적 계획법의 예
                <ul>
                  <li>상태 공간을 훑으며 각 상태에서 가능한 전이의 분포를 생성</li>
                  <li>각 분포는 업데이트 값 (업데이트 대상) 을 계산하는 데 사용되며, 상태의 추정값이 업데이트됨</li>
                </ul>
              </li>
              <li>다양한 다른 상태-공간 계획 방법들 또한 이러한 구조에 맞으며, 개별적인 방법들은 업데이트 종류, 수행 순서, 얼마나 오래 업데이트 대상을 유지하는지 정도의 차이만 존재</li>
            </ul>
          </li>
          <li>학습(learning) 과 계획 (planning) 방법
            <ul>
              <li>두 방법의 핵심은 백업 업데이트 연산에 의한 가치 함수의 추정임</li>
              <li>차이점은 계획은 모델에서 생성된 시뮬레이션 경험을 사용하는 반면, 학습 방법은 환경에서 생성된 실제 경험을 사용한다는 것임
                <ul>
                  <li>이 차이는 성능 평가 방법과 경험 생성 유연성 등 다른 여러 가지 차이를 유발</li>
                </ul>
              </li>
              <li>그러나 위의 공통된 구조는 계획과 학습 사이에서 많은 아이디어와 알고리즘을 상호 전달할 수 있음을 의미함
                <ul>
                  <li>특히 많은 경우 학습 알고리즘은 계획 방식의 핵심 업데이트 단계를 대체하는 데 사용될 수 있음
                    <ul>
                      <li>학습 방법은 경험만을 입력으로 필요로 하며, 많은 경우 실제 경험과 똑같이 시뮬레이션 경험에도 적용할 수 있음.</li>
                      <li>One-step tabular Q-learning 과 샘플 모델(Sample model) 로부터의 무작위 샘플을 기반으로 한 계획 방법의 psuedo code</li>
                    </ul>

                    <p><img src="/assets/images/posts/8_1_3_ramdom_sample_one_step_tabular_q_planning_psuedo_code.png" alt="8_1_3_ramdom_sample_one_step_tabular_q_planning_psuedo_code" /></p>

                    <ul>
                      <li>위 방법은 ramdom-sample one-step tabular Q-planning 이라 한다.</li>
                      <li>One-step tabular Q-learning 이 실 환경에서 최적 정책으로 수렴하는 것과 동일 조건으로 최적 정책에 수렴을 보장한다.
                        <ul>
                          <li>Step 1 에서 각 상태-행동 쌍이 무한히 선택</li>
                          <li>시간이 지남에 따라 $\alpha$ 가 적절하게 감소</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>작고 점진적인 단계로 계획을 수행
            <ul>
              <li>계획과 학습 방법의 통합적 관점 이외에도, 작고 점진적인 단계로 계획을 수행하는 것은 장점이다.</li>
              <li>계획은 언제든 중단되거나 방향을 바꿀 수 있으며, 거의 낭비되지 않은 계산으로 이루어질 수 있음.
                <ul>
                  <li>이는 계획과 행동, 그리고 모델의 학습을 효율적으로 혼합하기 위한 핵심 요건임</li>
                </ul>
              </li>
              <li>매우 작은 단계로 계획을 수행하는 것은 순수한 계획 문제에서도 가장 효율적인 접근방법일 수 있으며, 만약 문제가 정확하게 해결될 수 없을 정도로 크다면 특히 그러함.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Dyna: Integrated Planning, Acting, and Learning</p>

    <ul>
      <li>개요
        <ul>
          <li>온라인으로 계획을 수행할 때 환경과 상호작용하면서 흥미로운 문제들이 발생함
            <ul>
              <li>상호작용으로 얻은 새로운 정보는 모델을 변경하고, 이로 인해 계획과 상호작용할 수 있다.</li>
              <li>현재 고려 중이거나 가까운 장래에 예상되는 상태 또는 결정에 대해 어떤 식으로든 계획 프로세스를 사용자 지정하는 것이 바람직할 수 있다.</li>
              <li>만약 의사결정과 모델학습이 모두 계산량이 많은 과정이라면, 사용 가능한 계산 자원을 이들 간에 분배해야 할 수도 있음.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Dyna-Q 의 구조</p>

        <p><img src="/assets/images/posts/8_2_1_dyna_q_diagram.png" alt="8_2_1_dyna_q_diagram" /></p>

        <ul>
          <li>Dyna-Q 개요
            <ul>
              <li>온라인 계획 에이전트에 필요한 주요 기능들을 통합한 간단한 구조</li>
              <li>간결하게 기능 구성</li>
              <li>각 기능을 달성하는 다른 대안적인 방법과, 이들 간의 균형점은 다음 섹션에서 다룸</li>
            </ul>
          </li>
          <li>직접적 방법과 간접적 방법
            <ul>
              <li>계획 에이전트 내에서 실제 경험은 적어도 두 가지 역할을 할 수 있음
                <ul>
                  <li>모델 학습 : 모델을 개선하기 위해 사용 (실제 환경과 더 정확하게 일치하도록 만드는 역할)</li>
                  <li>직접적 강화학습 (direct RL) : 강화학습 방법을 사용하여 가치함수와 정책을 직접적으로 개선하기 위해 사용 (이전 장에서 논의한 방법)</li>
                </ul>
              </li>
              <li>계획에 관여하는 것은 간접적 강화학습이라고도 불림.</li>
            </ul>
          </li>
          <li>직접적 방법과 간접적 방법의 장단점
            <ul>
              <li>간접적 방법
                <ul>
                  <li>종종 제한된 경험을 보다 효과적으로 활용하여 더 적은 환경 상호작용으로부터 더 나은 정책을 달성함</li>
                </ul>
              </li>
              <li>직접적 방법
                <ul>
                  <li>간접적 방법보다 훨씬 간단하며, 모델 설계에 영향을 주지 않음</li>
                  <li>대부분의 인간과 동물의 학습에 기여하는 방법</li>
                </ul>
              </li>
              <li>심리학, 인공지능 분야에서는 의사결정과 반복적 학습 간의 상대적 중요성, 계획적 의사 결정과 반응적 의사 결정 간의 상대적 중요성에 대한 논의가 이루어지고 있음.
                <ul>
                  <li>첵의 관점은 이러한 논쟁들에서 대안 간의 대비는 과장된 것으로 보고, 실제 이 두가지 접근 방식 간의 유사성을 인정함으로써 더 많은 통찰력을 얻을 수 있다고 보고있음.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Dyna-Q 에 대하여
            <ul>
              <li>위 다이어그램에 표시된 모든 과정 - 계획, 행동, 모델학습 및 직접적 강화학습 - 이 지속적으로 발생하는 것을 포함한다.</li>
              <li>계획 : random-sample one-step tabular Q-planning (위에 설명함)</li>
              <li>직접 강화학습 (direct RL) : one-step tabular Q-learning</li>
              <li>모델학습 : 테이블 기반, 결정론적 환경이라고 가정
                <ul>
                  <li>각 전이 $S_t, A_t \to R_{t+1}, S_{t+1}$ 이후 모델은 $S_t, A_t$ 에 대한 예측으로 $R_{t+1}, S_{t+1}$ 이 결정론적으로 발생한다는 정보를 기록</li>
                  <li>따라서 모델이 이전에 경험한 상태-행동 쌍으로 쿼리되면, 마지막으로 관찰된 다음 상태와 보상을 예측으로 반환함</li>
                  <li>계획 중 Q-planning 알고리즘은 이전에 경험한 상태-행동 쌍에서만 무작위 샘플링을 수행하기 때문에 모델은 정보가 없는 쌍으로 쿼리되지 않음.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/8_2_2_figure_8_1_general_dyna_architecture.png" alt="8_2_2_figure_8_1_general_dyna_architecture" /></p>

            <ul>
              <li>Dyna-Q 알고리즘을 포함한 Dyna 에이전트의 전체적인 구조
                <ul>
                  <li>중앙 열은 에이전트와 환경 간의 상호작용을 나타내며, 실제 경험의 궤적을 만들어 냄</li>
                  <li>왼쪽 화살표는 실제 경험에 적용하여 가치함수와 정책을 개선하는 직접적 강화학습을 나타냄</li>
                  <li>오른쪽 부분은 모델 기반 프로세스를 나타냄
                    <ul>
                      <li>모델은 실제 경험으로부터 학습되어 모의 경험을 생성함</li>
                      <li>모델이 생성한 모의 경험의 시작 상태와 행동을 선택하는 프로세스를 탐색 제어(search control) 라고 함</li>
                    </ul>
                  </li>
                  <li>계획은 모의 경험 (simulated experience) 에 대해 실제로 일어난 것처럼 강화학습 방법을 적용함으로써 달성됨
                    <ul>
                      <li>일반적으로 (Dyna-Q 도 해당) 실제 경험으로부터 학습하는 데 사용된 강화학습 방법과 모의 경험으로부터 계획하는데 사용되는 강화학습 방법은 동일함</li>
                      <li>따라서 강화학습 방법은 학습, 계획 모두에 대한 공통경로가 됨</li>
                      <li>학습과 계획은 거의 모든 부분을 공유하므로 깊게 통합되어 있고, 경험의 소스만 다르다. (환경, 모델)</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/8_2_3_tabular_dyna_q_psuedo_code.png" alt="8_2_3_tabular_dyna_q_psuedo_code" /></p>

            <ul>
              <li>Tabular Dyna-Q
                <ul>
                  <li>개념적으로 계획, 행동, 모델학습, 직접적인 강화학습은 Dyna 에이전트에서 동시에 병렬적으로 발생한다.</li>
                  <li>그러나 직렬 컴퓨터에서 구현하기 위해 시간 단위 내에 발생하는 순서를 지정한다.
                    <ul>
                      <li>Dyna-Q 에서 행동, 모델 학습 및 직접적 강화학습 프로세스는 거의 계산이 필요하지 않으며, 시간의 일부분만 소비한다고 가정한다.</li>
                      <li>각 단계에서 남은 시간은 계산이 많이 필요한 계획 프로세스에 할당된다.</li>
                      <li>행동, 모델 학습 및 직접적 강화학습 후 각 단계에서 Q-planning 알고리즘의 n 번째 반복 (step 1-3) 을 완료하기 위한 시간이 있다고 가정한다.</li>
                      <li>위 psuedo code 알고리즘에서 $Model(s,a)$ 는 상태-행동 쌍 $(s,a)$ 에 대한 (다음 상태와 보상) 의 내용을 나타낸다.</li>
                      <li>직접 강화학습, 모델 학습 및 계획은 각각 단계 (d), (e), (f) 로 구현된다.</li>
                      <li>(e) 와 (f) 를 생략하면 남은 알고리즘은 one-step tabular Q-learning 이 된다.</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>예제 8.1 : Dyna Maze</p>

        <p><img src="/assets/images/posts/8_2_4_figure_8_2_dyna_maze.png" alt="8_2_4_figure_8_2_dyna_maze" /></p>

        <ul>
          <li>문제의 정의
            <ul>
              <li>그림 8.2의 소형 미로를 고려한다.</li>
              <li>상태 : 47개의 상태</li>
              <li>동작
                <ul>
                  <li>위, 아래, 오른쪽 및 왼쪽으로 이동.</li>
                  <li>각 동작은 대응하는 인접 상태로 결정론적으로 이동시킨다.</li>
                  <li>다만 장애물 또는 미로의 가장자리로 이동하는 경우에는 에이전트가 현재 위치에 머물게 됨.</li>
                </ul>
              </li>
              <li>보상
                <ul>
                  <li>모든 전이에서 0</li>
                  <li>목표 상태로의 전이에서 +1</li>
                </ul>
              </li>
              <li>목표 상태(G)에 도달한 후, 에이전트는 새로운 에피소드를 시작하기 위해 시작 상태(S) 로 돌아감.</li>
              <li>감가율 적용 ($\gamma = 0.95$)</li>
              <li>에피소드 형태의 문제</li>
            </ul>
          </li>
          <li>결과의 해석
            <ul>
              <li>그림 8.2 에서 미로 과제에 Dyna-Q 에이전트가 적용된 실험에서의 평균 학습 곡선을 보여줌
                <ul>
                  <li>초기 행동가치는 0이며, step-size 파라미터는 $\alpha = 0.1$, 탐색 파라미터는 $\varepsilon = 0.1$ 이다.</li>
                  <li>탐욕 방식으로 동작하였을 때, 동점일 경우 무작위로 결정한다.</li>
                </ul>
              </li>
              <li>에이전트는 실제 단계마다 수행하는 계획 단계의 수인 n 에 의해 다양하게 변한다.
                <ul>
                  <li>각 n에 대해 실험을 30회 반복하여 에피소드에서 목표에 도달하기까지 취한 스텝 수를 평균화한 결과를 보여준다.
                    <ul>
                      <li>각 반복에서 난수생성기의 초기 시드는 동일하게 유지함</li>
                      <li>따라서 첫 버째 에피소드는 모둔 n의 값에 대해 정확히 동일했으며 (약 1700 스텝), 그 데이터는 그림에서 표시되지 않았음.</li>
                    </ul>
                  </li>
                  <li>첫 번째 에피소드 이후, 모든 n 값에 대해 성능이 향상되었지만, 특히 n 값이 큰 경우 더 빠르게 종료됨.</li>
                  <li>$n=0$ 인 에이전트는 계획 없는 에이전트로서 직접 강화학습 (one-step tabular Q-learning) 만 사용</li>
                  <li>계획 없는 에이전트는 최적 성능 ($\varepsilon$-optimal) 에 도달하는 데 약 25번의 에피소드가 걸렸으며, $n=5$ 인 에이전트는 5번의 에피소드, $n=50$ 인 에이전트는 3번의 에피소드가 걸렸음.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/8_2_5_dyna_maze_planning.png" alt="8_2_5_dyna_maze_planning" /></p>

            <ul>
              <li>그림 8.3은 계획 에이전트가 계획 없는 에이전트보다 훨씬 빠르게 해결책을 찾은 이유를 보여준다.
                <ul>
                  <li>그림은 두 번째 에피소드의 중간 지점에서 $n=0$ 및 $n=50$ 에이전트가 찾은 정책이 표시된 것이다.</li>
                  <li>$n=0$ 인 경우 (계획이 없는 경우), 각 에피소드는 정책에 마지막 한 스텝만 추가하므로, 하나의 스텝만을 학습하였다.</li>
                  <li>계획을 사용하면, 첫 번째 에피소드에서는 하나의 단계만 배웠지만, 두 번째 에피소드에서는 광범위한 정책이 개발된다.
                    <ul>
                      <li>첫 번째 에피소드는 환경과 상호작용을 하면서 실제 경험을 얻는 과정이며, 이 경험은 진행된 경로에 한정하여 활용된다.</li>
                      <li>두 번째 에피소드부터 모델 기반 학습을 통해 미래 상태에 대한 예측을 수행한다.
                        <ul>
                          <li>모델은 환경에서 얻은 실제 경험으로부터 구축되며, 각 상태에서 가능한 모든 행동과 보상에 대한 확률 분포를 기록한다.
                            <ul>
                              <li>실제 환경과의 상호작용을 통해 얻을 경험을 통해 일정한 정책이 개발</li>
                              <li>모델 기반 학습을 통해 얻은 미래 상태에 대한 예측을 사용하여 더 다양한 정책을 탐색하고 개발</li>
                            </ul>
                          </li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li>세 번째 에피소드가 끝날 때 완전한 최적 정책이 발견되고, 완전한 성능이 달성된다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Dyna-Q 의 진행 방식
            <ul>
              <li>Dyna-Q에서 학습과 계획은 정확히 동일한 알고리즘을 사용하여 실제 경험에 대해 학습하고 계획에서 가상 경험을 활용하여 수행
                <ul>
                  <li>계획은 점진적으로 진행되므로 계획과 행동을 혼합하는 것이 매우 간단하다.</li>
                </ul>
              </li>
              <li>에이전트는 최신 감각 정보에 즉각적으로 응답하지만 항상 백그라운드에서 계획을 진행한다.</li>
              <li>또한 백그라운드에서 모델 학습 프로세스도 진행한다.
                <ul>
                  <li>새로운 정보가 얻어지면 모델은 현실과 더 잘 일치하도록 업데이트된다.</li>
                </ul>
              </li>
              <li>모델이 변경되면 지속적으로 계획되는 프로세스가 새로운 모델과 일치하도록 서서히 다른 방식으로 동작을 계산할 것임.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="what-is-a-model">What is a model?</h2>

<ul>
  <li>
    <p>What is a Model?</p>

    <ul>
      <li>개요
        <ul>
          <li>실 생활에서의 결정의 예
            <ul>
              <li>결정할 때 많은 생각을 하지 않는 경우 - 직장에 어떻게 운전해서 가는지?</li>
              <li>결정을 할 때 많은 가능한 시나리오를 생각하는 경우 - 한손으로 취약한 물건을 운반할 때 벌어질 수 있는 시나리오들을 상상하는 것</li>
            </ul>
          </li>
          <li>이전에 배운 학습법의 경우
            <ul>
              <li>TD : 샘플링된 경험으로부터만 학습함</li>
              <li>DP : 완성된 정보를 이용하여 계획함 (결정이 불필요)</li>
            </ul>
          </li>
          <li>위 두 방법의 중간에 해당하는 방법을 통해 양 방법의 이점을 모두 활용할 방법 찾기
            <ul>
              <li>이번 장에서 다룰 Dyna 구조도 그러한 형태 중 하나임</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>학습목표
        <ul>
          <li>모델이 무엇인지와 어떻게 쓰이는지 설명</li>
          <li>모델을 분포모델 (distribution models) 또는 샘플모델 (sample models) 로 분류</li>
          <li>언제 분포모델을 쓸지 샘플모델을 쓸지 식별하기</li>
        </ul>
      </li>
      <li>
        <p>모델에 대하여</p>

        <p><img src="/assets/images/posts/models_store_knowledge.png" alt="models_store_knowledge" /></p>

        <ul>
          <li>모델은 역학에 대한 지식을 저장한다.</li>
          <li>이 장에서 모델은 상태전이와 보상에 대한 역학을 저장한다.
            <ul>
              <li>이것은 실제 행동을 하지 않고도 행동에 대한 결과를 살펴볼 수 있게 해줌</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/planning_with_models.png" alt="planning_with_models" /></p>

        <ul>
          <li>모델은 계획(Planning) 을 가능하게 한다.
            <ul>
              <li>계획 (Planning) 이란 모델을 이용하여 정책을 개선하는 프로세스를 말한다.</li>
              <li>모델을 이용하여 계획하는 하나의 방법은 모델을 활용해 가상의 경험을 생성하여 가치함수와 정책을 개선하는 것이다.
                <ul>
                  <li>가상의 경험을 이용한다는 것은 최적 정책에 도달하기 위해 실제 환경과 상호작용이 덜 필요하다는 것을 의미함</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>모델의 종류</p>

            <p><a href="/assets/images/posts/sample_models.png">sample_models</a></p>

            <ul>
              <li>Sample model (샘플모델)
                <ul>
                  <li>기본 확률에서 도출된 실제 결과를 생성한 것</li>
                  <li>예를 들어 하나의 코인을 던져 앞면인지 뒷면인지에 대한 무작위 시퀀스를 생성하는 것</li>
                  <li>샘플 모델은 일련의 규칙에 따라 무작위 결과를 생성할 수 있기 때문에 저렴하다.
                    <ul>
                      <li>예를 들어 5개의 동전을 던지기 위해 임의로 하나의 동전을 독립적으로 5번 던져서 하나의 결과를 생성</li>
                      <li>CloudFlare 사의 라바램프(불규칙한 자연적 무작위성)를 활용(샘플)한 암호화</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/distribution_models.png" alt="distribution_models" /></p>

            <ul>
              <li>Distribution model (분포모델)
                <ul>
                  <li>모든 결과의 가능성 또는 확률을 완전히 지정한 것</li>
                  <li>예를 들어 하나의 코인을 던졌을 때 앞면일 확률은 50% 이고 뒷면인 확률은 50% 인 것, 이 정보를 이용해 특정 시퀀스가 발생할 확률을 생성할 수 있음</li>
                  <li>분포모델은 더 많은 정보가 포함되어 있지만, 특정하기 어렵고 비용도 비싸다.
                    <ul>
                      <li>예를 들어 5개의 동전을 던질 때 가능한 앞면과 뒷면의 시퀀스 32개의 결과를 완전히 설명
                        <ul>
                          <li>결과의 명시 확률에 따라 결과를 샘플 모델로 사용할 수 있다. (분포모델이 더 많은 정보를 포함하고 있다.)</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Comparing Sample and Distribution Models</p>

    <ul>
      <li>학습목표
        <ul>
          <li>샘플모델과 분포모델의 장단점 설명</li>
          <li>왜 샘플모델이 분포모델보다 간결하게 표현될 수 있는지 이유 설명</li>
        </ul>
      </li>
      <li>12개의 주사위 문제
        <ul>
          <li>의도
            <ul>
              <li>12개의 주사위를 던지는 행위에 대한 샘플모델과 분포모델의 접근방식에 대해 알아보고자 함</li>
            </ul>
          </li>
          <li>샘플모델
            <ul>
              <li>하나의 주사위를 12번 던져보는 것</li>
              <li>프로그램으로 생각하면 1-6 사이의 무작위 수를 12번 생성하는 것</li>
              <li>간결하고, 공동의 확률을 생각하지 않는다.</li>
              <li>적은 메모리를 차지한다.</li>
              <li>많은 샘플을 평균화 함으로써 예측되는 결과를 근사할 수 있다.</li>
            </ul>
          </li>
          <li>분포모델
            <ul>
              <li>12개의 주사위가 가질 수 있는 모든 경우의 수와 그것에 대한 확률을 고려해야 한다.</li>
              <li>12개의 주사위는 $6^{12}$ 의 경우의 수 (2176782336 가지) 를 가진다.</li>
              <li>결과에 대한 정확한 확률을 생산한다는 장점이 있다.
                <ul>
                  <li>예상하는 결과를 직접 계산하거나 결과의 변동성을 정량화할 수 있음</li>
                </ul>
              </li>
              <li>확률로 가중치를 부여한 모든 결과를 합산하여 정확한 예상결과를 계산할 수 있다.</li>
              <li>위험을 평가할 수 있는 유연성이 있다.
                <ul>
                  <li>예를 들어 의사가 약을 처방할 때 가능한 많은 부작용과 발생할 가능성을 고려할 경우</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="planning">Planning</h2>

<ul>
  <li>
    <p>Random Tabular Q-planning</p>

    <ul>
      <li>학습목표
        <ul>
          <li>정책 개선을 위해 계획이 어떻게 쓰이는지 설명</li>
          <li>random-sample one-step tabular Q-planning 설명</li>
        </ul>
      </li>
      <li>계획 (Planning)
        <ul>
          <li>모델을 강화학습에 적용 : 환경과의 상호작용 없이 모델을 활용하여 더 나은 의사결정을 할 수 있도록 하는 것
            <ul>
              <li>이 과정을 모델 경험을 통한 계획이라 한다.</li>
            </ul>
          </li>
          <li>
            <p>계획 (Planning) 의 정의</p>

            <p><img src="/assets/images/posts/planning_diagram_1.png" alt="planning_diagram_1" /></p>

            <ul>
              <li>모델을 입력값으로 개선된 정책을 생성하는 과정</li>
            </ul>

            <p><img src="/assets/images/posts/planning_diagram_2.png" alt="planning_diagram_2" /></p>

            <ul>
              <li>계획에 대한 한 가지 접근 방식은 먼저 모델에서 경험을 샘플링하는 것임
                <ul>
                  <li>세상이 어떻게 돌아가는지에 대한 이해를 바탕으로 세상에서 가능한 시나리오를 상상하는 것과 같음.</li>
                </ul>
              </li>
              <li>이 생성된 경험은 마치 실제 상호작용이 발생한 것처럼 가치함수에 대한 업데이트를 수행하는 데 사용할 수 있음.</li>
              <li>이러한 개선된 가치에 탐욕 행동을 선택하면 정책이 개선됨.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Random-sample one-step tabular Q-planning</p>

        <ul>
          <li>Q-learning 과 Q-planning 에 대해
            <ul>
              <li>Q-learning 은 환경에서 경험한 것을 사용하여 정책을 개선하기 위해 업데이트를 수행함.</li>
              <li>Q-planning 은 모델의 경험을 사용하고, 유사한 업데이트를 수행하여 정책을 개선함.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/random_sample_one_step_tabular_q_planning.png" alt="random_sample_one_step_tabular_q_planning" /></p>

        <ul>
          <li>Random-sample one-step tabular Q-planning
            <ul>
              <li>가정
                <ul>
                  <li>전이 역학에 대한 샘플 모델을 가지고 있다고 가정한다.</li>
                  <li>샘플에 상응하는 상태, 행동 쌍을 가지고 있다고 가정한다.</li>
                </ul>
              </li>
              <li>하나의 선택지는 상태와 행동을 균일하게 샘플링하는 것이다.
                <ul>
                  <li>전체 상태, 행동 집합에서 랜덤하게 상태, 행동 쌍을 선택한다.</li>
                  <li>그 뒤로 샘플 모델을 이용해 상태, 행동값에 대한 다음 상태와 보상을 질의한다.</li>
                </ul>
              </li>
              <li>위의 입력값과 결과값을 이용해 Q-learning update 를 진행한다.</li>
              <li>탐욕화를 이용해 정책을 개선한다.</li>
            </ul>
          </li>
          <li>
            <p>Planning 의 특징</p>

            <p><img src="/assets/images/posts/planning_using_simulated_experiences.png" alt="planning_using_simulated_experiences" /></p>

            <ul>
              <li>Planning 은 환경과 에이전트의 상호작용 결과가 아닌, 가상의 (상상의) 경험을 사용한다.</li>
            </ul>

            <p><img src="/assets/images/posts/planning_advantages.png" alt="planning_advantages" /></p>

            <ul>
              <li>환경과 에이전트의 상호작용 없이 진행하거나, 상호작용을 하는 중에 동시에 진행되기도 한다.
                <ul>
                  <li>행동이 특정 시간대에 일어나면서, 학습 업데이트가 상대적으로 더 빠를 경우 시간적 공백이 생긴다.</li>
                  <li>예를 들면, 이 공백 시간에 계획을 업데이트 할 수 있다.</li>
                  <li>예를 들어 로봇이 절벽 쪽에 다가갔을때의 결과가 모델에 있고, 가치함수나 정책에 아직 반영이 되지 않은 경우 가상의 경험을 생성하여 계획을 진행해 볼 수 있다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="dyna-as-formalism-for-planning">Dyna as formalism for planning</h2>

<ul>
  <li>
    <p>The Dyna Architecture</p>

    <ul>
      <li>
        <p>개요</p>

        <p><img src="/assets/images/posts/q_learning_q_planning.png" alt="q_learning_q_planning" /></p>

        <ul>
          <li>Direct RL : World (환경) 와 직접적인 상호작용을 하고, Q-learning 을 통해 학습하는 것</li>
          <li>Planning : 모델로부터 생성된 가상 경험을 통해 학습하는 것</li>
          <li>Dyna 구조 : Direct RL + Planning</li>
        </ul>
      </li>
      <li>학습목표
        <ul>
          <li>모델로부터의 가상경험과 환경으로부터의 상호작용 간의 차이점을 이해</li>
          <li>Dyna 구조를 통해 직접 RL(direct RL) 과 계획 (planning) 업데이트를 결합하는 방법 이해</li>
        </ul>
      </li>
      <li>Q-learning 과 Q-planning 의 결합
        <ul>
          <li>Q-learning update: 환경으로부터의 경험을 통해 정책과 가치함수 업데이트</li>
          <li>Q-planning update: 모델에서 생성한 경험을 통해 정책과 가치함수 업데이트</li>
          <li>Dyna 구조를 통한 Q-learning 과 Q-planning 의 결합</li>
        </ul>
      </li>
      <li>
        <p>Dyna 구조</p>

        <p><img src="/assets/images/posts/dyna_architecture.png" alt="dyna_architecture" /></p>

        <ul>
          <li>(중간부분) 환경과의 상호작용을 통해 경험의 흐름을 생성한다.</li>
          <li>(왼쪽부분) 위 경험을 직접적으로 이용해 정책/가치함수를 업데이트 하는 것을 direct RL update 라 한다.</li>
          <li>(오른쪽 부분) Planning 을 위해서는 모델이 필요하다. 환경과의 상호작용을 통해 얻은 경험으로 모델을 학습시킬 수 있다.
            <ul>
              <li>모델은 model experience 를 생성한다.</li>
              <li>위 경험을 생성할 때, 어떠한 가상 경험을 생성하여 계획을 구성할지 제어하는 것을 search control 이라 한다.</li>
              <li>planning update 는 모델로부터 생성된 경험으로 정책/가치함수를 업데이트 하는 것을 말한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Dyna 의 예시 : simple maze
        <ul>
          <li>
            <p>문제의 설명</p>

            <p><img src="/assets/images/posts/dyna_example_maze_1.png" alt="dyna_example_maze_1" /></p>

            <ul>
              <li>로봇이 미로를 탈출하는 문제</li>
              <li>로봇은 Goal 에서 +1 의 보상을 얻고, 그렇지 않은 부분에서는 0의 보상을 얻는다.</li>
            </ul>

            <p><img src="/assets/images/posts/dyna_example_maze_2.png" alt="dyna_example_maze_2" /></p>

            <ul>
              <li>로봇은 첫 시도에 헤메게 되며, 결국 골에 도착하고 보상 1 을 획득, 에피소드를 종료하게 된다.
                <ul>
                  <li>노랗게 표시된 부분은 로봇에 실제 한번 이상 방문한 상태이다.</li>
                  <li>로봇은 행동가치함수를 업데이트 하는데, 실제 영향을 받는 부분은 보라색 상태 뿐이다.
                    <ul>
                      <li>$Q(s,a) \gets Q(s,a) + \alpha (r + \gamma \max_{a’} Q(s’,a’) - Q(s,a))$</li>
                      <li>보상값이 존재하는 상태가 종료 상태 뿐이기 때문</li>
                      <li>위의 업데이트가 direct RL 을 통해 이루어진 업데이트이다.</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>Dyna 는 첫 번째 에피소드 동안 생성된 모든 경험을 이용하여 모델을 학습한다.
                <ul>
                  <li>노랗게 표시된 부분이 첫 에피소드 동안 방문한 상태이다. 로봇은 전체 상태를 방문하지 않았지만, 대부분의 상태를 이미 방문하였다.</li>
                  <li>Dyna 는 모든 타임스텝에 대해 planning 을 진행한다.
                    <ul>
                      <li>하지만, planning 은 첫 에피소드 동안에는 정책에 영향을 주지 않는다. (비록 모델이 각 타입스텝마다 점점 정확해지더라도…)</li>
                      <li>첫 에피소드가 끝나면, planning 이 작동하기 시작한다.
                        <ul>
                          <li>모델을 통해 노랗게 표시된 부분에서 어떠한 반환값이 나올 지 이미 알고 있다.</li>
                          <li>Dyna 는 이미 방문한 상태-행동 쌍에 대한 전이를 시뮬레이션 할 수 있다. (World 의 모방)</li>
                          <li>Planning 의 각 타임스텝 동안 가상의 경험을 통해 Q-learning 업데이트를 진행할 수 있다.</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/dyna_example_maze_3.png" alt="dyna_example_maze_3" /></p>

            <ul>
              <li>충분한 Planning 단계를 거쳐, 에이전트는 모든 방문한 상태에 대한 정책을 업데이트 할 수 있다.
                <ul>
                  <li>Dyna 는 더 많은 계산을 하지만, 제한된 경험을 보다 효율적으로 활용한다.</li>
                  <li>이것은 Cartoon 의 예시일 뿐, 실제로 에이전트는 위 정책보다 더 탐색적으로 행동하게 되고, Planning 단계에서 정책은 계속 수정되게 된다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>The Dyna Algorithm</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Tabular Dyna-Q 알고리즘 설명</li>
          <li>Tabular Dyna-Q 내에서 직접RL과 계획 업데이트 식별</li>
          <li>Tabular Dyna-Q 내에서 모델학습과 탐색제어 요소를 식별</li>
        </ul>
      </li>
      <li>Tabular Dyna-Q 에서의 모델 학습
        <ul>
          <li>
            <p>우선 Tabular Dyna-Q 는 결정론적 전이를 가정한다.</p>

            <p><img src="/assets/images/posts/model_learning_deterministic_model.png" alt="model_learning_deterministic_model" /></p>

            <ul>
              <li>위 그림에서 토끼가 A 상태에서 오른쪽으로 움직이기로 결정하였다면, 오직 한 종류의 결과만이 발생한다. (B, 0)</li>
              <li>에이전트가 위 세 가지 상태-행동을 경험하였다면, 모델은 위 세 가지 상태-행동에 따르는 결과를 알게 된다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Tabular Dyna-Q psuedo code</p>

        <p><img src="/assets/images/posts/tabular_dyna_q_psuedo_code_1.png" alt="tabular_dyna_q_psuedo_code_1" /></p>

        <ul>
          <li>에이전트가 환경과 상호작용 하며, $\varepsilon$-greedy 정책을 따르고 있다.</li>
          <li>위 정책에 따른 행동을 하면, 결과 보상과 다음 상태를 관측할 수 있다.</li>
          <li>위 값들도 Q-learning update 를 진행한다. (여기까지가 direct-RL)</li>
          <li>(여기에서 멈춘다면, Q-learning 알고리즘이 된다.)</li>
        </ul>

        <p><img src="/assets/images/posts/tabular_dyna_q_psuedo_code_2.png" alt="tabular_dyna_q_psuedo_code_2" /></p>

        <ul>
          <li>Dyna-Q는 이 전이를 이용해 model learning step 을 진행한다. (Model-free method 와 차이)
            <ul>
              <li>모델은 위의 전이를 기억, 저장한다. (환경이 결정론적이라는 가정)</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/tabular_dyna_q_psuedo_code_3.png" alt="tabular_dyna_q_psuedo_code_3" /></p>

        <ul>
          <li>Dyna-Q는 planning 을 n-step 진행한다.
            <ul>
              <li>각각의 planning step 은 3가지 단계를 포함한다.
                <ul>
                  <li>search control : 이전 방문한 상태, 행동 쌍을 랜덤하게 결정한다.</li>
                  <li>model query : 위 선택된 상태, 행동 쌍을 이용, 모델에 다음 상태와 보상을 질의한다.</li>
                  <li>value update : Q-learning update 를 진행한다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/tabular_dyna_q_planning.png" alt="tabular_dyna_q_planning" /></p>

        <ul>
          <li>Dyna-Q는 각 전이에 대해 많은 planning update 를 수행한다.
            <ul>
              <li>첫 에피소드 184 step 이후 (이 때의 결과를 Model learning 에 활용)</li>
              <li>두 번째 에피소드 1 step 당 100 회의 planning 을 진행함으로서 많은 정책이 개발되었음을 확인할 수 있음.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Dyna &amp; Q-learning in a Simple Maze</p>

    <ul>
      <li>학습목표
        <ul>
          <li>작은 GridWorld 내에서 Model-free Q-learning 학습과 Dyna-Q 학습을 비교</li>
          <li>환경의 경험과 모델의 가상경험으로부터의 학습이 성능에 어떤 영향을 주는지 확인</li>
          <li>정확한 모델이 에이전트가 환경과의 상호작용의 요구도를 낮추는 방법을 설명</li>
        </ul>
      </li>
      <li>미로 환경에서의 실험
        <ul>
          <li>
            <p>문제의 설정</p>

            <p><img src="/assets/images/posts/dyna_q_q_learning_example_maze.png" alt="dyna_q_q_learning_example_maze" /></p>

            <ul>
              <li>행동 : 4가지 방향으로 이동</li>
              <li>보상 : 목표 상태로 전이시 +1, 그 외의 경우 0</li>
              <li>에피소딕 문제</li>
              <li>할인율 0.95</li>
              <li>$\alpha = 0.1$, $\varepsilon = 0.1$</li>
              <li>행동 가치의 초기값은 모두 0으로 세팅</li>
            </ul>
          </li>
          <li>의도
            <ul>
              <li>3 개의 에이전트를 비교 (n=0, n=5, n=50)</li>
              <li>각각의 실험을 50 에피소드, 30번 실행하고 결과의 평균을 구함</li>
            </ul>
          </li>
          <li>
            <p>결과해석</p>

            <p><img src="/assets/images/posts/dyna_q_q_learning_result_comparison.png" alt="dyna_q_q_learning_result_comparison" /></p>

            <ul>
              <li>각 에이전트가 에피소드를 완료하는 데 걸린 평균 단계 수 (30번 수행의 평균) 를 표현. 즉, 에이전트가 잘 수행한다면 단계의 수가 감소 (y 값이 낮을 수록 좋음)</li>
              <li>Dyna-Q 를 0 계획 단계로 수행하면 Q-러닝 알고리즘과 정확히 동일한 결과이다.
                <ul>
                  <li>14 에피소드 정도에서 수렴을 함</li>
                </ul>
              </li>
              <li>Dyna-Q 를 5 계획 단계로 수행하면 더 빠르게 수렴함.</li>
              <li>Dyna-Q 를 50 계획 단계로 수행하면 3번째 에피소드에서 수렴함.
                <ul>
                  <li>즉, 샘플을 더 효율적으로 사용함 (모델이 정확한 경우 환경 경험을 더 잘 활용함)</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>다른 미로에서의 진행상황 해석
            <ul>
              <li>하나의 에피소드 후 Q-러닝은 목표 옆 상태에서 위 동작에 해당하는 하나의 동작 값만 업데이트 됨 (0이 아닌 보상이 발생한 유일한 전환)</li>
              <li>이 상태의 값을 다른 인접 상태로 부트스트랩 하는 데는 몇 에피소드가 더 필요함</li>
            </ul>

            <p><img src="/assets/images/posts/dyna_q_maze_random_search_control.png" alt="dyna_q_maze_random_search_control" /></p>

            <ul>
              <li>search control 이 계획에 미치는 영향을 살펴본다. (Dyna-Q와 조금 다른 방식으로 작동하나, 포인트를 더 강조할 수 있음)
                <ul>
                  <li>계획 단계를 10개로 설정하고, 계획 루프를 10번 연속 호출 (총 100개의 계획 단계)</li>
                  <li>보다시피, 많은 계획 업데이트가 가치함수를 변경하지 못하고, 단 2개의 상태 행동 쌍만 업데이트하였음.</li>
                  <li>계속 진행 (각 호출마다 100개의 계획 단계를 시도)</li>
                  <li>몇 번의 호출은 소수의 동작 값만 업데이트 한다.
                    <ul>
                      <li>검색 제어 (Search Control) 가 상태-행동 쌍을 무작위로 샘플링하기 때문. 즉, 샘플 상대-행동 쌍이 T 오류를 0 으로 생성한다면 업데이트는 아무런 효과가 없음.</li>
                      <li>모든 보상도 0이고, 초기값도 0이기 때문에 이 환경에서 자주 발생한다.</li>
                      <li>검색 제어의 주제에 대해서는 교재의 8.4 섹션을 확인.</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="dealing-with-inaccurate-models">Dealing with inaccurate models</h2>

<ul>
  <li>
    <p>What if the model is inaccurate?</p>

    <ul>
      <li>학습목표
        <ul>
          <li>어떠한 모델이 부정확한건지 식별하는 방법</li>
          <li>부정확한 모델에서 계획이 어떤 효과를 주는지 설명</li>
          <li>부분적으로 부정확한 모델에서 Dyna 가 성공적으로 계획하는 방법 서술</li>
        </ul>
      </li>
      <li>
        <p>부정확한 모델이란?</p>

        <p><img src="/assets/images/posts/model_inaccurate.png" alt="model_inaccurate" /></p>

        <ul>
          <li>모델이 저장한 전이가 환경에서 일어난 전이와 다를 때 발생</li>
          <li>불완전한 모델 : 학습 초기에 에이전트가 일부의 상태에서 일부의 행동만 시도했을 경우, 모델에 누락된 전이 정보가 생기게 됨.</li>
          <li>부정확한 모델 : 모든 상태에서 모든 행동을 수행했더라도, 환경이 변화하여 실제 환경과 모델간 전이 정보가 다를 경우.</li>
        </ul>
      </li>
      <li>부정확한 모델로 계획을 세울 떄 발생할 수 있는 일
        <ul>
          <li>불완전한 모델로 계획을 세울 때 전이 정보가 없는 상태에서 계획을 세울 수 없음. 그러나 에이전트가 환경과 상호작용하면서 더 많은 전이를 경험하면 학습이 가능해진다.</li>
        </ul>

        <p><img src="/assets/images/posts/planning_with_inaccurate_model.png" alt="planning_with_inaccurate_model" /></p>

        <ul>
          <li>부정확한 모델일 경우 계획 업데이트 시 가치 함수나 정책이 잘못된 방향으로 변경될 수 있음.</li>
        </ul>
      </li>
      <li>
        <p>불완전한 모델로 성공적인 계획을 하는 방법</p>

        <p><img src="/assets/images/posts/dyna_with_incomplete_model.png" alt="dyna_with_incomplete_model" /></p>

        <ul>
          <li>Dyna-Q 의 경우에서 처럼, 계획 단계에서 모델이 어떤 상태-행동 쌍을 쿼리할지 결정
            <ul>
              <li>Dyna-Q 는 이미 방문한 상태-행동 쌍에 대해서만 계획 업데이트를 수행한다. (이미 모델에 전이 정보가 존재)</li>
              <li>초기에는 이미 방문한 상태-행동에 대해서만 반복적으로 업데이트를 수행할 수 있으나, 에이전트가 점점 더 많은 상태-행동 쌍을 방문하면 계획 업데이트가 상태-행동 공간 전체에 더 고르게 진행됨.</li>
            </ul>
          </li>
          <li>부정확한 모델로 계획을 세울 때 계획은 모델을 기준으로 정책이나 가치함수를 개선함 (환경을 기준으로 개선되지 않음)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>In-depth with changing environments</p>

    <ul>
      <li>학습목표
        <ul>
          <li>모델의 부정확성이 또다른 탐색-이용 trade-off 를 생성하는지 설명</li>
          <li>위의 trade-off 를 Dyna-Q+ 가 해결하는 방법 설명</li>
        </ul>
      </li>
      <li>모델이 부정확한 경우
        <ul>
          <li>모델이 부정확하면 계획은 환경을 기준으로 정책이나 가치함수를 악화시킬 수 있음.
            <ul>
              <li>이는 에이전트가 모델이 정확한지 확인하기 위해 노력해야 한다는 것을 의미한다.</li>
              <li>즉, 에이전트가 환경에서 전이를 경험한 후 모델을 수정해야 함</li>
            </ul>
          </li>
          <li>일반적으로 에이전트는 자신의 모든 모델 전이가 올바른지 다시 확인하려고 할 것임.
            <ul>
              <li>그러나 낮은 가치의 행동 전이를 다시 확인하면, 낮은 보상을 얻게 됨.</li>
              <li>변화하는 환경에서는 언제든지 에이전트의 모델이 부정확해질 수 있음.</li>
              <li>
                <p>에이전트는 선택을 해야 한다.</p>

                <p><img src="/assets/images/posts/model_exploration_and_exploitation.png" alt="model_exploration_and_exploitation" /></p>

                <ul>
                  <li>모델이 정확한 것으로 가정하여, 최적의 정책 계산을 위해 환경 탐사</li>
                  <li>모델이 정확한지 확인</li>
                </ul>
              </li>
              <li>환경이 변화하면, 모델은 부정확해진다.
                <ul>
                  <li>환경이 변경된 부분을 재방문하고 모델을 업데이트 하기 전까지 모델은 부정확한 상태로 유지된다.</li>
                  <li>즉, 오랜 기간 동안 방문하지 않은 장소를 탐사해야 한다는 것</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Dyna-Q+ 에서의 해결 방법
        <ul>
          <li>모델은 에이전트가 오랫동안 방문하지 않은 상태에서 더욱 잘못될 가능성이 높음.</li>
          <li>에이전트가 주기적으로 상태를 다시 방문하도록 유도하기 위해 계획에 사용되는 보상에 보너스를 추가할 수 있다.</li>
          <li>
            <p>이것을 탐사 보너스라고 한다.</p>

            <p><img src="/assets/images/posts/dyna_q_plus_exploration_bonus.png" alt="dyna_q_plus_exploration_bonus" /></p>

            <ul>
              <li>이 보너스는 단순히 $\kappa$ (Kappa) 에 $\tau$ (Tau) 의 제곱근을 곱한 것이다.</li>
              <li>$r$ : 모델에서의 보상</li>
              <li>$\tau$ : 환경에서 해당 상태 행동 쌍을 마지막으로 방문한 시간.
                <ul>
                  <li>계획 루프에서 업데이트되지 않음. (실제 방문이 아님.)</li>
                </ul>
              </li>
              <li>$\kappa$ : 보너스가 계획 업데이트에 미치는 영향을 조절하는 작은 상수.
                <ul>
                  <li>0이면 보너스를 완전히 무시함.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>탐사 보너스를 계획 업데이트에 추가하면 Dyna-Q+ 알고리즘이 생성됨.</p>

            <p><img src="/assets/images/posts/dyna_q_plus_dyna_q_algorithm.png" alt="dyna_q_plus_dyna_q_algorithm" /></p>

            <ul>
              <li>계획에 사용되는 보상을 인위적으로 증가시킴으로써, 최근에 방문하지 않은 상태 행동 쌍의 가치를 증가시킨다.
                <ul>
                  <li>오랫동안 방문하지 않은 상태 행동 쌍에 대한 $\tau$ 가 큰 상태 : $\tau$ 가 커질수록 보너스가 점점 커진다는 것을 의미</li>
                  <li>결국 계획은 큰 보너스 때문에 해당 상태 $S$ 로 직접 가도록 정책을 변경하게 된다.
                    <ul>
                      <li>에이전트가 상태 $S$ 에 방문하면 큰 보상을 볼 수도 있고, 실망을 할 수도 있다. 어떤 경우든 모델은 환경의 역학을 반영하도록 업데이트된다.</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Dyna-Q 와 Dyna-Q+ 간 비교
        <ul>
          <li>문제의 설정
            <ul>
              <li>기본적인 미로 문제이며, 시작 상태에서 목표 상태까지 빠르게 도달하는 것이 목표임.
                <ul>
                  <li>보상은 목표 지점을 제외하고 0이며, 목표 지점에서 +1의 보상을 제공</li>
                  <li>할인율은 1보다 작음</li>
                </ul>
              </li>
              <li>$\varepsilon$-greedy 정책을 사용</li>
            </ul>
          </li>
          <li>
            <p>결과의 해석</p>

            <p><img src="/assets/images/posts/dyna_q_vs_dyna_q_plus_changing_environment_1.png" alt="dyna_q_vs_dyna_q_plus_changing_environment_1" /></p>

            <ul>
              <li>실험의 절반에서는 Dyna-Q 와 Dyna-Q+ 가 매우 유사하게 작동
                <ul>
                  <li>이 경우 Dyna-Q+ 의 증가된 탐사는 더 빨리 좋은 정책을 찾는데 도움이 됨. (실제로 라인이 위에 있음)</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/dyna_q_vs_dyna_q_plus_changing_environment_2.png" alt="dyna_q_vs_dyna_q_plus_changing_environment_2" /></p>

            <ul>
              <li>실험이 절반 쯤 진행되었을 때 벽의 오른쪽에 지름길을 제공
                <ul>
                  <li>환경이 변화된 후 Dyna-Q+ 는 지름길을 찾아냈음.</li>
                  <li>Dyna-Q는 시간 내에 지름길을 찾지 못하였음.
                    <ul>
                      <li>결국 Dyna-Q도 $\varepsilon$-greedy 정책에 의해 전체 상태-행동 공간을 탐사함으로써 지름길을 찾을 것이다.</li>
                      <li>그러나 위의 경우 많은 탐사가 필요하게 된다.</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>에이전트는 모델이 정확한지 확인하기 위해 탐사를 해야하며, Dyna-Q+ 가 환경을 탐사하기 위해 탐사 보너스를 활용하는 방법에 대해 알아보았다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Drew Bagnell: self-driving, robotics, and Model Based RL
    <ul>
      <li>자율주행, 로보틱스, 모델기반 강화학습에 대해
        <ul>
          <li>자율주행에 대해
            <ul>
              <li>거리의 복잡성을 기계학습을 통해 인식함.</li>
              <li>의사결정이 필요
                <ul>
                  <li>타 주체들의 행동에 따른 복잡성, 간단한 규칙 (좌회전 신호에 좌회전을 한다 등) 의 조정 등의 어려움</li>
                  <li>연속된 상태와 동작</li>
                  <li>즉, 연속된 상태 및 동작의 모델이 필요함.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>모델
            <ul>
              <li>현재 상태와 동작을 다음 상태로 매핑하는 전환함수 혹은 동역학</li>
              <li>특정상태의 동작을 평가할 수 있는 함수</li>
            </ul>
          </li>
          <li>의사결정 문제
            <ul>
              <li>의사결정이 필요한 로봇 학습은 모두 모델기반이다.
                <ul>
                  <li>로봇들 간의 상호작용에 따르는 효율, 비용적 측면에서 지수적인 차이가 발생</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>연속된 상태 동작 모델의 활용법
            <ul>
              <li>제곱근 가치 함수 근사법 (Quadratic Value Function Approximation)
                <ul>
                  <li>환경이 선형이거나 선형으로 근사 가능한 경우 사용 가능</li>
                  <li>여기에서는 다루지 않음</li>
                </ul>
              </li>
              <li>Differential Dynamic Programming (DDP)
                <ul>
                  <li>최적 제어 정책을 추정</li>
                  <li>여기에서는 다루지 않음</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Week 4 Summary (Planning, Learning, Acting)
    <ul>
      <li>
        <p>Types of models (distribution vs sample)</p>

        <p><img src="/assets/images/posts/distribution_models_and_sample_models.png" alt="distribution_models_and_sample_models" /></p>

        <ul>
          <li>Distribution models : 모든 전이확률을 모델 데이터로 가지고 있음, 많은 메모리 필요, Sample model 생성 가능</li>
          <li>Sample models : 전이확률을 따로 저장하지 않음, 많은 메모리 불필요</li>
        </ul>
      </li>
      <li>
        <p>One-step Q-planning</p>

        <p><img src="/assets/images/posts/random_sample_one_step_tabular_q_planning_algorithm.png" alt="random_sample_one_step_tabular_q_planning_algorithm" /></p>

        <ul>
          <li>Q-learning 과 동일하나, 모델에서 생성한 경험을 이용하여 업데이트함.</li>
        </ul>
      </li>
      <li>
        <p>Dyna architecture</p>

        <p><img src="/assets/images/posts/dyna_architecture_and_dyna_q_algorithm.png" alt="dyna_architecture_and_dyna_q_algorithm" /></p>

        <ul>
          <li>Planning 과 Learning 을 Single agent 에서 수행</li>
          <li>많은 planning update 를 통해 학습을 더 빠르게 진행할 수 있음 (환경과의 상호작용을 덜 하게 됨)</li>
          <li>불완전한 모델의 경우 상호작용을 통해 모델이 점점 완성되게 된다.</li>
        </ul>
      </li>
      <li>
        <p>Dyna-Q+</p>

        <p><img src="/assets/images/posts/dyna_q_plus_algorithm.png" alt="dyna_q_plus_algorithm" /></p>

        <ul>
          <li>부정확한 모델의 경우 탐색 보너스를 통해 오랫동안 방문하지 않은 상태를 방문하도록 정책을 유도하여 모델을 업데이트하도록 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Summary (생각해볼 점)</p>

    <ul>
      <li>Part 1 에서 배운 강화학습의 종류</li>
    </ul>

    <p><img src="/assets/images/posts/reinforcement_learning_dimensions_part_1.png" alt="reinforcement_learning_dimensions_part_1" /></p>

    <ul>
      <li>생각해볼 점
        <ul>
          <li>반환(리턴)의 정의: 과제는 에피소드식인지 계속적인지, 할인 적용 여부에 따라 다름.</li>
          <li>행동 가치 vs 상태 가치 vs 후상태 가치: 어떤 종류의 가치를 추정해야 하는지? 상태 가치만 추정하는 경우, 행동 선택을 위해 모델이나 별도의 정책(액터-크리틱 방법과 같은)이 필요함.</li>
          <li>행동 선택/탐사: 탐사와 활용 사이의 적절한 균형을 유지하기 위해 어떻게 행동을 선택해야 하는지? 우리는 이를 위한 가장 간단한 방법만 고려해봄: e-greedy, 낙관적 초기화, 소프트 맥스 및 상한 신뢰 구간.</li>
          <li>동기화 vs 비동기화: 모든 상태의 업데이트는 동시에 수행되어야 하는가, 또는 어떤 순서로 하나씩 수행되어야 하는가?</li>
          <li>실제 vs 모의: 실제 경험 또는 모의 경험을 기반으로 업데이트. 둘 다 하는 경우, 각각 어느 정도씩 해야하는지?</li>
          <li>업데이트 위치: 어떤 상태나 상태-행동 쌍을 업데이트해야하는지? Model-free 방법은 실제로 만난 상태와 상태-행동 쌍 중에서만 선택할 수 있지만, Model-based 방법은 임의로 선택할 수 있음. 이 부분에는 여러 가지 가능성이 있음.</li>
          <li>업데이트 시점: 업데이트는 행동 선택의 일부로 수행되어야 할지, 아니면 그 후에만 수행되어야 할지?</li>
          <li>업데이트 기억: 업데이트된 값은 얼마나 오래 유지되어야 할지? 영구적으로 유지, 아니면 휴리스틱 탐색과 같이 행동 선택을 계산하는 동안만 유지</li>
        </ul>
      </li>
      <li>앞으로 살펴볼 점
        <ul>
          <li>이러한 차원들은 절대적이거나 상호배반적인 것이 아님.
            <ul>
              <li>각각의 알고리즘은 다양한 방식으로 차이가 있으며, 많은 알고리즘들은 여러 차원에서 여러 위치에 위치한다.</li>
              <li>예를 들어, Dyna 방법은 실제 경험과 모의 경험을 모두 사용하여 동일한 가치 함수에 영향을 미칩니다.</li>
              <li>서로 다른 방식이나 다른 상태 및 행동 표현에 걸쳐 계산된 여러 가치 함수를 유지하는 것도 합리적인 방법임.</li>
            </ul>
          </li>
          <li>그러나 이러한 차원들은 다양한 가능한 방법의 넓은 공간을 묘사하고 탐구하기 위한 일관된 아이디어의 집합을 형성한다.</li>
          <li>여기에 언급되지 않은 가장 중요한 차원은 함수 근사의 차원이다.
            <ul>
              <li>함수 근사는 한쪽 끝에서는 테이블 기반 방법을 통해 상태 집합, 다양한 선형 방법 및 다양한 비선형 방법으로 이어지는 여러 가지 가능성의 스펙트럼으로 볼 수 있다. 이 차원은 제 2부에서 탐구한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Dyna - Q" /><summary type="html"><![CDATA[관련 자료 (RLbook Pages 159-166)]]></summary></entry><entry><title type="html">Sample-based Learning Methods - 03. Week 3. Temporal Difference Learning Methods for Control</title><link href="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_03_Week3/" rel="alternate" type="text/html" title="Sample-based Learning Methods - 03. Week 3. Temporal Difference Learning Methods for Control" /><published>2023-07-12T10:00:00+09:00</published><updated>2023-07-12T10:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_03_Week3</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_03_Week3/"><![CDATA[<h2 id="관련-자료-rlbook-pages-129-134">관련 자료 (RLbook Pages 129-134)</h2>

<ul>
  <li>
    <p>Sarsa: On-policy TD Control</p>

    <ul>
      <li>개요
        <ul>
          <li>제어 문제를 위한 TD 예측 방법
            <ul>
              <li>평소와 같이 GPI (generalized policy iteration) 패턴을 따른다.</li>
              <li>이번에는 평가 혹은 예측 부분을 TD 방식을 이용한다.</li>
              <li>몬테카를로 방식과 마찬가지로 탐색과 이용사이의 등가교환 문제에 직면하게 된다.
                <ul>
                  <li>이에 대한 2가지 접근 방법 : On-policy, Off-policy</li>
                </ul>
              </li>
              <li>이 장에서는 On-policy TD 제어 방법을 활용한다.</li>
            </ul>
          </li>
          <li>On-policy TD 제어
            <ul>
              <li>첫 단계로 상태가치함수가 아닌 행동가치함수를 학습해야 한다.</li>
              <li>On-policy 방식에서는 모든 상태 $s$ 및 동작 $a$ 에 대해 현 정책 $\pi$ 에 대한 $q_\pi (s,a)$ 를 추정해야 한다.
                <ul>
                  <li>이는 위 $v_\pi$ 학습의 TD 방식과 거의 동일하게 수행할 수 있음.
                    <ul>
                      <li>에피소드는 상태와 상태-행동 쌍의 번갈아가는 순서로 구성되는 것을 기억한다.</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/6_4_1_td_sequence.png" alt="6_4_1_td_sequence" /></p>

            <ul>
              <li>이전 상태가치함수에 대한 TD 예측문제와의 비교
                <ul>
                  <li>이전 섹션에서는 상태에서 상태로의 전이를 고려하고 상태의 가치를 학습했다면 이제는 상태-동작 쌍에서 상태-동작 쌍으로의 전이를 고려해서 상태-동작 쌍의 가치를 학습한다.</li>
                  <li>위 두 경우의 형식은 동일하다 : 보상 프로세스가 있는 Markov chain</li>
                  <li>TD(0)에 따른 상태 값의 수렴을 보장하는 정리는 행동 값에 대한 알고리즘에도 적용된다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/6_4_2_td_algorithm_for_action_value.png" alt="6_4_2_td_algorithm_for_action_value" /></p>

            <ul>
              <li>업데이트 식과 Sarsa 알고리즘에 대해
                <ul>
                  <li>이 업데이트는 비종료 상태 $S_t$ 로부터 모든 전이 후에 수행된다.</li>
                  <li>만약 $S_{t+1}$ 이 종료 상태라면, $Q(S_{t+1}, A_{t+1})$ 은 0으로 정의된다.</li>
                  <li>이 규칙은 상태-동작 쌍에서 다음 상태-동작 쌍으로의 전이를 이루는 이벤트인 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ 의 모든 요소를 사용한다.</li>
                  <li>이러한 다섯 요소에서 알고리즘의 이름 Sarsa 가 유래되었다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/6_4_3_sarsa_backup_diagram.png" alt="6_4_3_sarsa_backup_diagram" /></p>

            <ul>
              <li>Sarsa 예측 방법 기반 정책 제어 알고리즘
                <ul>
                  <li>모든 On-policy 방법과 마찬가지로 행동 정책 $\pi$ 에 대해 $q_\pi$ 를 지속적으로 추정하고 동시에 $\pi$ 를 $q_\pi$ 에 대한 탐욕 정책으로 변경한다.</li>
                </ul>
              </li>
              <li>Sarsa 알고리즘과 수렴 속성
                <ul>
                  <li>Sarsa 알고리즘의 수렴은 정책이 Q에 의존하는 특성에 따라 달라진다.
                    <ul>
                      <li>예 : $\varepsilon$-greedy, $\varepsilon$-soft 정책 등</li>
                    </ul>
                  </li>
                  <li>Sarsa는 모든 상태-행동 쌍이 무한한 횟수로 방문되고 정책이 한계에서 탐욕적인 정책으로 수렴하는 경우 (예를들어 $\varepsilon$-greedy 정책에서 $\varepsilon = \frac{1}{t}$) 확률 1로 최적 정책과 동작-가치함수로 수렴한다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/6_4_4_sarsa_psuedo_code.png" alt="6_4_4_sarsa_psuedo_code" /></p>
          </li>
        </ul>
      </li>
      <li>예제 6.5: Windy Gridworld
        <ul>
          <li>문제의 정의
            <ul>
              <li>일반적인 그리드월드 (시작 상태, 목표 상태 존재) 에서 한 가지 다른 점이 존재
                <ul>
                  <li>그리드 중간을 통해 위로 향하는 크로스윈드가 존재함
                    <ul>
                      <li>바람의 세기는 열마다 다름.</li>
                      <li>각 열 아래에 바람의 세기가 표시되어 있음.</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>행동 : 상, 하, 좌, 우
                <ul>
                  <li>중간 지역에서는 바람에 의해 위로 이동됨
                    <ul>
                      <li>예를 들어, 목표 상태의 오른쪽 셀에서 왼쪽으로 이동하면 목표 상태의 바로 위의 셀로 이동함</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>할인이 없는 에피소드 기반의 작업</li>
              <li>목표에 도달할 때까지 보상은 -1 로 제공</li>
            </ul>
          </li>
          <li>
            <p>결과</p>

            <p><img src="/assets/images/posts/6_4_5_example_6_5_windy_gridworld.png" alt="6_4_5_example_6_5_windy_gridworld" /></p>

            <ul>
              <li>위 그래프는 $\varepsilon$-greedy Sarsa 를 적용한 결과를 보여준다. ($\varepsilon = 0.1, \alpha = 0.5$, $Q(s,a,)=0$ (초기값, 모든 s, a 에 대해))</li>
              <li>그래프의 증가하는 기울기는 시간이 지남에 따라 목표에 더 빨리 도달하였음을 보여준다.</li>
              <li>8000 번째 타임 스텝에서는 탐욕 정책이 이미 최적이었음 (inset 에서 해당 경로가 표시됨)</li>
              <li>지속적인 $\varepsilon$-greedy 탐사로 인해 평균 에피소드 길이는 약 17단계로 유지되었으며, 최소값인 15보다 2단게 더 많았다.</li>
              <li>Monte Carlo 방법은 모든 정책에 대해 종료가 보장되지 않기 때문에 여기에 쉽게 사용할 수 없음.
                <ul>
                  <li>에이전트가 같은 상태에 머무르는 것을 유도하는 정책을 찾은 경우 다음 에피소드는 결코 종료되지 않음.</li>
                </ul>
              </li>
              <li>Sarsa 와 같은 온라인 학습방법은 이러한 문제가 없다.
                <ul>
                  <li>에피소드 동안 빠르게 학습하여 이러한 정책이 좋지 않음을 배우고 다른 정책으로 전환한다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Q-learning: Off-policy TD Control</p>

    <ul>
      <li>개요
        <ul>
          <li>강화학습의 초기 돌파구 중 하나는 Q-learning (Watkins, 1989) 으로 알려진 Off-policy TD Control 알고리즘의 개발이었다.</li>
        </ul>

        <p><img src="/assets/images/posts/6_5_1_q_learning_algorithm.png" alt="6_5_1_q_learning_algorithm" /></p>

        <ul>
          <li>위 경우, 학습된 행동가치함수인 Q는 따르는 정책과는 독립적으로 최적의 행동가치함수인 $q_\pi$ 를 직접 근사함
            <ul>
              <li>이것은 알고리즘의 분석을 크게 단순화하고 초기 수렴 증명을 가능하게 함</li>
              <li>정책은 여전히 영향을 미치며, 어떤 상태-행도 쌍이 방문되고 업데이트되는지를 결정함.
                <ul>
                  <li>그러나 올바른 수렴을 위해 필요한 것은 모든 쌍이 계속해서 업데이트 되는 것 뿐임</li>
                  <li>이것은 일반적인 경우에 최적의 행동을 찾기 위해 보장된 어떤 방법이든 필요한 최소한의 요구사항임</li>
                </ul>
              </li>
              <li>위 가정과 일반적인 확률 근사 조건의 변형에 따라, Q는 확률 1로 $q_\pi$ 에 수렴한다는 것이 증명되었음.</li>
            </ul>
          </li>
          <li>아래는 Q-러닝 알고리즘의 psuedo code 이다.</li>
        </ul>

        <p><img src="/assets/images/posts/6_5_2_q_learning_psuedo_code.png" alt="6_5_2_q_learning_psuedo_code" /></p>
      </li>
      <li>예제 6.6: Cliff Walking
        <ul>
          <li>이 그리드월드 예제에서는 Sarsa와 Q-러닝을 비교하여 On-policy (Sarsa) 및 Off-policy (Q-learning) 방법의 차이를 강조한다.</li>
        </ul>

        <p><img src="/assets/images/posts/6_5_3_example_6_6_cliff_walking.png" alt="6_5_3_example_6_6_cliff_walking" /></p>

        <ul>
          <li>문제의 정의
            <ul>
              <li>표준적인 할인되지 않은 에피소드 형태</li>
              <li>시작 상태와 목표 상태가 존재</li>
              <li>행동 : 위, 아래, 오른쪽, 왼쪽</li>
              <li>“Cliff” 로 표시된 영역으로의 이동을 제외한 모든 전이의 보상은 -1</li>
              <li>“Cliff” 로 표시된 영역으로 진입하면 -100의 보상을 받고 즉시 시작지점으로 되돌아감.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/6_5_4_example_6_6_cliff_walking_result.png" alt="6_5_4_example_6_6_cliff_walking_result" /></p>

        <ul>
          <li>결과
            <ul>
              <li>위 그래프는 $\varepsilon$-greedy 행동 선택 ($\varepsilon = 0.1$) 정책에 대한 Sarsa 와 Q-learning 방법의 성능을 보여준다.
                <ul>
                  <li>초기 이후에 Q-러닝은 최적 정책인 절벽 가장자리를 따라 이동하는 경로의 값을 학습한다.</li>
                  <li>그러나 $\varepsilon$-greedy 정책 때문에 가끔 절벽에서 떨어지는 경우가 발생한다.</li>
                  <li>반면 Sarsa 는 행동 선택을 고려하여 멀지만 더 안정적인 경로를 학습한다.</li>
                  <li>Q-러닝은 사실 최적의 정책을 학습하지만 온라인 성능은 Sarsa 보다 나쁘게 된다. (Sarsa 는 우회정책을 학습한다.)</li>
                  <li>$\varepsilon$ 이 점차적으로 감소한다면 두 방법 모두 최적 정책으로 수렴하게 된다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Expected Sarsa</p>

    <ul>
      <li>개요
        <ul>
          <li>Q-러닝과 동일하지만 다음 상태-행동 쌍에 대한 최대값 (Q-러닝) 대신 현재 정책에 따라 각 행동의 확률을 고려하여 예상값을 사용하는 학습 알고리즘을 가정해보자.</li>
          <li>즉 업데이트 법칙의 알고리즘은 아래와 같지만, 나머지 부분은 Q-러닝의 영역을 따른다고 가정한다.</li>
        </ul>

        <p><img src="/assets/images/posts/6_6_1_expected_sarsa_algorithm.png" alt="6_6_1_expected_sarsa_algorithm" /></p>

        <ul>
          <li>다음의 상태 $S_{t+1}$ 이 주어지는 측면에서 알고리즘은 결정론적으로 Sarsa 와 같은 움직임 이나 예상값을 사용하는 측면에서 Expected Sarsa 라고 부른다.</li>
        </ul>

        <p><img src="/assets/images/posts/6_6_2_expected_sarsa_backup_diagram.png" alt="6_6_2_expected_sarsa_backup_diagram" /></p>

        <ul>
          <li>Expected Sarsa 의 백업 다이어그램은 위와 같다.</li>
          <li>Expected Sarsa 는 계산적으로 Sarsa 보다 복잡하다.
            <ul>
              <li>그러나 결과에서 $A_{t+1}$ 의 무작위 선택에 따른 분산을 제거해준다.</li>
              <li>즉, 같은 양의 학습경험을 할 경우 Sarsa 보다 성능이 조금 더 나으리라 기대할 수 있고, 실제로도 그렇다.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/6_6_3_expected_sarsa_cliff_walking_result.png" alt="6_6_3_expected_sarsa_cliff_walking_result" /></p>

        <ul>
          <li>위의 결과는 cliff-walking 문제에서 Expected Sarsa 와 Sarsa, Q-learning 간 결과의 비교값이다.
            <ul>
              <li>Expected Sarsa는 이 문제에서 Sarsa에 비해 상당한 이점을 유지한다.</li>
              <li>또한, Expected Sarsa는 step-size 파라미터 $\alpha$ 의 다양한 값 범위에서 Sarsa에 비해 상당한 개선을 보여준다.</li>
              <li>Cliff walking 에서 상태 전이는 모두 결정론적이며, 모든 무작위성은 정책으로부터 발생한다.
                <ul>
                  <li>이와 같은 경우, Expected Sarsa는 $\alpha = 1$ 로 안전하게 설정할 수 있으며, 한계적인 성능 저하 없이 수렴 성능을 유지할 수 있습니다.</li>
                  <li>반면, Sarsa는 작은 $\alpha$ 값에서만 잘 동작하며, 이 경우 학습 초기 성능(Interim Performance)이 낮다.
                    <ul>
                      <li>Asymptotic Performance : 알고리즘이 충분한 학습을 진행한 후에 얻게 되는 최종적인 성능을 나타냄. 이는 알고리즘이 수렴하여 최적 정책과 가치 함수를 얻었을 때의 성능을 의미.</li>
                      <li>Interim Performance : 알고리즘이 학습 도중에 어떤 성능을 보이는지를 나타냄. 즉, 알고리즘이 아직 수렴하지 않은 초기 학습 단계에서의 성능을 평가하는 지표.</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>이 Cliff walking 결과에서 Expected Sarsa 는 On-policy 로 사용되었지만, 목표 정책 $\pi$ 와 다른 정책을 사용하여 동작을 생성할 수 있으며, 이 경우 Off-policy 알고리즘이 된다.</li>
              <li>예를 들어, $\pi$ 가 탐욕 정책이가 동작이 더 탐색적인 경우, Expected Sarsa 는 정확히 Q-learning 이 된다.
                <ul>
                  <li>이런 의미에서 Expected Sarsa 는 Q-learning 을 포괄하고 일반화하며, Sarsa 에 비해 신뢰할 수 있는 개선을 제공한다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>적은 추가적인 계산 비용만 제외하면, Expected Sarsa 는 더 잘 알려진 TD 제어 알고리즘인 두 알고리즘을 완전히 압도할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="td-for-control">TD for Control</h2>

<ul>
  <li>
    <p>Sarsa: GPI with TD</p>

    <ul>
      <li>학습목표
        <ul>
          <li>GPI (generalized policy iteration) 을 TD 와 함께 사용하여 개선된 정책을 찾는 법 설명하기.</li>
          <li>Sarsa 제어 알고리즘 서술하기.</li>
        </ul>
      </li>
      <li>GPI (generalized policy iteration) 에 대한 복기
        <ul>
          <li>GPI : policy iteration (policy evaluation $\to$ policy improvement)</li>
          <li>GPI in Monte-carlo : 에피소드 진행 후 policy evaluation $\to$ policy improvement)
            <ul>
              <li>몬테카를로의 경우 정책개선 이전에 완전한 정책평가를 하지 않는다. (에피소드마다 정책평가, 정책개선을 한다.)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>TD within GPI
        <ul>
          <li>GPI 에 TD 를 적용하기 위해서는 행동가치함수를 학습해야 한다.
            <ul>
              <li>이전 섹션에서 TD 를 상태가치함수로 학습 (상태-상태)
                <ul>
                  <li>여기에서는 행동가치함수 (상태,행동-상태,행동) 의 값을 학습 : 이것을 Sarsa 예측이라고 함.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Sarsa</p>

        <p><img src="/assets/images/posts/sarsa_algorithm.png" alt="sarsa_algorithm" /></p>

        <ul>
          <li>Sarsa 의 약어는 업데이트에 쓰인 데이터 요소를 뜻함
            <ul>
              <li>$S_t$ : state</li>
              <li>$A_t$ : action</li>
              <li>$R_{t+1}$ : reward</li>
              <li>$S_{t+1}$ : next state</li>
              <li>$A_{t+1}$ : next action</li>
            </ul>
          </li>
          <li>Sarsa 의 업데이트식은 상태가치함수를 업데이트하는 TD 식과 유사
            <ul>
              <li>단, Sarsa 는 상태-행동 쌍에 대한 행동가치함수 $Q(S_t,A_t)$ 에 대해 업데이트함.</li>
            </ul>
          </li>
          <li>위의 그림은 정책평가에 대한 내용만 담겨 있다. (고정된 정책에 대한 행동가치함수의 학습)
            <ul>
              <li>그러나 GPI 의 요소를 이용, 제어 알고리즘으로 변환할 수 있다. (예: $\varepsilon$-greedy)
                <ul>
                  <li>Sarsa 제어 : TD 학습을 적용한 GPI 의 한 예</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Sarsa in the Windy GridWorld</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Sarsa 제어 알고리즘이 예제 MDP 에서 작동하는 방식 이해하기.</li>
          <li>학습 알고리즘의 성능 분석 방법을 경험하기</li>
        </ul>
      </li>
      <li>
        <p>The Windy Gridworld</p>

        <ul>
          <li>
            <p>문제의 정의</p>

            <p><img src="/assets/images/posts/sarsa_example_windy_gridworld_1.png" alt="sarsa_example_windy_gridworld_1" /></p>

            <ul>
              <li>State : 각각 하나의 시작과 종료상태가 존재
                <ul>
                  <li>열에 따라 바람이 위쪽 방향으로 불어, 의도한 행동의 결과와 다른 상태전이가 일어남
                    <ul>
                      <li>예를 들어 위 그림에서 에이전트는 왼쪽으로 움직이는게 아니라 좌측 상단 (대각선) 으로 움직임</li>
                    </ul>
                  </li>
                  <li>가장자리에 부딪힐 경우 아무 일도 일어나지 않음</li>
                </ul>
              </li>
              <li>Action : 에이전트는 4개의 방향으로 이동이 가능</li>
              <li>Reward : 모든 상태에서 -1
                <ul>
                  <li>에이전트가 최대한 빨리 종료상태에 도달하도록 동기부여함</li>
                </ul>
              </li>
              <li>Discount factor $\gamma$ : 1 (에피소딕)</li>
            </ul>
          </li>
          <li>
            <p>문제에 Sarsa 를 적용하고 그 결과를 해석</p>

            <p><img src="/assets/images/posts/sarsa_example_windy_gridworld_2.png" alt="sarsa_example_windy_gridworld_2" /></p>

            <ul>
              <li>
                <p>Sarsa 설정값</p>

                <ul>
                  <li>정책 : $\varepsilon$-greedy action selection
                    <ul>
                      <li>$\varepsilon = 0.1$</li>
                    </ul>
                  </li>
                  <li>$\alpha = 0.5$</li>
                  <li>초기값 = 0
                    <ul>
                      <li>optimistic initial values : 초기 탐색 장려</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>
                <p>결과 해석</p>
                <ul>
                  <li>그래프 : 각 스텝 별로 몇 번의 에피소드를 끝냈는지를 의미
                    <ul>
                      <li>위 결과는 각각 100번의 시행의 평균 값임</li>
                      <li>첫 몇 번의 에피소드는 2000 스텝을 진행하고 종료됨</li>
                      <li>그래프의 기울기는 점점 가파르게 상승하고 이는 짧은 스텝으로 에피소드를 종료함을 뜻함</li>
                      <li>7000 스텝 즈음 기울기는 더이상 상승하지 않는다. (탐욕 정책의 개선 종료-최적화)</li>
                    </ul>
                  </li>
                  <li>몬테카를로 방법은 위 방식에 맞지 않다.
                    <ul>
                      <li>많은 정책이 종료상태까지 도달하지 못함</li>
                      <li>몬테카를로 방식은 에피소드가 종료되어야 학습을 시작함
                        <ul>
                          <li>따라서 결정론적 정책은 함정에 빠지고, 좋은 정책을 배우지 못하게 됨
                            <ul>
                              <li>예를 들어 초기 정책이 왼쪽으로 이동하는 것이라면 에피소드가 영원히 끝나지 않게 됨</li>
                            </ul>
                          </li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li>Sarsa 는 에피소드를 진행하며 현재의 정책이 안좋은 정책이라는 것을 학습하고, 정책을 바꾸게 된다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="off-policy-td-control--q-learning">Off-policy TD Control : Q-learning</h2>

<ul>
  <li>
    <p>What is Q-learning?</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Q-learning 알고리즘 서술하기.</li>
          <li>Q-learning 과 벨만 최적 방정식 (Bellman Optimality equations) 간 관계 설명하기.</li>
        </ul>
      </li>
      <li>The Q-learning algorithm
        <ul>
          <li>Q 러닝은 1989년에 개발되었고, 강화학습 알고리즘 중 첫번째 메인 온라인 학습 알고리즘이다.</li>
        </ul>

        <p><img src="/assets/images/posts/q_learning_psuedo_code.png" alt="q_learning_psuedo_code" /></p>

        <ul>
          <li>위 그림은 Q-learning 알고리즘의 psuedo code 이다.
            <ul>
              <li>에이전트는 상태에서 행동을 선택하고, 다음 상태와 보상을 관측한다.</li>
              <li>이후 업데이트를 진행하고 사이클이 반복된다.</li>
              <li>타 알고리즘과의 차이점은 업데이트 규칙에 있다.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/q_learning_sarsa_bellman_equation.png" alt="q_learning_sarsa_bellman_equation" /></p>

        <ul>
          <li>위 그림은 벨만방정식과 Sarsa, Q-learning 간의 수식 비교이다.
            <ul>
              <li>Sarsa : 벨만방정식의 행동가치 식과 유사
                <ul>
                  <li>Sarsa 는 벨만방정식의 행동가치 식을 풀기 위한 샘플기반의 알고리즘이다.</li>
                  <li>Sarsa 는 샘플기반의 정책 반복 (policy iteration) (벨만 방정식 행동가치함수 사용)</li>
                </ul>
              </li>
              <li>Q-learning : 벨만최적방정식의 행동가치 식과 유사
                <ul>
                  <li>Q-learning 또한 환경으로부터의 샘플을 이용해 벨만 방정식을 풀지만, 벨만 방정식 대신 벨만 최적 방정식을 사용한다.</li>
                  <li>즉, $q_*$ 를 바로 학습함으로서, 정책평가와 정책개선 단계를 번갈아 가며 진행할 필요가 없게 된다.</li>
                  <li>Q-learning 은 샘플 기반의 가치 반복 (value iteration) (벨만 최적 방정식 행동가치함수 사용)</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Q-learning 은 Value iteration 과 마찬가지로 최적가치함수에 수렴한다. (에이전트가 모든 상태-행동 쌍에 대해 지속적으로 탐색한다는 가정)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Q-learning in the Windy Grid World</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Q-learning 이 예제 MDP 에서 작동하는 방식 이해</li>
          <li>단일 MDP 에서 여러 학습 알고리즘의 성능을 비교하는 방식 경험</li>
          <li>Q-learning 과 Sarsa 의 차이점 이해하기.</li>
        </ul>
      </li>
      <li>
        <p>The Windy Gridworld</p>

        <p><img src="/assets/images/posts/q_learning_sarsa_windy_grid_world_1.png" alt="q_learning_sarsa_windy_grid_world_1" /></p>

        <ul>
          <li>Sarsa 와 Q-learning 간 비교 ($\alpha = 0.5$)
            <ul>
              <li>Q-learning 이 Sarsa 보다 뛰어난 결과를 보여준 이유를 명확히 설명할 수 없지만, 아마 update target 이 더 stable 했기 때문으로 추정함.
                <ul>
                  <li>Sarsa 의 경우 탐색에 해당하는 샘플의 업데이트가 큰 영향을 줬을 수도 있음. (탐색적인 행동을 할 때마다 update target 이 변경될 수 있음)</li>
                  <li>Q-learning 의 경우 max 값을 이용해 업데이트 하기 때문에, 한 행동이 이전 행동보다 더 낫다는 것을 학습할 때에 update target 이 변경됨.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/q_learning_sarsa_windy_grid_world_2.png" alt="q_learning_sarsa_windy_grid_world_2" /></p>

        <ul>
          <li>
            <p>Sarsa 와 Q-learning 간 비교 (Salsa : $\alpha = 0.1$)</p>

            <ul>
              <li>더 나은 Sarsa 의 결과를 위한 파라미터의 변경
                <ul>
                  <li>$\alpha = 0.1$</li>
                  <li>더 많은 Time Steps 의 진행</li>
                </ul>
              </li>
              <li>Sarsa 와 Q-learning 모두 동일한 정책으로 수렴함</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>강화학습은 파라미터의 값에 따라 다른 결과물을 보여줌.
        <ul>
          <li>$\alpha$, $\varepsilon$, initial values, length of the experiments</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>How is Q-learning off-policy?</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Q-learning 이 importance sampling 없이 off-policy 로 동작할 수 있는 이유를 이해하기.</li>
          <li>On-policy 기반과 Off-policy 기반 학습이 각각 제어 성능에 어떤 영향을 주는지 서술하기.</li>
        </ul>
      </li>
      <li>
        <p>On-policy 와 Off-policy 관점에서 Sarsa 와 Q-learning 비교</p>

        <p><img src="/assets/images/posts/q_learning_sarsa_on_policy_off_policy.png" alt="q_learning_sarsa_on_policy_off_policy" /></p>

        <ul>
          <li>On-policy 와 Off-policy 복습
            <ul>
              <li>에이전트는 목표정책(target policy) 의 예상 리턴값을 토대로 가치함수를 추측한다.</li>
              <li>에이전트는 행동정책(behavior policy) 에 따라 실제 행동한다.</li>
              <li>목표정책과 행동정책이 같다면, 에이전트가 On-policy 학습을 한다 고 한다.</li>
              <li>목표정책과 행동정책이 다르다면, 에이전트는 Off-policy 학습을 한다 고 한다.</li>
            </ul>
          </li>
          <li>Sarsa : On-policy algorithm
            <ul>
              <li>에이전트가 다음에 수행할 작업의 값을 부트스트래핑 한다.
                <ul>
                  <li>다음 수행할 작업의 값은 행동정책 ($\pi$) 에 의해 샘플링한 값이다.</li>
                </ul>
              </li>
              <li>즉 Sarsa 는 On-policy 학습이다.</li>
            </ul>
          </li>
          <li>Q-learning : Off-policy algorithm
            <ul>
              <li>에이전트의 다음상태에 해당하는 행동값 중 가장 큰 값을 부트스트래핑한다.
                <ul>
                  <li>다음상태의 행동값 중 가장 큰 값은 탐욕정책(최적정책) ($\pi_* \noteq \pi$) 에 의해 샘플링된 값이다.</li>
                </ul>
              </li>
              <li>즉 Q-learning 은 Off-policy 학습이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Q-learning 의 Behavior policy 와 Target policy</p>

        <p><img src="/assets/images/posts/q_learning_behavior_policy_target_policy.png" alt="q_learning_behavior_policy_target_policy" /></p>

        <ul>
          <li>강화학습에서의 자연스러운 질문 : 목표정책(target policy) 과 행동정책(behavior policy)은 무엇인가?
            <ul>
              <li>Q-learning 에서의 Target policy : 현재 값에 대한 탐욕 정책</li>
              <li>Q-learning 에서의 Behavior policy : 모든 상태-행동 쌍에 접근할 수 있는 어떠한 정책도 될 수 있음 (예: $\varepsilon$-greedy)</li>
            </ul>
          </li>
          <li>위에서 볼 수 있듯 Target policy 와 Behavior policy 가 다르므로 Off-policy 라 할 수 있다.</li>
        </ul>
      </li>
      <li>
        <p>Q-learning with No importance sampling</p>

        <p><img src="/assets/images/posts/q_learning_no_importance_sampling.png" alt="q_learning_no_importance_sampling" /></p>

        <ul>
          <li>Q-learning 이 importance sampling 이 필요없는 이유
            <ul>
              <li>에이전트가 정해지지 않은 정책으로부터 행동값을 추정하고 있기 때문
                <ul>
                  <li>importance sampling ratio 로 행동 선택의 차이를 수정할 필요가 없다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>상태가치함수 추정의 예시
            <ul>
              <li>행동가치함수는 주어진 상태에서 각 행동의 리턴값을 나타낸다.</li>
              <li>에이전트의 target policy 는 주어진 상태에서 각각의 행동을 할 확률을 나타낸다.</li>
              <li>즉 에이전트는 주어진 상태에서 target policy 를 따를 때 예상되는 리턴을 위 2개의 항목을 결합하여 나타낼 수 있다.</li>
            </ul>
          </li>
          <li>Q-learning 의 target policy - greedy policy
            <ul>
              <li>가장 큰 리턴값을 가지는 행동을 선택. 즉, 다른 행동은 확률 0 을 가지게 됨.</li>
              <li>해당 상태에서의 리턴 예상값은 행동값의 최대 값과 동일하다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Q-learning 이 성능에 영향을 주는 경우
        <ul>
          <li>Q-learning 은 정책반복 (정책평가와 정책개선) 을 하지 않고 바로 최적값을 직접 학습한다.</li>
          <li>
            <p>최적가치함수와 정책을 직접 학습하는 것은 효율적으로 보이나, 가끔 미묘한 경우가 존재한다.</p>

            <p><img src="/assets/images/posts/q_learning_bad_case.png" alt="q_learning_bad_case" /></p>

            <ul>
              <li>예 : $\varepsilon$-greedy 정책의 Q-learning 의 경우 Cliff walking 문제에서 Sarsa 보다 안좋은 결과를 보여준다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="expected-sarsa">Expected Sarsa</h2>

<ul>
  <li>
    <p>Expected Sarsa</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Expected Sarsa 알고리즘 설명</li>
        </ul>
      </li>
      <li>
        <p>벨만방정식 (action-values) 과 Sarsa 알고리즘</p>

        <p><img src="/assets/images/posts/expected_sarsa_bellman_equation_sarsa.png" alt="expected_sarsa_bellman_equation_sarsa" /></p>

        <ul>
          <li>위 벨만방정식을 살펴보면, 해당 상태-행동 쌍의 온전한 값을 구하기 위해 상태전이에 의한 다음상태의 합계와 정책에 의한 행동에 대한 합계를 구하는 것을 알 수 있다.</li>
          <li>Sarsa 의 업데이트 식의 경우 Error 를 구하는 부분에서 다음 상태의 값을 샘플링하여 계산하는데, 이 샘플링 데이터는 상태전이와 정책에 의한 행동 요소가 들어가 있다.
            <ul>
              <li>이 때, 에이전트는 이미 정책을 알고 있으므로, 이 부분을 샘플링 데이터에 의지하지 않고 계산을 하면 어떻게 될까?</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Expected Sarsa 알고리즘</p>

        <p><img src="/assets/images/posts/expected_sarsa_algorithm_1.png" alt="expected_sarsa_algorithm_1" /></p>

        <ul>
          <li>명시적으로 다음 행동의 예측값을 계산하는 것이 Expected Sarsa 의 핵심 아이디어이다.</li>
          <li>Expected Sarsa 또한 벨만 방정식 (action-values) 에 기반을 두고 있기에 업데이트식의 형식은 타 알고리즘과 유사하다.
            <ul>
              <li>차이점은 TD error 값을 샘플링을 통한 부트스트래핑 값이 아닌 다음 행동의 예측값으로서 정책과 부트스트래핑 값을 이용해 계산을 해낸다는 점이다.
                <ul>
                  <li>이것의 의미는 매 타임스텝 마다 정책 하의 예측 값(부트스트래핑)을 계산해야 한다는 의미이다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/expected_sarsa_algorithm_2.png" alt="expected_sarsa_algorithm_2" /></p>

        <ul>
          <li>Expected Sarsa 는 Sarsa 에 비해 더 안정적인 update target 을 가지게 된다.
            <ul>
              <li>Sarsa 의 경우 샘플에 따라 잘못된 방향 (target) 으로 업데이트가 될 수 있다.
                <ul>
                  <li>물론 많은 업데이트를 통해 참 값으로 수렴하게 된다.</li>
                </ul>
              </li>
              <li>Expected Sarsa 의 경우 곧바로 참 값 (target) 으로 업데이트가 이루어진다.
                <ul>
                  <li>이는 분산이 작아지는 효과를 가져온다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Expected Sarsa 의 단점은 컴퓨팅 리소스 부분이다.
            <ul>
              <li>행동의 가지수가 늘어날 수록 계산 가격이 비싸진다. (특히 매 타임스텝마다 계산되는 부분이므로 더 부담스럽다.)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Expected Sarsa in the Cliff World</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Expected Sarsa 의 행동을 예시 MDP에서 설명</li>
          <li>Expected Sarsa 와 Sarsa 의 경험적 비교</li>
        </ul>
      </li>
      <li>
        <p>Cliff Walking 환경에서의 Sarsa 와 Expected Sarsa 비교 ($\varepsilon = 0.1$, 100회 학습, 50000회 독립시행 평균결과)</p>

        <p><img src="/assets/images/posts/expected_sarsa_cliff_world_result_1.png" alt="expected_sarsa_cliff_world_result_1" /></p>

        <ul>
          <li>Expected Sarsa 는 Sarsa 에 비해 더 큰 $\alpha$ 값을 사용할 수 있다.
            <ul>
              <li>정책의 무작위성을 명시적 평균을 통해 해결</li>
              <li>이 문제에서의 상태이동은 결정론적이기에 다른 무작위성이 없다.</li>
              <li>즉 위 두 조건으로 Expected Sarsa 의 업데이트는 결정론적인 값이 된다.</li>
            </ul>
          </li>
          <li>Sarsa 의 업데이트의 경우 다음 행동에 크게 영향을 받게 된다.</li>
        </ul>
      </li>
      <li>
        <p>Cliff Walking 환경에서의 Sarsa 와 Expected Sarsa 비교 ($\varepsilon = 0.1$, 100000회 학습, 50000회 독립시행 평균결과)</p>

        <p><img src="/assets/images/posts/expected_sarsa_cliff_world_result_2.png" alt="expected_sarsa_cliff_world_result_2" /></p>

        <ul>
          <li>Expected Sarsa 는 $\alpha$ 값에 영향 없이 같은 결과를 보여줌
            <ul>
              <li>업데이트가 결정론적이기 때문</li>
              <li>위 경우 step size 는 얼마나 목표 값에 빠르게 수렴하는지에만 영향을 줌</li>
            </ul>
          </li>
          <li>Sarsa 의 경우 $\alpha$ 값에 큰 영향을 받음
            <ul>
              <li>$\alpha$ 값이 크면 수렴에 실패하며, $\alpha$ 값이 작으면 오랜 시간 학습 끝에 Expected Sarsa 와 거의 동일한 결과를 보여줌</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Generality of Expected Sarsa</p>

    <ul>
      <li>Sarsa, Q-learning, Expected Sarsa 간 비교 (도입부)
        <ul>
          <li>Sarsa 와 Expected Sarsa 는 동일한 벨만 방정식으로부터 유도되었다는 점에서 유사성이 있음.</li>
          <li>Q-learning 과 Expected Sarsa 간에는 어떠한 관계가 있을까?</li>
        </ul>
      </li>
      <li>학습목표
        <ul>
          <li>Expected Sarsa 가 importance sampling 없이 off-policy 학습을 할 수 있는 이유 설명</li>
          <li>Expected Sarsa 가 Q-learning 의 일반화 버전인 이유</li>
        </ul>
      </li>
      <li>
        <p>Off-policy Expected Sarsa</p>

        <p><img src="/assets/images/posts/expected_sarsa_off_policy.png" alt="expected_sarsa_off_policy" /></p>

        <ul>
          <li>(On-policy 의 경우를 가정) On-policy 의 경우에도 Expected Sarsa 는 실제 다음 선택된 행동 관련 업데이트를 하는 것이 아닌 정책 $\pi$ 에 관한 업데이트를 진행함
            <ul>
              <li>즉, $\pi$ 가 behavior policy 와 같을 필요가 없다고 볼 수 있음.</li>
              <li>위의 관점은 Expected Sarsa 가 Q-learning 과 같이 importance sampling 없이 Off-policy 학습이 가능함을 보여줌.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Greedy Expected Sarsa</p>

        <p><img src="/assets/images/posts/expected_sarsa_greedy_q_learning_1.png" alt="expected_sarsa_greedy_q_learning_1" /></p>

        <ul>
          <li>만약 학습할 target policy 가 탐욕정책이라면 어떻게 될까?
            <ul>
              <li>이는 다음 상태에서 최대값을 이끌어내는 행동을 수행하는 것이고, Q-learning 과 동일한 방식이다.</li>
            </ul>

            <p><img src="/assets/images/posts/expected_sarsa_greedy_q_learning_2.png" alt="expected_sarsa_greedy_q_learning_2" /></p>

            <ul>
              <li>즉, Q-learning 은 Expected Sarsa 의 특별한 케이스이다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Week 3 Summary</p>

    <ul>
      <li>
        <p>TD control and Bellman equations</p>

        <p><img src="/assets/images/posts/td_control_and_bellman_equations.png" alt="td_control_and_bellman_equations" /></p>

        <ul>
          <li>TD 제어는 벨만 방정식에 기반을 둔다.
            <ul>
              <li>Sarsa
                <ul>
                  <li>벨만 방정식의 샘플 기반 버전</li>
                  <li>$q_\pi$ 를 학습</li>
                  <li>On-policy 알고리즘 (현재 정책의 action value 를 학습)</li>
                </ul>
              </li>
              <li>Q-learning
                <ul>
                  <li>벨만 최적 방정식을 이용</li>
                  <li>$q_*$ 를 학습</li>
                  <li>Off-policy 알고리즘 (현재 정책과 무관하게 optimal action value 를 학습)</li>
                </ul>
              </li>
              <li>Expected Sarsa
                <ul>
                  <li>Sarsa 와 동일한 벨만 방정식을 이용</li>
                  <li>그래나 Sarsa 와 샘플링을 하는 방식이 다름
                    <ul>
                      <li>다음 행동가치의 예측값 (Sum) 을 활용함.</li>
                    </ul>
                  </li>
                  <li>On-policy, Off-policy 둘다 가능
                    <ul>
                      <li>behavior policy 가 deterministic 하다면 On-policy 가 된다.</li>
                      <li>behavior policy 가 탐욕정책이라면 Q-learning 과 동일한 알고리즘이 되어버린다. (Q-learning 은 Expected Sarsa 의 특이 케이스)</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Subtleties with off-policy control (Off-policy control 의 미묘함)</p>

        <p><img src="/assets/images/posts/subtleties_with_off_policy_control_1.png" alt="subtleties_with_off_policy_control_1" /></p>

        <ul>
          <li>Online 학습에서 Q-learning 은 Sarsa 보다 값이 안좋게 나오는데, 이는 탐색정책의 랜덤성 때문이다.</li>
        </ul>

        <p><img src="/assets/images/posts/subtleties_with_off_policy_control_2.png" alt="subtleties_with_off_policy_control_2" /></p>

        <ul>
          <li>Expected Sarsa 의 경우 Online 학습에서 Sarsa 와 동일한 결과를 보여주면서, 동시에 더 좋은 학습률을 보여준다.
            <ul>
              <li>학습한 정책이 동일하다.</li>
              <li>다음 행동가치의 예측값으로 학습하기 때문에, behavior policy 에 학습의 영향을 받는 Sarsa 보다 더 안정적인 학습을 한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Chapter summary</p>

    <ul>
      <li>TD learning
        <ul>
          <li>예측 : TD 방식은 예측 문제 해결을 위한 Monte Carlo 방법의 대안임.</li>
          <li>제어 : 두 경우 모두 제어 문제에 대한 확장은 동적 프로그래밍에서 추상화한 일반화된 정책 반복(GPI)의 아이디어를 통해 이루어짐
            <ul>
              <li>근사 정책과 가치 함수가 둘 다 최적을 향해 움직이는 방향으로 상호 작용해야 한다는 아이디어임</li>
              <li>가치 함수를 구동하여 현재 정책의 수익을 정확하게 예측 (예측의 문제)</li>
              <li>현재 가치 함수와 관련하여 정책을 국지적으로 개선(예: e-greedy)하도록 유도 (정책개선)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>TD 제어 방법의 분류
        <ul>
          <li>경험을 기반으로 하는 경우에서의 탐색 문제가 발생한다.</li>
          <li>On-policy : Sarsa</li>
          <li>Off-policy : Q-learning, Expected Sarsa</li>
          <li>이 외에 행위자 비평 방법 (Actor-Critic) 이 있는데 이러한 방법은 13장에서 다룸.</li>
        </ul>
      </li>
      <li>TD learning 의 장점
        <ul>
          <li>가장 널리 사용되는 강화학습 방법
            <ul>
              <li>매우 단순
                <ul>
                  <li>최소한의 계산으로 환경과의 상호 작용에서 생성된 경험에 온라인으로 적용할 수 있음</li>
                  <li>작은 컴퓨터 프로그램으로 구현할 수 있는 단일 방정식으로 거의 완벽하게 표현</li>
                </ul>
              </li>
              <li>단순한 알고리즘을 확장하여 약간 더 복잡하고 훨씬 강력하게 만들 수 있음
                <ul>
                  <li>본질은 TD 의 것 그대로임.
                    <ul>
                      <li>적은 계산으로 온라인경험 처리 가능</li>
                      <li>TD 오류에 의해 구동됨</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>이 장에서 다룬 TD 학습법의 특징
        <ul>
          <li>1 Step</li>
          <li>Tabular</li>
          <li>모델이 없는 TD 방법</li>
        </ul>
      </li>
      <li>앞으로 배울 내용
        <ul>
          <li>n단계 형식(Monte Carlo 방법 관련)과 환경 모델을 포함하는 형식(계획 및 동적 프로그래밍 관련)으로 확장</li>
          <li>테이블(딥 러닝 및 인공 신경망 관련)이 아닌 다양한 형태의 함수 근사치로 확장</li>
        </ul>
      </li>
      <li>일반적인 방법으로서의 TD
        <ul>
          <li>강화 학습의 문제 맥락에서 TD 방법에 대해 살펴보았지만 실제로 TD 방법은 이보다 더 일반적임
            <ul>
              <li>동적 시스템에 대한 장기 예측을 학습하기 위한 일반적인 방법</li>
              <li>강화학습이 아닌 순수한 예측의 방법으로 분석
                <ul>
                  <li>재무 데이터, 수명, 선거 결과, 날씨 패턴, 동물 행동, 발전소에 대한 수요, 고객 구매 예측 등의 문제에도 쓰일 수 있음</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>아직 TD 학습 방법의 잠재적 응용은 광범위하게 탐색되지 않음.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Temporal Difference Learning" /><category term="TD Learning" /><summary type="html"><![CDATA[관련 자료 (RLbook Pages 129-134)]]></summary></entry><entry><title type="html">Sample-based Learning Methods - 02. Week 2. Temporal Difference Learning Methods for Prediction</title><link href="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_02_Week2/" rel="alternate" type="text/html" title="Sample-based Learning Methods - 02. Week 2. Temporal Difference Learning Methods for Prediction" /><published>2023-07-07T10:00:00+09:00</published><updated>2023-07-07T10:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_02_Week2</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_02_Week2/"><![CDATA[<h2 id="관련-자료-rlbook-pages-119-128">관련 자료 (RLbook Pages 119-128)</h2>

<ul>
  <li>개요
    <ul>
      <li>강화 학습의 중심적이고 참신한 아이디어로 하나를 선택해야 한다면 temporal-difference (TD) learning (시간차학습) 이 될 것이다.</li>
      <li>TD 학습은 몬테카를로 방식과 동적프로그래밍(DP) 방식의 조합이다.
        <ul>
          <li>TD 학습은 몬테카를로 방식과 마찬가지로 역학 없이 원시 경험에서 직접 학습할 수 있다.</li>
          <li>TD 학습은 DP 방식과 마찬가지로 최종 결과를 기다리지 않고 다른 학습된 추정치를 기반으로 추정치를 업데이트한다. (부트스트랩)</li>
        </ul>
      </li>
      <li>TD, DP 및 몬테카를로 방법 간의 관계는 강화학습에서 되풀이 되는 주제이다.
        <ul>
          <li>TD 에서 몬테카를로 방법으로 연결되는 n-step 알고리즘을 소개</li>
          <li>TD 와 몬테카를로를 완벽하게 통합하는 TD($\lambda$) 알고리즘을 소개</li>
        </ul>
      </li>
      <li>제어 문제 (최적 정책 찾기) 의 경우 DP, TD, 몬테카를로 방법 모두 일반화된 정책 반복(GPI) 의 일부 변형을 사용함.</li>
      <li>정책평가와 예측문제, 주어진 정책 $\pi$ 에 대한 가치함수 $v_\pi$ 를 추정하는 문제에 초점을 맞춤</li>
    </ul>
  </li>
  <li>TD Prediction
    <ul>
      <li>몬테카를로 방식과 TD 방식
        <ul>
          <li>공통점
            <ul>
              <li>몬테카를로와 TD 방법 모두 경험을 사용하여 예측 문제를 해결함.</li>
              <li>정책 $\pi$ 에 따른 일부 경험을 통해 해당 경험에서 발생하는 비종료 상태 $S_t$ 에 대한 $v_\pi$ 의 추정치 $V$ 를 업데이트한다.</li>
            </ul>
          </li>
          <li>몬테카를로 방식
            <ul>
              <li>몬테카를로 방식은 방문에 대한 리턴값을 알 때까지 기다린 다음, 해당 값을 $V(S_t)$ 의 목표값으로 사용한다.</li>
              <li>
                <p>비정상성 환경에서 가장 적합한 단순한 every-visit 몬테카를로 방식은 아래와 같다.</p>

                <p><img src="/assets/images/posts/6_1_1_mc_every_visit_nonstationary.png" alt="6_1_1_mc_every_visit_nonstationary" /></p>

                <ul>
                  <li>$G_t$ : $t$ 시점의 실제 리턴값</li>
                  <li>$\alpha$ : 상수값의 step-size 파라미터</li>
                </ul>
              </li>
              <li>위의 방식을 constant-$\alpha$ MC 라 한다.</li>
              <li>몬테카를로 방식은 에피소드가 끝나고 $V(S_t)$ 의 증분값이 결정될 때까지 기다려야만 한다. (그래야만 $G_t$ 를 알 수 있음)</li>
            </ul>
          </li>
          <li>TD 방식
            <ul>
              <li>TD 방식은 다음 time step 까지만 기다리면 된다.</li>
              <li>
                <p>$t+1$ 시점에 즉시 목표를 생성하고, 관측된 보상 $R_{t+1}$ 과 추정치 $V(S_{t+1})$ 을 이용하여 유효한 업데이트를 진행한다.</p>

                <p><img src="/assets/images/posts/6_1_2_td_update_for_estimate_V.png" alt="6_1_2_td_update_for_estimate_V" /></p>

                <ul>
                  <li>가장 단순한 TD 방식은 $S_{t+1}$ 로 전환되고 $R_{t+1}$ 을 받자마자 업데이트 된다.</li>
                  <li>몬테카를로 업데이트에서 목표값은 $G_t$ 였으나, TD 업데이트에서는 $R_{t+1} + \gamma V(S_{t+1}$ 이다.</li>
                  <li>위 방식을 TD(0) 혹은 one-step TD 라 부른다.</li>
                  <li>이는 TD($\lambda$) 의 특이 케이스이며 n-step TD 는 추후에 다룬다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>TD(0)
        <ul>
          <li>
            <p>TD(0) 에 대해</p>

            <p><img src="/assets/images/posts/6_1_3_td_0_psuedo_code.png" alt="6_1_3_td_0_psuedo_code" /></p>

            <ul>
              <li>TD(0) 는 기 존재하는 추정값의 일부로 업데이트를 하기 때문에 DP처럼 부트스트래핑 한다고 볼 수 있다.</li>
            </ul>

            <p><img src="/assets/images/posts/6_1_4_td_bootstrapping.png" alt="6_1_4_td_bootstrapping" /></p>

            <ul>
              <li>단순히 말해서, 몬테카를로 방식은 (6.3) 의 추정치를 목표로 사용하는 것이고, DP 의 경우 (6.4) 를 추정치의 목표로 삼는 것이다.</li>
              <li>몬테카를로 목표는 (6.3) 의 예상 값을 알 수 없기 때문에 추정치이다. (실제 예상 리턴값 대신 샘플 리턴값이 사용됨)</li>
              <li>DP 목표는 환경 모델에서 완전히 제공된다고 가정한 기대값 때문이 아니라 $v_\pi(S_{t+1})$ 을 알 수 없고, 현재 추정값 $V(S_{t+1})$ 이 사용되기에 추정치이다.</li>
              <li>TD 목표는 두 가지 이유로 추정치이다.
                <ul>
                  <li>(6.4) 에서 예상 값을 샘플링함.</li>
                  <li>실제 $v_\pi$ 대신 현재 추정치 $V$ 를 사용함.</li>
                </ul>
              </li>
              <li>따라서 TD 방법은 몬테카를로의 샘플링과 DP 의 부트스트래핑을 결합한 방식이다.</li>
            </ul>
          </li>
          <li>
            <p>TD(0) 의 백업 다이어그램</p>

            <p><img src="/assets/images/posts/6_1_5_td_0_backup_diagram.png" alt="6_1_5_td_0_backup_diagram" /></p>

            <ul>
              <li>위 그림은 tabular TD(0) 의 백업 다이어그램이다.</li>
              <li>백업 다이어그램 상단에 있는 상태 노드에 대한 값 추정치는 바로 다음 상태로의 하나의 샘플 전환을 기반으로 업데이트 된다.
                <ul>
                  <li>우리는 TD 및 몬테카를로 업데이트를 샘플 업데이트라 한다.</li>
                  <li>백업된 값을 계산하는 과정에서 후속 값과 보상을 사용 (샘플의 후속 상태 (또는 상태-행동 쌍) 을 봄) 하여 원래 상태의 값을 업데이트함.</li>
                  <li>샘플 업데이트는 가능한 모든 후속 작업의 전체 배포가 아닌 단일 샘플 후속 작업을 기반으로 한다는 점에서 DP의 예상값 업데이트와 다르다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>TD error</p>
            <ul>
              <li>위 TD(0) 업데이트 pusedo code 의 대괄호 내의 값은 에러의 한 종류 ($S_t$ 의 추정치와 더 나은 추정값 $R_{t+1} + \gamma V(S_{t+1})$ 간의 차이) 이다.</li>
              <li>이 값을 TD error 라 부르며, 강화 학습 전반에 걸쳐 다양한 형태로 나타난다.</li>
            </ul>

            <p><img src="/assets/images/posts/6_1_6_td_error.png" alt="6_1_6_td_error" /></p>

            <ul>
              <li>각 시간의 TD error 는 해당 시간에 만들어진 추정치의 오류이다.</li>
              <li>TD error 는 다음 상태와 보상에 영향을 받으므로 다음 스텝의 진행 이전까지 접근할 수 없다.</li>
              <li>즉 , $\delta_t$ 는 시간 $t+1$ 에 사용할 수 있는 $V(S_t)$ 의 오류이다.</li>
              <li>또한 배열 $V$ 가 에피소드 내 변경이 없을 경우 (몬테카를로 방법에서는 변경되지 않음) Monte Carlo error 는 TD errors 의 합으로 기록될 수 있다.</li>
            </ul>

            <p><img src="/assets/images/posts/6_1_7_monte_carlo_td_error.png" alt="6_1_7_monte_carlo_td_error" /></p>

            <ul>
              <li>위 항등식은 에피소드 중 $V$ 가 업데이트 되는 경우 (TD(0) 와 같이) 정확하지 않지만, step-size 가 작으면 여전히 대략적으로 유지될 수 있다.</li>
              <li>이 항등식의 일반화는 시간차 학습의 이론과 알고리즘에서 중요한 역할을 한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>예제 6.1 : Driving Home
        <ul>
          <li>문제의 설정
            <ul>
              <li>매일 직장에서 집으로 운전하여 퇴근하는데 걸리는 시간을 예측하고자 함.</li>
              <li>사무실을 떠날 때 시간, 요일, 날짜 및 관련이 있을 수 있는 모든 것을 메모함</li>
              <li>금요일에 정확히 6시에 출발한다고 가정하고 집에 도착하는 데 30분이 소요될 것으로 예상함.</li>
              <li>차에 도착했을 때 시간은 6시 5분이고 비가 내리기 시작하는 것을 알게 됨.</li>
              <li>비가 오면 교통 체증으로 인해 느려지는 경우가 많으므로 그 때부터 35분, 즉 총 40분이 소요될 것으로 재추산함.</li>
              <li>15분 후 고속도로 구간을 순조롭게 완주하였고, 2차 도로로 나가면 예상 총 이동시간이 35분으로 단축됨.</li>
              <li>불행하게도 이 시점, 느린 트럭 뒤에 갇히고 도로가 너무 좁아 추월할 수 없어 6시 40분까지 트럭을 따라감.</li>
              <li>3분 후 집에 도착함.</li>
              <li>상태, 시간 및 예측의 순서는 아래와 같음.</li>
            </ul>

            <p><img src="/assets/images/posts/6_1_8_example_6_1_1.png" alt="6_1_8_example_6_1_1" /></p>

            <ul>
              <li>보상은 여정의 각 구간에서 경과된 시간임.</li>
              <li>할인을 적용하지 않음($\gamma = 1$). 따라서 각 상태에 대한 수익은 해당 상태에서 이동하는 실제 시간임.</li>
              <li>각 상태의 값은 이동 예상 시간임.</li>
              <li>위 표의 두번째 열은 각 상태에 대한 현재 예상 값을 제공함.</li>
            </ul>
          </li>
          <li>
            <p>문제의 해석</p>

            <p><img src="/assets/images/posts/6_1_9_figure_6_1.png" alt="6_1_9_figure_6_1" /></p>

            <ul>
              <li>몬테카를로 방법의 동작을 보는 간단한 방법은 시퀀스에 대한 예측 총 시간 (마지막 열) 을 플로팅 하는 것이다. (위 그림 참조)
                <ul>
                  <li>빨간 화살표는 $\alpha = 1$ 인 경우의 constant-$\alpha$ MC 방식 (6.1) 에서 권장하는 예측의 변화를 보여준다.</li>
                  <li>이것은 정확히 각 상태의 예상 값 (예상되는 이동시간) 과 실제 리턴값 (실제 이동 시간) 사이의 오차이다.
                    <ul>
                      <li>예를 들어 고속도로를 빠져나올 때 집 까지 15분 더 걸릴 줄 알았는데 실제로는 23분이 걸림.</li>
                      <li>방정식 6.1 이 이 시점에 적용되며 고속도로를 빠져나간 후 이동하는 예상 시간의 증분을 결정함.</li>
                      <li>이 때 error $G_t - V(S_t)$ 는 8분이다.</li>
                      <li>step size 매개변수 $\alpha$ 가 $\frac{1}{2}$ 라 가정하면, 이 경험의 결과로 고속도로를 빠져나온 후 예상 소요시간이 4분 상향조정됨.
                        <ul>
                          <li>이 경우 너무 큰 변화일 수 있음.</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li>어떤 경우든 변경은 오프라인에서만, 집에 도착한 후에만 가능함. 이 시점에서만 실제 수익을 알 수 있음.</li>
                </ul>
              </li>
              <li>학습을 시작하기 전에 최종 결과가 알려질 때까지 기다려야 하는가?
                <ul>
                  <li>또 다른 날 사무실에서 집까지 차로 30분 걸릴 것이라 예상했지만, 엄청난 교틍 체증에 갇히게 되었다고 가정해보자.
                    <ul>
                      <li>사무실을 나온 지 25분이 지나도 여전히 고속도로 위에 있다.</li>
                      <li>여기에서부터 25분이 더 소요될 것으로 예상한다. (총 50분)</li>
                      <li>교통체증 속에서 이미 초기 예상시간인 30분은 너무 낙관적이었다는 것을 알고 있음.</li>
                      <li>그럼에도 초기상태에 대한 추정치를 늘리기 전에 집에 도착할 때까지 기다려야 하는가?</li>
                      <li>몬테카를로 접근 방식에 따르면 진정한 리턴값을 모르기 때문에 반드시 기다려야 한다.</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>TD 접근방식의 경우
                <ul>
                  <li>TD 접근 방식에 따르면 즉시 학습하여 초기 추정치를 30분에서 50분으로 이동함.</li>
                  <li>실제로 각 추정치는 바로 뒤의 후속 추정치로 이동함.</li>
                  <li>첫 날로 돌아가서 그림 (6.1) 은 TD 규칙 (6.2) 에서 권장하는 예측의 변경 사항을 보여줌. (이는 $\alpha = 1$ 인 경우 규칙에 의해 변경됨.)</li>
                  <li>각 오류는 예측의 시간 경과에 따른 변화 (즉, 예측의 시간적 차이) 에 비례함.</li>
                  <li>실제 반환을 알고 종료될 때까지 기다리는 것보다 현재 예측을 기반으로 학습하는 것이 유리한 몇 가지 계산상의 이유가 있음 (다음 섹션에서 학습)</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Advantages of TD Prediction Methods</p>

    <ul>
      <li>TD 방식은 좋은 방식일까?
        <ul>
          <li>TD 방식은 타 추정치를 기반으로 추정치를 업데이트한다.
            <ul>
              <li>추측으로부터 추측을 배움 (부트스트랩) : 이것은 좋은 방식일까?</li>
            </ul>
          </li>
          <li>TD 방식은 몬테카를로, DP 방식에 비해 어떤 이점이 있는 것일까?</li>
        </ul>
      </li>
      <li>TD 와 몬테카를로, DP 간 비교
        <ul>
          <li>TD 방식은 환경 모델, 다음 상태 확률 분포가 필요하지 않다는 점에서 DP 보다 이점이 있음.</li>
          <li>몬테카를로 방식과 비교하여 TD 방식은 온라인에서 완전 증분 방식우로 자연스럽게 구현될 수 있음.
            <ul>
              <li>몬테카를로 방식은 에피소드가 끝날 때까지 기다려야 함 (반환값을 알기 위해)</li>
              <li>TD 방식은 한 단계만 더 기다려도 된다.
                <ul>
                  <li>일부 응용프로그램에서의 에피소드는 너무 길어, 끝날 때까지 학습을 지연시키는 것은 비효율적임.</li>
                  <li>또한 다른 응용프로그램에서는 에피소드 형태가 아닌 연속적 형태일 수 있음.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>특정 몬테카를로 방식은 실험적인 행동이 이루어진 에피소드를 무시하거나 할인해야 하는데, 이것이 학습 속도에 큰 영향을 줄 수 있다.
            <ul>
              <li>TD 방식은 후속 조치에 관계없이 각 전환에서 학습하기 때문에 이러한 문제에 훨씬 덜 취약하다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>TD 의 수렴 보장
        <ul>
          <li>확실히 실제 결과를 기다리지 않고 다음 추측에서 추측값을 배우는 것은 여전히 정답에 대한 수렴을 보장한다.
            <ul>
              <li>모든 고정된 정책 $\pi$ 에 대해 TD(0) 는 $v_\pi$ 로 수렴하는 것이 입증되었음.</li>
              <li>이는 step-size 파라미터가 충분히 작은 경우에서, 확률적 근사조건(2.7) 을 통해 파라미터 값이 감소할 경우 1의 확률로 수렴하게 된다.</li>
            </ul>
          </li>
          <li>대부분의 수렴 증명은 위에 제시된 알고리즘 (6.2) 이 적용된 테이블 기반의 경우에만 적용되지만 일부는 일반 선형 함수 근사의 경우에도 적용됨. (9장에서 다룸)</li>
        </ul>
      </li>
      <li>TD 의 수렴 속도
        <ul>
          <li>현재로서는 한 방법이 다른 방법보다 더 빨리 수렴된다는 것을 수학적으로 증명할 수 없음.</li>
          <li>이 질문을 표현하는 가장 적절한 공식적인 방법이 무엇인지조차 명확하지 않음.</li>
          <li>그러나 실제로 TD 방식은 일반적으로 예제 6.2 에 설명된 것처럼 확률론적 작업에서 constant-$\alpha$ MC 방법보다 빠르게 수렴하는 것으로 나타남.</li>
        </ul>
      </li>
      <li>예제 6.2 : Random Walk
        <ul>
          <li>문제의 정의
            <ul>
              <li>이 예에서는 MRP (Markov reward process) 상에서의 TD(0) 와 constant-$\alpha$ MC 간 예측 능력을 경험적으로 비교한다.
                <ul>
                  <li>MRP 란?
                    <ul>
                      <li>행동이 없는 MDP</li>
                      <li>환경과 에이전트로 인한 역학을 구분할 필요가 없는, 예측문제에 초점을 맞출 때 MRP 를 사용</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>이 mrp 에서 모든 에피소드는 중앙의 상태 C 에서 시작한다.</li>
              <li>그 후 왼쪽, 또는 오른쪽으로 동일 확률로 한 스탭씩 이동한다.</li>
            </ul>

            <p><img src="/assets/images/posts/6_2_1_example_6_2_random_walk.png" alt="6_2_1_example_6_2_random_walk" /></p>

            <ul>
              <li>에피소드는 좌측 끝 혹은 우측 끝에서 끝난다.
                <ul>
                  <li>오른 쪽에서 에피소드가 종료되면 +1 의 보상이 발생하고 다른 모든 경우의 보상은 0 이다.</li>
                  <li>따라서 일반적인 에피소드의 예는 (C,0, B,0, C,0, D,0, E,1) 과 같은 상태-보상 시퀀스로 구성될 수 있다.</li>
                </ul>
              </li>
              <li>이 작업은 할인되지 않기 때문에, 각 상태의 진정한 가치는 해당 상태에서 시작하는 경우 오른쪽에서 종료할 확률이다.
                <ul>
                  <li>따라서 중심 상태의 참 값은 $v_\pi (C) = 0.5$ 이다.</li>
                  <li>A 부터 E 까지 모든 상태의 참 값은 [$\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}, \frac{5}{6}$] 이다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>TD(0) 와 constant-$\alpha$ MC 의 결과값</p>

            <p><img src="/assets/images/posts/6_2_2_example_6_2_random_walk_result.png" alt="6_2_2_example_6_2_random_walk_result" /></p>

            <ul>
              <li>왼쪽 그래프는 TD(0) 의 단일 실행에서 다양한 에피소드를 수 차례 학습한 값을 보여준다.
                <ul>
                  <li>100회의 에피소드 이후 추정치는 실제 값에 거의 근접한다.</li>
                  <li>일정 크기 이상의 매개변수 (이 예에서는 $\alpha = 0.1$) 을 사용하면 가장 최근 에피소드의 결과에 따라 무한정 변동한다.</li>
                </ul>
              </li>
              <li>우측 그래프는 다양한 알파 값에 대한 두 가지 방법의 학습 곡선을 보여준다.
                <ul>
                  <li>표시된 성능 측정은 학습된 가치함수와 실 가치함수 사이의 평균 제곱 오류 (RMS) 로, 5개의 상태에서 평균을 낸 다음 100회 실행에 대한 평균값이다.</li>
                  <li>모든 경우 근사 가치함수는 모든 상태 $s$ 에 대한 중간값 $V(s) = 0.5$ 로 초기화하였다.</li>
                </ul>
              </li>
              <li>위 작업에서는 TD 방식이 MC 방식보다 일관되게 우수하였다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimality of TD(0)
    <ul>
      <li>제약된 에피소드에서의 배치 업데이트 가정
        <ul>
          <li>10개의 에피소드 또는 100개의 타임 스텝 과 같이 제한된 양의 경험만 사용할 수 있다고 가정
            <ul>
              <li>이 경우 증분 학습의 일반적 접근 방식은 답이 도달할 때까지 경험을 반복적으로 제시하는 것이다.</li>
              <li>근사 함수 $V$ 가 주어지면, (6.1) 또는 (6.2) 로 지정된 증분은 종료상태에 도달하기 전까지, 매 타임스텝 $t$ 마다 계산되어진다.</li>
              <li>하지만 가치함수는 모든 증분을 합계하여, 단 한번 바뀌게 된다.</li>
              <li>그런 다음 사용 가능한 모든 경험은 새로운 가치 함수로 다시 처리되어, 가치 함수가 수렴하게 될 때까지 새로운 전체 증분을 생성한다.</li>
              <li>학습 데이터의 전체 배치를 처리한 후에만 업데이트되기 때문에, 이 과정을 배치 업데이트라고 한다.</li>
            </ul>
          </li>
          <li>배치 업데이트에서 TD(0) 와 constant-$\alpha$ MC 의 차이점
            <ul>
              <li>배치 업데이트에서 TD(0) 은 $\alpha$ 가 충분히 작게 선택되는 한 step-size 파라미터와 독립적으로, 단일 답변으로 수렴하게 된다</li>
              <li>Constant-$\alpha$ MC 방법도 동일한 조건에서 결정론적으로 수렴하지만, 다른 답으로 수렴하게 된다.</li>
              <li>정상적인 업데이트에서 각각의 방법은 배치의 답으로 완전히 이동하지 않지만, 이러한 방향으로 조치를 취하게 된다.</li>
              <li>일반적으로 두 가지 답변을 이해하기 전, 몇가지 예를 살펴보자.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>예제 6.3 : Random walk under batch updating
        <ul>
          <li>아래는 TD(0) 및 constant-$\alpha$ MC 의 배치 업데이트 버전의 Random walk 예제 (예제 6.2) 의 적용 결과이다.</li>
          <li>새로운 에피소드마다, 지금까지 본 모든 에피소드를 배치로 취급하였다.</li>
          <li>이들은 알고리즘에 반복적으로 제시되었고, $\alpha$ 가 충분히 작아서 가치함수가 수렴하였다.</li>
          <li>결과적으로 가치함수는 $v_\pi$ 와 비교되었으며, 다섯 개의 상태를 기준으로 한 평균 제곱근 오차를 계산하여 학습곡선을 얻기 위해 100번의 독립적인 실험을 반복적으로 수행하였다. ( 이 결과는 6.2 그림에 나타나 있음.)</li>
        </ul>

        <p><img src="/assets/images/posts/6_3_1_example_6_2_random_walk_batch_result.png" alt="6_3_1_example_6_2_random_walk_batch_result" /></p>

        <ul>
          <li>배치 TD 학습법이 몬테카를로 방식보다 일관되게 우수한 결과를 보여주었다.</li>
          <li>배치 훈련에서의 Constant-$\alpha$ MC 방식은 각 상태 $s$ 를 방문한 후 경험한 실제 반환값의 샘플 평균인 $V(s)$ 에 수렴한다.</li>
          <li>이는 훈련 세트의 실제 반환값과 평균 제곱 오차를 최소화하는 의미에서 최적의 추정치이다.</li>
          <li>이런 의미에서 배치 TD 방식이 그림에 표기된 평균 제곱근 오차 측정에 따라 더 잘 수행할 수 있었다는 것으 놀라운 일이다.</li>
          <li>답은 몬테카를로 방법이 한정적인 방식으로만 최적이고, TD 가 반환값 예측과 더 관련성 있는 방식으로 최적이기 때문.</li>
        </ul>
      </li>
      <li>예제 6.4 : You are the Predictor
        <ul>
          <li>알려지지 않은 Markov reward process 에 대한 반환값을 예측해보자.</li>
          <li>다음 8개의 에피소드가 관찰되었다고 가정한다.
            <ul>
              <li>(A,0,B,0) (B,1) (B,1) (B,1) (B,1) (B,1) (B,1) (B,0)</li>
              <li>이것은 첫 번째 에피소드가 상태 A 에서 시작하여 보상이 0인 B로 전환된 다음 보상이 0인 B 에서 종료됨을 의미한다.</li>
              <li>이 데이터 배치가 주어지면 최적의 예측, 추정치 $V(A)$ 및 $V(B)$ 에 대한 값을 무엇이라고 말하겠는가?</li>
              <li>$V(B)$ 의 경우 최적값이 $\frac{3}{4}$ 라는 데 동의할 것이다.
                <ul>
                  <li>B 상태에서 8번중 6번의 에피소드가 1의 반환과 함께 즉시 종료되었고, 나머지 2번은 0인 상태에서 즉시 종료되었기 때문.</li>
                </ul>
              </li>
              <li>그렇다면 추정값 $V(A)$ 는 무엇일까?</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/6_3_2_example_6_3_you_are_the_predictor.png" alt="6_3_2_example_6_3_you_are_the_predictor" /></p>

        <ul>
          <li>여기에는 두 가지 합리적인 답변이 있다.
            <ul>
              <li>에피소드가 상태 A에 있었던 시간의 100% 가 즉시 B로 이동했음을 관찰하는 것 (보상 0)
                <ul>
                  <li>B가 $\frac{3}{4}$ 의 값을 갖는다고 이미 결정했기 때문에 A도 $\frac{3}{4}$ 의 값을 가져야 한다.</li>
                  <li>위의 그림에서 표시된 것처럼 Markov 프로세스를 모델링 한 다음 다음 모델이 주어진 올바른 추정치를 계산하는 것을 기반으로 함.</li>
                  <li>위의 경우 실제로 $V(A) = \frac{3}{4}$ 이다.</li>
                  <li>이것은 배치 TD(0) 가 제공하는 답이기도 하다.</li>
                </ul>
              </li>
              <li>단순히 우리가 A를 한번 보았고 그에 따른 반환값이 0이라는 것을 관찰하는 것
                <ul>
                  <li>따라서 $V(A) = 0$ 으로 추정한다.</li>
                  <li>이것은 배치 몬테카를로 방법이 제공하는 답임.</li>
                  <li>학습 데이터에 대한 최소 제곱 오차를 제공하는 답이기도 함.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>실제 데이터 상 두번째 답이 오류가 없으나, 우리는 여전히 첫번째 답변이 더 나을 것으로 기대한다.</li>
          <li>프로세스가 Markov 인 경우 몬테카를로 답변이 기존 데이터에서 더 우수하더라도 첫 번째 답변이 향후 데이터에서 더 낮은 오류를 생성할 것으로 예상한다.</li>
        </ul>
      </li>
      <li>TD 방식과 몬테카를로 방식 간 비교
        <ul>
          <li>예제 6.4는 배치 TD(0) 와 배치 몬테카를로 방법으로 찾은 추정치 간의 일반적인 차이를 보여준다.
            <ul>
              <li>배치 몬테카를로 방법은 항상 훈련 세트에서 평균 제곱 오차를 최소화하는 추정치를 찾는다.</li>
              <li>배치 TD(0) 는 항상 Markov 프로세스의 최대 가능도 모델에 대한 정확한 추정치를 찾는다.
                <ul>
                  <li>최대 가능도 (maximum-likelihood)
                    <ul>
                      <li>통계학에서 확률 분포의 모수(파라미터) 값을 추정하는 방법 중 하나</li>
                      <li>주어진 데이터에 가장 적합한 모델 파라미터 값을 찾는 통계적 추정 방법 중 하나</li>
                    </ul>
                  </li>
                  <li>일반적으로, 최대 가능도 추정은 데이터를 생성하는 확률이 가장 높은 파라미터 값을 의미한다.</li>
                  <li>이 경우, 최대 가능도 추정은 관측된 에피소드로부터 자명한 방법으로 형성된 마코프 프로세스의 모델이다.</li>
                  <li>여기서 i에서 j로의 전이 확률은 i에서 j로 간 관측된 전이의 비율로 추정되며, 관련된 기대 보상은 해당 전이에서 관측된 보상들의 평균이다.</li>
                  <li>위 모델이 정확하다면 정확한 가치 함수의 추정치를 계산할 수 있다.
                    <ul>
                      <li>이는 프로세스의 추정치가 근사화되지 않고 확실하게 알려져있다고 가정하는 것과 동일하기 때문에 확실성 등가 추정치(certainty-equivalence estimate)라고 한다.</li>
                    </ul>
                  </li>
                  <li>일반적으로 배치 TD(0) 는 확실성 등가 추정치 (certainty-equivalence estiamte)로 수렴된다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>TD 방법이 몬테카를로 방법보다 더 빨리 수렴되는 이유를 설명
            <ul>
              <li>배치 형식에서 TD(0) 은 진정한 확실성 등가 추정치를 계산하기 때문에 몬테카를로 방법보다 빠르다.
                <ul>
                  <li>이는 랜덤 워크 작업에 대한 배치 결과에 표시된 TD(0) 의 이점을 설명함 (그림 6.2)</li>
                  <li>비배치 TD(0) 에서도 더 나은 추정치를 향해 움직이고 있기 떄문에 constant-$\alpha$ MC 보다 더 빠를 수 있음.</li>
                  <li>현재 온라인 TD 및 몬테카를로 방법의 상대적 효율성에 대해 더 명확하게 말할 수 있는 것은 없다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>확실성 등가 추정(certainty-equivalence estimate)에 대해
            <ul>
              <li>어떤 의미에서 확실성 등가성 추정은 최적의 솔루션이지만 이를 직접 계산하는 것은 거의 불가능하다.</li>
              <li>만약 $n=| \mathbb{S} |$ 의 상태의 수에서 프로세스 최대 가능도 추정을 형성하는 것만으로도 $n^2$ 의 메모리가 필요할 수 있으며, 해당 가치함수를 계산하려면 기존의 방식대로 수행하는 경우 $n^3$ 의 계산단계가 필요함.</li>
              <li>이러한 상황에서 TD 방법이 $n$ 차 이하의 메모리와 트레이닝 세트에 대한 반복 계산을 사용하여 동일한 솔루션을 근사화할 수 있다는 점은 놀라운 점임.</li>
              <li>상태공간이 큰 작업에서 TD 방법은 확실성 등가 솔루션을 근사화하는 유일한 실행 가능 방법일 수 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="introduction-to-temporal-difference-learning">Introduction to Temporal Difference Learning</h2>

<ul>
  <li>
    <p>What is Temporal Difference (TD) learning?</p>

    <ul>
      <li>학습목표
        <ul>
          <li>temporal-difference learning 정의하기</li>
          <li>temporal-difference error 정의하기</li>
          <li>TD(0) 알고리즘 이해하기</li>
        </ul>
      </li>
      <li>Review : Estimating Values from Returns
        <ul>
          <li>예측 문제에서 우리의 목표는 주어진 상태에서 반환값을 유추하는 가치함수를 배우는 것이다.</li>
          <li>$v_\pi (s) \doteq E_\pi [ G_t | S_t = s]$</li>
          <li>밴딧 문제와 동일하게, 몬테카를로 방식에서도 상수를 이용해 가치함수를 업데이트할 수 있으며, 이는 반환값 리스트를 저장할 필요가 없음을 의미한다.</li>
          <li>$V(S_t) \gets V(S_t) + \alpha [G_t - V(S_t)]$
            <ul>
              <li>몬테카를로 방식에서 리턴값 $G_t$ 를 구하기 위해서 우리는 전체 궤적에 대한 샘플이 필요하다.</li>
              <li>이것은 우리가 에피소드 내에서 학습을 할 수 없음을 의미한다.</li>
              <li>하지만 우리는 에피소드가 끝나기 전에 증분의 방식으로 학습하기를 원하고, 이는 새로운 업데이트 목표가 필요함을 의미한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Bootstrapping
        <ul>
          <li>
            <p>value function</p>

            <p><img src="/assets/images/posts/value_function_bootstrapping.png" alt="value_function_bootstrapping" /></p>
          </li>
          <li>
            <p>Temporal Difference</p>

            <p><img src="/assets/images/posts/temporal_difference_bootstrapping.png" alt="temporal_difference_bootstrapping" /></p>

            <ul>
              <li>위의 표기된 부분은 TD error 값임</li>
            </ul>
          </li>
          <li>
            <p>DP 와의 차이점</p>

            <p><img src="/assets/images/posts/difference_between_DP_and_TD.png" alt="difference_between_DP_and_TD" /></p>

            <ul>
              <li>DP 의 경우 환경역학을 알고있어, 다음 단계의 총 합을 구해 값을 업데이트 하는 반면, TD의 경우 환경과 상호작용한 다음단계의 결과값만을 이용해 TD error 의 일부분만큼 업데이트 한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>1-Step TD</p>

        <ul>
          <li>
            <p>1-Step TD</p>

            <p><img src="/assets/images/posts/1_step_TD.png" alt="1_step_TD" /></p>
          </li>
          <li>
            <p>TD(0) psuedo code</p>

            <p><img src="/assets/images/posts/TD_0_psuedo_code.png" alt="TD_0_psuedo_code" /></p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Rich Sutton : The Importance of TD Learning</p>
    <ul>
      <li>Temporal Difference Learning : 예측 학습 (Prediction learning) 에 특화되어 있음.
        <ul>
          <li>Prediction learning : 기다림으로서 목표를 확보. 즉, 별도의 라벨링이 필요없는 unsupervised supervised learning 이다.</li>
        </ul>
      </li>
      <li>TD는 추측으로부터의 추측을 통해 배운다. TD error 는 두 예측 사이의 차이 값이다.
        <ul>
          <li>위의 컨셉이 없으면 TD 학습은 supervised learning, backpropagating the error 와 동일하다.</li>
        </ul>
      </li>
      <li>multi-step predictions
        <ul>
          <li>multi-step 을 큰 one-step 으로 생각하고 one-step 메서드를 사용할 수 있지 않을까?</li>
          <li>one-step prediction 을 배우고, 그것을 반복하여 multi-step prediction을 생산할 수 있지 않을까?</li>
          <li>둘 다 불가하며, 그렇게 되길 원치 않는다.</li>
        </ul>
      </li>
      <li>one-step trap
        <ul>
          <li>long-term prediction 을 시뮬레이션을 통해 만들 수 있다.
            <ul>
              <li>이론에서는 가능하지만 실제로는 불가함.
                <ul>
                  <li>long-term prediction 을 시뮬레이션으로 만드는 것은 지수적으로 복잡하다.</li>
                  <li>one-step prediction 에서의 작은 에러 또한 증폭된다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>이 함정에 빠지는 경우는 매우 흔하다.
            <ul>
              <li>POMDPs, Bayesians, control theory, compression enthusiasts.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>유명한 one-step supervised learning 방식을 이용할 수 있을까?
        <ul>
          <li>목표를 추측값이 아닌 관측된 결과로 확정한 뒤 one-step 메서드를 사용? (게임이 끝날때 까지 기다린 뒤 결과값 회귀)
            <ul>
              <li>엄청난 컴퓨팅 자원이 필요함.</li>
              <li>목표 값을 모르는 경우도 존재 (off-policy)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>TD 의 중요성
        <ul>
          <li>보편적이며, 중요한 학습</li>
          <li>예측을 학습하는 것이며, 확장 가능한 유일한 학습 형태일 수도 있음.</li>
          <li>일반적이고 다단계의 예측에 특화된 학습이며, 인식, 의미부여 및 세계 모델링에 중요한 개념일 수 있음.</li>
          <li>상태 속성을 활용하여 빠르고, 데이터 효율적으로 학습함.</li>
          <li>점진적으로 편향되는 특성이 있음.</li>
          <li>계산적으로 적합하며, 보상 이외의 다른 목적으로 활용을 시작함.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="advantages-of-td">Advantages of TD</h2>

<ul>
  <li>지난 내용 복습 (About temporal difference learning)
    <ul>
      <li>TD (Temporal Difference learning) 는 Dynamic Programming 과 Monte Carlo 의 핵심 아이디어를 채용한 방식이다.
        <ul>
          <li>DP : Bootstrapping</li>
          <li>MC : learn directly from experience</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>The advantages of temporal difference learning</p>

    <ul>
      <li>학습목표
        <ul>
          <li>TD 방식으로 실시간 학습의 장점 이해하기</li>
          <li>DP 와 몬테카를로 방식과 관련하여 TD 방식이 가지는 핵심 이점 식별하기</li>
        </ul>
      </li>
      <li>예제 : Driving Home
        <ul>
          <li>문제의 정의
            <ul>
              <li>매일, 당신은 집에 오기까지 얼마나 걸릴지를 예측한다.</li>
              <li>시간, 요일, 날씨, 그 외의 요인 등을 관측한다.</li>
              <li>이미 예전부터 많은 예측을 해왔었다.</li>
            </ul>
          </li>
          <li>해석
  <img src="/assets/images/posts/td_example_driving_home_01.png" alt="td_example_driving_home_01" />
            <ul>
              <li>원 안의 값은 남은 운전시간에 대한 예측치이다.</li>
              <li>원가 원 사이의 값은 보상 값으로, 다음 단계까지의 실제 걸린 시간을 의미한다.</li>
              <li>하단의 시간은 실제 걸린 누적 시간을 의미한다.</li>
            </ul>

            <p><img src="/assets/images/posts/td_example_driving_home_02.png" alt="td_example_driving_home_02" /></p>
            <ul>
              <li>몬테카를로 (Constant-$\alpha$ MC) 의 경우
                <ul>
                  <li>$G_t$ 의 값은 에피소드가 끝날 때 (집에 도착했을 때) 알수 있다.</li>
                  <li>위의 표기된 $G_1$ 의 값은 38이며, 업데이트 식에 의해 35의 값은 38의 값으로 업데이트 되어야 한다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/td_example_driving_home_03.png" alt="td_example_driving_home_03" /></p>
            <ul>
              <li>TD 의 경우
                <ul>
                  <li>에피소드가 끝날 때까지 기다리는 것이 아닌, 다음 스텝까지만 진행하면 학습을 할 수 있음. (TD(0) 의 경우)</li>
                  <li>위에 표기된 목표값은 할인($\gamma = 1$)된 다음 스텝의 예측값(35)과 보상(5)의 합인 40이 되며, TD error 는 10이 된다.</li>
                  <li>상수 값($\alpha = 1$)을 적용한 업데이트가 이루어져, 30은 40으로 업데이트 된다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>TD 의 이점
            <ul>
              <li>DP 와 달리 환경 모델을 필요로 하지 않는다. (경험으로부터 배움)</li>
              <li>MC 와 달리 TD 는 매 스텝마다 학습한다. (부트스트래핑 사용)</li>
              <li>TD 는 점근적으로 올바른 예측에 수렴한다.</li>
              <li>TD 는 보편적으로 MC 보다 더 빠른 수렴이 가능하다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Comparing TD and Monte Carlo</p>

    <ul>
      <li>학습목표
        <ul>
          <li>TD 학습의 경험적 이점을 식별하기</li>
        </ul>
      </li>
      <li>
        <p>예제 : Random Walk</p>

        <p><img src="/assets/images/posts/td_monte_carlo_compare_random_walk_01.png" alt="td_monte_carlo_compare_random_walk_01" /></p>

        <ul>
          <li>A, B, C, D, E 의 Nonterminal 상태</li>
          <li>left, right 의 deterministic actions</li>
          <li>정책 : uniform random policy</li>
          <li>모든 에피소드는 C 에서 초기화 (시작)</li>
          <li>극좌, 극우의 상태에서 에피소드는 종료됨</li>
          <li>극우의 상태에서 보상 1, 그 외에는 0</li>
          <li>위의 설정 상, 결국 가치함수의 값은 해당 상태에서 우측으로 진행되어 종료될 확률과 같다.</li>
        </ul>

        <p><img src="/assets/images/posts/td_monte_carlo_compare_random_walk_02.png" alt="td_monte_carlo_compare_random_walk_02" /></p>

        <ul>
          <li>C, D, E 의 상태를 거쳐 우측에서 종료했을 경우
            <ul>
              <li>TD 에이전트의 경우 E 의 상태만 업데이트됨 (업데이트 식에서 그 이유를 알 수 있음)</li>
              <li>몬테카를로 에이전트의 경우 C, D, E 가 다 같이 업데이트됨.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/td_monte_carlo_compare_random_walk_03.png" alt="td_monte_carlo_compare_random_walk_03" /></p>

        <ul>
          <li>위 에피소드를 진행 후 두 번째 에피소드를 진행
            <ul>
              <li>TD 의 경우 매 스텝마다 값이 업데이트되나, 몬테카를로의 경우 Terminate 상태가 되지 않는 한 값이 업데이트되지 않는다.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/td_monte_carlo_compare_random_walk_04.png" alt="td_monte_carlo_compare_random_walk_04" /></p>

        <ul>
          <li>위의 그래프는 TD 학습의 에피소드 학습 횟수별로 실제 값에 근사하는 모습을 보여줌 (constant-$\alpha$ = 0.1)
            <ul>
              <li>보다 작은 learning rate 나 decaying learning rate 를 사용하면 더 좋은 결과값을 얻을 수 있음.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/td_monte_carlo_compare_random_walk_05.png" alt="td_monte_carlo_compare_random_walk_05" /></p>

        <ul>
          <li>위의 그래프는 TD 학습과 몬테카를로 학습 간의 결과치를 비교한 것임.
            <ul>
              <li>TD 가 몬테카를로 보다 전반적으로 좋은 결과를 보여줌
                <ul>
                  <li>learning rate 가 크면, 학습 초반에 더 빠른 결과를 보여주나, learning rate 가 작은 학습이 최종적으로 에러율이 적게 나타남.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Temporal Difference Learning" /><category term="TD Learning" /><summary type="html"><![CDATA[관련 자료 (RLbook Pages 119-128)]]></summary></entry><entry><title type="html">Sample-based Learning Methods - 01. Week 1. Monte Carlo Methods for Prediction &amp;amp; Control</title><link href="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1/" rel="alternate" type="text/html" title="Sample-based Learning Methods - 01. Week 1. Monte Carlo Methods for Prediction &amp;amp; Control" /><published>2023-06-12T15:00:00+09:00</published><updated>2023-06-12T15:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1/"><![CDATA[<h2 id="관련-자료-rlbook2018-pages-91-104">관련 자료 (RLbook2018 Pages 91-104)</h2>

<ul>
  <li>Monte Carlo Methods
    <ul>
      <li>개요
        <ul>
          <li>이번 챕터에서는 환경에 대한 완전한 지식을 가정하지 않는다.</li>
          <li>몬테 카를로 방식은 경험만을 필요로 한다.
            <ul>
              <li>환경과의 실제 혹은 가상의 상호작용을 통한 샘플 시퀀스 (상태, 행동, 보상)</li>
            </ul>
          </li>
          <li>실제 경험을 통해 배우는 것은 엄청난 이점인데, 환경 역학에 대한 선행 지식 없이 최적 행동을 얻을 수 있기 때문이다.</li>
          <li>가상 경험을 통해 배우는 것도 마찬가지이다.
            <ul>
              <li>비록 모델이 필요하지만, 모델은 오직 샘플 전환만을 생성하면 되고, DP 처럼 모든 가능한 전환의 완전한 확률 분포가 필요한 것은 아니다.</li>
            </ul>
          </li>
          <li>많은 경우 원하는 확률 분포에 따라 샘플링된 경험을 생성하는 것은 쉽지만, 명시적인 형태로 분포를 얻는 것은 불가능하다.</li>
        </ul>
      </li>
      <li>Monte Carlo Methods
        <ul>
          <li>몬테카를로 방식은 강화학습 문제를 샘플링된 리턴 값의 평균 기반으로 해결하는 방식이다.
            <ul>
              <li>잘 정의된 리턴 값이 가능하도록, 우리는 몬테카를로 방식을 episodic tasks 에만 정의한다.
                <ul>
                  <li>각 경험은 에피소드 단위로 나눈다.</li>
                  <li>모든 에피소드는 어떠한 행동을 선택하더라도 필연적으로 종료된다.</li>
                  <li>에피소드 완료 시에만 가치 추정 및 정책이 변경된다.</li>
                </ul>
              </li>
              <li>따라서 몬테카를로 방식은 에피소드 별로 증분이 된다.
                <ul>
                  <li>step-by-step (online) 방식은 아니다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>몬테카를로 라는 용어는 작업에 상당한 무작위 요소가 포함된 추정 방법에 광범위하게 사용되는 경우가 많다.</li>
          <li>여기에서 우리는 전체 수익의 평균을 기반으로 하는 방법에 대하여 몬테카를로 방식이라 한다.
            <ul>
              <li>추후 학습할 부분 수익으로 학습하는 방법과 반대</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>몬테카를로 방식과 Bandit 방식
        <ul>
          <li>몬테카를로 방식은 강화학습에서 상태-행동 쌍마다 샘플링하고 평균 반환값을 계산한다.
            <ul>
              <li>이는 2장에서 다룬 밴딧 방법과 유사하게 각 행동에 대한 샘플링과 평균 보상을 계산하는 것임.</li>
            </ul>
          </li>
          <li>차이점은 몬테카를로 방식에서는 여러 상태가 있고, 각각이 다른 밴딧 문제와 유사하게 작동하며, 각 문제들은 상호 관련되어 있다는 점이다.
            <ul>
              <li>즉, 하나의 상태에서 행동을 수행한 후, 이후 상태에서 수행한 행동에 따라 반환값이 영향을 받음.</li>
              <li>모든 행동선택이 학습을 진행하고 있는 중이기에 이전 상태의 관점에선 비정상성 (nonstationary) 의 변화하는 문제가 됨.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>몬테카를로 방식과 GPI (Generalized Policy Iteration)
        <ul>
          <li>비정상성 (nonstationary) 를 해결하기 위해, GPI 의 아이디어를 적용한다.
            <ul>
              <li>기존 GPI 에선 MDP 의 사전지식을 통해 가치함수를 계산했다면, 여기서는 MDP 에서 주는 샘플 리턴값을 통해 가치함수를 학습한다.</li>
              <li>가치함수와 상응하는 정책은 여전히 최적성을 얻기 위해 본질적으로 같은 방법 (GPI) 으로 상호작용한다.</li>
              <li>GPI 에서 했던 동일한 과정으로, 몬테카를로 케이스의 경우 (경험만 샘플링 가능한 상황) 에도 적용한다.
                <ul>
                  <li>Prediction Problem</li>
                  <li>Policy Improvement</li>
                  <li>Control Problem</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Monte Carlo Prediction
    <ul>
      <li>개요
        <ul>
          <li>우리는 시작으로, 몬테카를로 방식을 이용한 주어진 정책의 상태가치함수의 학습을 고려해 볼 것이다.</li>
          <li>상태의 값은 예상되는 리턴 값 (미래의 할인된 기대 보상 합) 임을 상기한다.
            <ul>
              <li>단순히 해당 상태에 방문하고, 리턴값을 관찰하는 것이 경험을 통해 측정하는 명백한 방법이다.</li>
              <li>더 많은 리턴 값이 관측되면, 평균은 반드시 예측 값에 수렴한다.</li>
              <li>위의 아이디어가 몬테카를로 방식의 기반이 된다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>몬테카를로 예측
        <ul>
          <li>우리가 특별히 $v_\pi (s)$ 를 예측, 즉 정책 $\pi$ 아래의 s 상태의 값을 $\pi$ 정책을 따르고, s 상태를 지나는 에피소드 셋을 통해 예측한다고 가정</li>
          <li>에피소드 내에서 각각의 상태 s 에 발생하는 것을 s 에 방문한다 (visit to s) 라 한다.
            <ul>
              <li>s 는 같은 에피소드 내에서도 여러 번 방문될 수 있다.</li>
              <li>첫 방문을 first visit to s 라 칭하도록 한다.</li>
            </ul>
          </li>
          <li>몬테카를로 방식 (MC method) 을 나누는 기준
            <ul>
              <li>every-visit MC method
                <ul>
                  <li>s 를 방문할 때마다의 리턴 값의 평균을 구한다.</li>
                  <li>함수 근사 (function approximation) 와 적격성 추적 (eligibility traces) 확장에 더 자연스럽다.</li>
                  <li>나중에 (Chapter 9, 12) 다룰 방식</li>
                  <li>아래 first-visit 방식에서 $S_t$ 가 에피소드 초반에 일어났는지 체크하는 부분을 제외하고 동일하다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/5_1_1_first_visit_mc_prediction.png" alt="5_1_1_first_visit_mc_prediction" /></p>

            <ul>
              <li>first-visit MC method
                <ul>
                  <li>s 의 첫 방문의 리턴 값의 평균을 구한다.</li>
                  <li>1940년대 부터 가장 많이 연구 되는 방식</li>
                  <li>이 장에서 다룰 방식</li>
                  <li>Termination 상태부터 할인된 보상합을 계산하되, 이 값이 $S_t$ 가 되면 이전 에피소드 보상합에 포함시켜 평균을 구한다.</li>
                </ul>
              </li>
              <li>first-visit MC 와 every-visit MC 모두 $s$ 상태에 대한 방문 (혹은 첫 방문) 의 수가 무한대에 가까워질 경우 $v_\pi (s)$ 로 수렴한다.
                <ul>
                  <li>first-visit MC 의 경우</li>
                  <li>각 리턴 값은 유한한 분산을 가진 $v_\pi (s)$ 의 독립적이고 동일하게 분포된 추정치이다.</li>
                  <li>대수의 법칙에 따라 이러한 추정치의 평균 시퀀스는 기대값으로 수렴한다.</li>
                  <li>각 평균은 그 자체로 편향되지 않은 추정치이며, 그 오차의 표준 편차는 $\frac{1}{\sqrt{n}}$ 으로 감소한다.
                    <ul>
                      <li>n 은 평균을 구한 반환값의 개수이다.</li>
                    </ul>
                  </li>
                  <li>every-visit MC 는 덜 직관적이지만, 그 추정치 또한 $v_\pi (s)$ 로 2차 수렴한다. (Singh and Sutton, 1996)</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Example 5.1 : Blackjack
        <ul>
          <li>게임의 설명
            <ul>
              <li>블랙잭이라는 대중적인 카지노 카드 게임의 목적은 수치의 합이 21을 초과하지 않고 가능한 한 큰 카드를 얻는 것</li>
              <li>모든 FACE 카드는 10으로 계산되며 에이스는 1 또는 11로 계산될 수 있습니다.</li>
              <li>각 플레이어가 딜러와 독립적으로 경쟁하는 버전을 고려함.</li>
              <li>게임은 딜러와 플레이어 모두에게 두 장의 카드를 나눠주는 것으로 시작됨.</li>
              <li>딜러의 카드 중 하나는 위를 향하고 다른 하나는 아래를 향함.</li>
              <li>플레이어가 즉시 21개(에이스와 10장의 카드)를 가지고 있으면 이를 내추럴이라고 함.</li>
              <li>딜러도 내추럴을 가지고 있지 않는 한 플레이어가 이기며, 가지고 있을 경우 게임은 무승부가 됨.</li>
              <li>플레이어가 내츄럴 카드를 가지고 있지 않다면, 21을 초과 (bust) 하지 않는 선에서, 추가 카드 한장을 요청 (hits) 하거나 멈출 (sticks) 수 있음.</li>
              <li>21을 초과 (bust) 하면 패배</li>
              <li>멈추면 (sticks) 딜러의 턴이 됨.</li>
              <li>딜러는 선택의 여지 없이 고정된 전략에 따라 히트하거나 스틱함. 그는 17 이상의 합계이면 스틱하고, 그렇지 않으면 히트함.</li>
              <li>딜러가 21을 초과 (bust) 하면 플레이어가 승리함.</li>
              <li>그렇지 않은 경우 결과 (승리, 무승부) 는 최종 합이 21에 가까운 사람이 승리함.</li>
            </ul>
          </li>
          <li>MDP 정의
            <ul>
              <li>블랙잭을 플레이하는 것은 자연스럽게 일시적인 유한 MDP로 공식화됨.</li>
              <li>각 게임은 에피소드</li>
              <li>보상은 승리, 패배, 무승부에 따라 각각 +1, -1 또는 0 이 주어짐.
                <ul>
                  <li>이 종료 보상은 수익이기도 함.</li>
                </ul>
              </li>
              <li>게임 내의 모든 보상은 0. 보상 할인은 사용하지 않음 ($\gamma = 1$)</li>
              <li>플레이어의 행동은 힛 또는 스틱임.</li>
              <li>상태는 플레이어의 카드와 딜러의 표시 카드임.</li>
              <li>이미 처리된 카드를 카운팅하는 이점이 없도록 무한한 덱 (교체 포함) 에서 카드가 처리된다고 가정함.</li>
              <li>플레이어가 버스트되지 않고 11로 셀 수 있는 에이스를 보유하고 있으면 그 에이스는 사용 가능하다고 한다.
                <ul>
                  <li>이 경우 1로 계산하면 합계가 11 이하가 되기 때문에 항상 11로 계산됨.</li>
                  <li>1로 계산하는 경우 플레이어는 항상 힛을 해야 하기 때문에 결정을 내릴 수 없음.</li>
                  <li>힛을 했을 때 버스트 되는 경우 에이스를 1로 계산할 수 있음.</li>
                </ul>
              </li>
              <li>따라서 플레이어는 자신의 현재 합계(12–21), 딜러가 보여주는 카드(ace–10), 사용 가능한 에이스 보유 여부의 세 가지 변수를 기반으로 결정을 내림.
                <ul>
                  <li>이렇게 하면 총 200개의 상태가 됨.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>정책의 설정
            <ul>
              <li>플레이어의 합계가 20 또는 21이면 스틱, 그렇지 않으면 힛을 하는 정책을 고려한다.</li>
              <li>몬테카를로 접근방식으로 이 정책에 대한 상태가치함수를 찾기 위해, 해당 정책으로 많은 블랙잭 게임을 시뮬레이션하고 각 상태에 따른 수익을 평균화함.</li>
              <li>사용 가능한 에이스가 있는 상태에 대한 추정치는 이러한 상태가 덜 일반적이기 때문에 덜 확실하고 덜 규칙적임.</li>
              <li>어쨌든 500,000 게임 후에 가치 함수는 매우 잘 근사됨.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/5_1_2_example_5_1_blackjack_1.png" alt="5_1_2_example_5_1_blackjack_1" /></p>
      </li>
      <li>DP 와 비교한 몬테카를로의 장점
        <ul>
          <li>위의 블랙잭 게임에서 우리는 환경에 대한 완벽한 지식을 갖고 있지만, DP 를 사용하여 가치함수를 계산하는 것은 쉽지 않은 문제이다.
            <ul>
              <li>DP 방식의 경우 다음 이벤트의 분포를 필요 (특히, 4개의 인수를 가지는 환경역학 $p(s’,r | s,a)$)</li>
              <li>블랙잭에 대해 이를 결정하는 것이 쉽지 않음.</li>
              <li>예를 들어 플레이어의 합이 14이고, 힛을 선택하는 경우 딜러의 공개 카드에 따른 보상이 +1 이 되는 확률</li>
              <li>DP를 적용하기 위해서는 모든 확률을 계산해야 하며, 이러한 계산은 종종 복잡하고 오류가 발생할 수 있음.</li>
            </ul>
          </li>
          <li>반면에, 몬테카를로 방식에서 필요한 샘플 게임을 생성하는 것은 쉽다.
            <ul>
              <li>샘플 에피소드만으로 작업하는 몬테카를로 방법이 환경 역학에 완전한 지식을 갖고 있는 경우에도 상당한 장점이 될 수 있다.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/5_1_3_mc_diagram_1.png" alt="5_1_3_mc_diagram_1" /></p>

        <ul>
          <li>몬테카를로에서는 백업 다이어그램의 개념을 사용할 수 없음.
            <ul>
              <li>백업 다이어그램 : 업데이트될 루트 노드를 상단에 표시하고, 업데이트에 기여하는 보상, 예측값을 가진 모든 전이, 리프 노드를 아래에 표기</li>
              <li>$v_\pi$ 의 몬테카를로 추정의 경우 루트는 상태노드이고, 그 아래에 최종 상태로 끝나는 특정 단일 에피소드의 전체 전이 궤적이 있다.</li>
              <li>DP 다이어그램은 가능한 모든 전이를 보여주는 반면, 몬테카를로 다이어그램은 하나의 에피소드에서 샘플링된 전이만을 보여줌.</li>
              <li>DP 다이어그램은 한 단계 전이만 포함하지만, 몬테카를로 다이어그램은 에피소드의 끝까지 이어짐.</li>
            </ul>
          </li>
          <li>몬테카를로 방식의 중요한 사실은 각 상태의 추정치가 서로 독립적이라는 것임.
            <ul>
              <li>현 상태의 추정치는 다른 상태에 의존하지 않음. (DP에서 정의한 부트스트랩 방식을 사용하지 않음.)</li>
            </ul>
          </li>
          <li>하나의 상태의 값을 추정하는 계산 비용은 상태의 수와 독립적임.
            <ul>
              <li>즉, 몬테카를로 방식은 하나 또는 일부의 상태의 값만 필요한 경우 특히 유리함.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Monte Carlo Estimation of Action Values</p>

    <ul>
      <li>모델이 없을 경우에서의 몬테카를로 방식
        <ul>
          <li>모델을 사용할 수 없는 경우, 상태 값이 아닌 행동 값 (상태-행동 쌍) 을 추정하는 것이 특히 유용함.
            <ul>
              <li>모델이 있는 경우, 상태 값만으로 정책을 결정하기 충분함.
                <ul>
                  <li>DP 에 대해 다룰 때, 단순히 한 단계 앞을 보고, 보상과 다음 상태의 최상의 조합으로 이어지는 행동을 선택하면 됐었음.</li>
                </ul>
              </li>
              <li>그러나 모델이 없을 경우, 상태값만으로 충분하지 않음.
                <ul>
                  <li>정책을 제안하는 데 유용하게 사용하기 위해 각 행동의 가치를 명시적으로 추정해야 함.</li>
                  <li>따라서 몬테카를로 방법의 주요 목표 중 하나는 $q_*$ 를 추정하는 것임.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>행동 값에 대한 정책 평가 문제를 고려하기
        <ul>
          <li>상태 $s$ 에서 시작하여 행동 $a$ 를 취한 후 정책 $\pi$ 를 따를 때 기대되는 보상인 $q_\pi (s,a)$ 를 추정하는 것.</li>
          <li>이를 위한 몬테카를로 방법은 위의 상태가치함수를 구하는 것과 본질적으로 동일하나 이제 상태가 아닌 상태-행동 쌍에 대한 방문에 대해 이야기함.
            <ul>
              <li>상태-행동 쌍, $s,a$ 는 상태 $s$ 에서 행동 $a$ 를 취할 때 에피소드 내에서 방문했다 (visited) 라고 표현함.</li>
              <li>every-visit MC 의 경우 모든 방문에 따른 리턴 값의 평균으로 상태-행동 쌍의 값을 추정함.</li>
              <li>first-visit MC 의 경우 각 에피소드 내에서 처음으로 상태 $s$ 에 방문하고 행동 $a$ 를 취한 리턴 값의 평균을 구함.</li>
              <li>이러한 방법은 방문 횟수가 무한대로 접근할 때 이전과 같이 2차적으로 수렴함. (변화가 점점 안정화 되는 패턴)</li>
            </ul>
          </li>
          <li>유일한 복잡성은 많은 상태-행동 쌍이 방문되지 않을 수도 있다는 점임.
            <ul>
              <li>$\pi$ 가 결정론적 정책인 경우, $\pi$ 를 따를 때, 각 상태에서는 행동 중 하나에 대한 리턴값만 관찰하게 될 것임.</li>
              <li>평균화할 리턴값이 없으므로, 다른 행동들의 몬테카를로 추정은 경험을 통해 개선되지 않을 것임.</li>
              <li>행동 값을 학습하는 목적이 각 상태에서 사용할 수 있는 행동 중에서의 선택을 돕는 것이기 때문에 이것은 심각한 문제임.</li>
              <li>행동의 대안을 비교하려면 현재 선호하는 것 뿐만 아니라 각 상태의 모든 행동의 가치를 추정해야 한다.</li>
              <li>이것은 2장의 k-armed bandit 문제의 맥락에서 논의된 탐색 유지 문제의 일반적인 문제이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>지속적인 탐색의 보장
        <ul>
          <li>행동 값에 대한 정책 팽가(policy evaluation) 를 위해서는 모든 상태-행동 쌍이 방문되는 것을 보장해야 함.</li>
          <li>이를 위한 한 가지 방법은 에피소드가 상태-행동 쌍에서 시작되도록 지정하고, 모든 쌍이 시작점으로 선택될 확률이 0이 아니게 하는 것이다.
            <ul>
              <li>이렇게 하면 모든 상태-행동 쌍이 에피소드의 무한한 횟수에 따라 방문되도록 보장됨.</li>
              <li>이를 탐색 시작 가정 (assumption of exploring starts) 이라 한다.</li>
            </ul>
          </li>
          <li>탐색 시작 가정은 때때로 유용하나, 일반적으로, 특히 환경과의 실제 상호 작용에서 직접 학습할 때에는 의존할 수 없게 된다.
            <ul>
              <li>이 경우, 시작 조건이 도움이 되지 않음.</li>
            </ul>
          </li>
          <li>모든 상태-행동 쌍이 발생하도록 보장하는 가장 일반적인 대안 접근 방식은 각 상태에서 모든 행동을 선택할 확률이 0이 아닌 확률로, 확률론적 정책만 고려하는 것이다.
            <ul>
              <li>이후 섹션에서 이 접근 방식의 두 가지 중요한 변형에 대해 설명할 것이나, 지금은 탐색 시작 가정을 유지한 전체 몬테카를로 제어 방법을 소개한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Monte Carlo Control
    <ul>
      <li>몬테카를로 제어(Control) 와 GPI (Generalized Policy Iteration)
        <ul>
          <li>몬테카를로 추정 (Monte Carlo estimation) 이 제어, 즉 최적의 정책에 근접하는 데 어떻게 사용될 수 있는지 고려해보자.</li>
          <li>전반적인 아이디어는 동적프로그래밍(DP) 와 동일한 패턴, 즉 일반화된 정책 반복(GPI:Generalized Policy Iteration)과 동일하다.</li>
          <li>
            <p>GPI 에서는 근사 정책과 근사 가치함수를 모두 유지한다.</p>

            <p><img src="/assets/images/posts/5_3_1_GPI_Diagram.png" alt="5_3_1_GPI_Diagram" /></p>

            <ul>
              <li>가치함수는 현재 정책의 가치함수에 더 근접하도록 반복적으로 변경되며, 정책은 현재 가치함수에 대해 반복적으로 개선됨.</li>
              <li>이 두 종류의 변화는 각각 서로에게 움직이는 목표를 생성하기 때문에, 어느 정도 서로에게 불리하게 작용하지만, 정책과 가치함수 모두 최적에 접근하게 됨.</li>
            </ul>

            <p><img src="/assets/images/posts/5_3_2_traditional_policy_iteration.png" alt="5_3_2_traditional_policy_iteration" /></p>

            <ul>
              <li>우선, 고전적인 정책 반복의 몬테카를로 버전을 살펴보면 임의의 정책 $\pi_0$ 에서 시작하여 최적 정책 및 최적 행동가치함수로 끝나는 정책 평가 및 정책 개선의 전체 단계를 번갈아 가며 수행한다.</li>
              <li>여기서 $\overset{E}{\rightarrow}$ 는 완전한 정책 평가를 나타내며, $\overset{I}{\rightarrow}$ 는 완전한 정책 개선을 나타낸다.</li>
              <li>정책 평가는 이전 섹션에서 설명한 대로 수행된다.</li>
              <li>실제 행동가치함수에 점근적으로 접근하기 위해, 많은 에피소드를 경험하게 된다.</li>
              <li>우리는 탐색 시작(exploring starts) 으로 생성되는 무한한 수의 에피소드를 관찰한다고 가정한다.
                <ul>
                  <li>이러한 가정 하에 몬테카를로 방식은 임의의 $\pi_k$ 에 대해 $q_{\pi_k}$ 를 정확하게 계산한다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>정책 개선은 현재 가치함수 기반으로 정책을 탐욕화 하여 이루어짐</li>
          <li>이 경우 우리는 행동가치함수를 가지고 있으므로, 탐욕 정책을 만드는데 환경 모델은 불필요하다.</li>
          <li>어떠한 행동가치함수 $q$ 에서도, 상응하는 탐욕 정책은 하나이다. (모든 $s \in S$ 에 대해 결정론적으로 최대값을 가지는 행동을 선택할 경우)</li>
        </ul>

        <p><img src="/assets/images/posts/5_3_3_greedy_policy_q_function.png" alt="5_3_3_greedy_policy_q_function" /></p>

        <ul>
          <li>정책 개선은 $q_{\pi_k}$ 에 대한 탐욕 정책 $\pi_{k+1}$ 을 설계함으로서 이루어진다.</li>
          <li>이 경우 policy improvement theorem 이 적용된다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_3_4_policy_improvement_theorem.png" alt="5_3_4_policy_improvement_theorem" /></p>

        <ul>
          <li>앞 장에서 이야기했던 것처럼 위 정리는 $\pi_{k+1}$ 이 $\pi_k$ 보다 균일하게 낫거나, 같음 (최적 정책일 경우) 을 보장한다.</li>
          <li>위의 내용을 기반으로 전체 프로세스가 최적 정책과 최적 가치함수로 수렴을 진행한다는 것을 확신할 수 있음.</li>
          <li>이러한 방식으로 몬테카를로 방식을 사용하여 환경 역학 지식 없이 샘플 에피소드만으로 최적의 정책을 찾을 수 있음.</li>
        </ul>
      </li>
      <li>몬테카를로 방식을 사용하기 위한 가정과 실제 해결책
        <ul>
          <li>몬테카를로 수렴 보장을 위한 현실적이지 않은 가정
            <ul>
              <li>에피소드가 탐색 시작 (exploring starts) 을 한다는 점</li>
              <li>무한히 많은 에피소드로 정책 평가를 할 수 있다는 점</li>
              <li>실용적인 알고리즘을 얻으려면 위 두 가지 가정을 모두 제거해야 함.</li>
              <li>이 장의 뒷부분 까지 우리는 첫 번째 가정에 대한 고려를 연기한다.</li>
              <li>지금은 정책 평가가 무한한 수의 에피소드에서 작동한다는 가정에 초점을 맞춘다.</li>
            </ul>
          </li>
          <li>무한한 수의 에피소드 진행을 피하는 방법
            <ul>
              <li>사실 반복 정책 평가 (iterative policy evaluation) 와 같은 고전적인 DP 방법에서도 동일한 문제가 발생함. (이 역시 참 가치 함수에 점근적으로만 수렴한다.)</li>
              <li>DP 와 몬테카를로 모두, 문제를 해결하는 두 가지 방법이 있다.</li>
              <li>각 정책 평가에서 $q_{\pi_k}$ 를 근사화한다는 아이디어를 확고히 유지하고, 측정과 가정을 통해 추정치의 오차 크기와 확률에 대한 한계를 얻고, 그런 다음 각 정책 평가 중에 충분한 단계를 취하여 이러한 한계가 충분히 작음을 보장함.
                <ul>
                  <li>어느 정도의 근사 수준까지 올바른 수렴을 보장</li>
                  <li>그러나 가장 작은 문제들을 제외한 모든 실 문제에 유용하려면 너무 많은 에피소드가 필요할 수도 있음.</li>
                </ul>
              </li>
              <li>정책 개선으로 돌아가기 전에 정책 평가를 완료하려는 시도를 포기함.
                <ul>
                  <li>각 평가 단계에서 가치함수를 $q_{\pi_k}$ 쪽으로 이동시키지만, 많은 단계를 거쳐야만 가까워짐.</li>
                  <li>GPI 개념과 동일한 방식의 아이디어를 사용함.</li>
                  <li>예를들어 value iteration 과 같이 정책 개선 사이에 정책 평가를 한 번만 수행. (in-place 버전의 경우 몇몇의 상태만 정책평가를 한 후 정책 개선을 진행)</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>몬테카를로 ES (Monte Carlo with Exploring Starts)
        <ul>
          <li>몬테카를로 정책 반복의 경우 에피소드 별로 평가와 개선을 번갈아 가며 수행하는 것이 자연스러움.</li>
          <li>각 에피소드 후에 관찰된 리턴값은 정책 평가에 사용되며 에피소드에서 방문한 모든 상태에서 정책이 개선됨.</li>
          <li>몬테카를로 ES (Monte Carlo with Exploring Starts) 라고 부르는 간단한 알고리즘은 아래의 psuedo-code 와 같은 형태이다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_3_5_monte_carlo_es.png" alt="5_3_5_monte_carlo_es" /></p>

        <ul>
          <li>몬테카를로 ES 에서 각 상태-행동 쌍에 대한 모든 리턴값은 관찰 당시 시행중인 정책에 관계 없이 누적 되고 평균화됨.</li>
          <li>즉 몬테카를로 ES 는 차선 정책으로 수렴할 수 없음.
            <ul>
              <li>만약 그럴 경우 가치함수가 차선 정책으로 수렴하면서 정책이 변경됨.</li>
            </ul>
          </li>
          <li>안정성은 정책과 가치함수 모두 최적일 때만 달성됨.
            <ul>
              <li>행동가치함수의 변화가 시간이 지남에 따라 감소하면서, 최적의 고정점으로의 수렴은 불가피해 보이지만 아직 공식적으로 증명되지 않았음.
                <ul>
                  <li>이것은 강화 학습에서 가장 기본적인 미해결 이론적 질문 중 하나임.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Example 5.3 : Solving Blackjack
        <ul>
          <li>몬테카를로 ES 를 블랙잭에 적용하는 것은 간단함.</li>
          <li>에피소드는 모두 시뮬레이션 게임이기 때문에 모든 가능성을 포함하는 탐색 시작의 준비가 쉬움.
            <ul>
              <li>이 경우 딜러의 카드, 플레이어의 합계, 플레이어가 사용 가능한 에이스를 가지고 있는지 여부를 모두 동일 확률로 무작위 선택함.</li>
              <li>초기 정책으로 이전 블랙잭의 예시에서 평가된 정책을 사용하며, 20 또는 21에만 적용됨.</li>
              <li>초기 행동가치함수는 모든 상태-행동 쌍에 대해 0 로 설정.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/5_3_6_blackjack_with_monte_carlo_es.png" alt="5_3_6_blackjack_with_monte_carlo_es" /></p>

        <ul>
          <li>Figure 5.2 는 몬테카를로 ES 가 찾아낸 최적의 정책을 보여줌.</li>
          <li>이 정책은 사용 가능한 에이스에 대한 정책에서 가장 왼쪽 노치를 제외하고 Thorp(1966)의 “기본” 전략과 동일함.
            <ul>
              <li>이러한 불일치의 이유는 확실하지 않지만, 여기에 표기된 것이 실제로 우리가 설명한 블랙잭 버전의 최적 정책이라고 확신할 수 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Monte Carlo Control without Exploring Starts
    <ul>
      <li>On-policy 방식과 Off-policy 방식
        <ul>
          <li>탐색 시작 (Exploring starts) 이라는 희박한 가정을 피하기 위해, 모든 행동이 무한정으로 자주 선택되도록 에이전트가 해당 행동을 계속 선택하는 방식이 있다.</li>
          <li>이를 보장하는 두 가지 접근 방식이 있으며, 이를 On-policy 방식과 Off-policy 방식이라고 부른다.
            <ul>
              <li>On-policy 방식 : 학습 정책과 탐색 정책이 동일한 경우를 가리킴 (탐색 정책을 사용하여 데이터를 수집하고 학습(정책개선)을 진행)</li>
              <li>Off-policy 방식 : 학습 정책과 탐색 정책이 다른 경우 (학습 정책은 실제로 개선하고자 하는 정책, 탐색 정책은 환경 탐색, 경험을 얻기 위한 정책)</li>
            </ul>
          </li>
          <li>위의 몬테카를로 ES 방식은 On-policy 방식의 한 예이다.</li>
          <li>이 섹션에서는 On-policy 몬테카를로 제어 방법의 설계에 대해 다루며, Off-policy 정책은 다음 섹션에서 고려한다.</li>
        </ul>
      </li>
      <li>On-policy 제어 방식
        <ul>
          <li>일반적으로 on-policy 제어 방식은 soft 하다.
            <ul>
              <li>즉, $\pi (a | s) &gt; 0$ for all $s \in S$ and all $a \in A(s)$</li>
              <li>점차적으로 결정론적 최적 정책에 가까워진다.</li>
            </ul>
          </li>
          <li>이 장에서의 on-policy 방식은 $\varepsilon -greedy$ 정책을 사용한다.
            <ul>
              <li>대부분은 행동가치 추정값이 최대인 행동을 선택하나, $\varepsilon$ 의 확률로 임의의 행동을 선택한다.</li>
              <li>모든 탐욕적이지 않은 행동은 최소의 선택 확률 $\frac{\varepsilon}{| A(s) |}$ 을 가지며, 탐욕 행동은 $1-\varepsilon + \frac{\varepsilon}{| A(s) |}$ 의 확률을 가진다.</li>
              <li>(집합을 절대값 기호로 묶은 표기는 해당 집합의 원소 수를 나타낸다.)</li>
              <li>$\varepsilon - greedy$ 정책은 $\varepsilon - soft$ 정책의 한 예이다.
                <ul>
                  <li>모든 상태와 행동에 대해, $\pi (a | s) \ge \frac{\varepsilon}{ | A(s) |}$ 이며, $\varepsilon &gt; 0$</li>
                  <li>$\varepsilon - greedy$ 정책은 $\varepsilon - soft$ 정책 중에서도 탐욕 정책에 가장 가까운 정책이다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>On-policy 몬테카를로 제어의 전반적인 아이디어는 여전히 GPI 의 아이디어이다.</li>
          <li>몬테카를로 ES 와 마찬가지로 이번에도 first-visit MC 방식 으로 현 정책의 행동가치함수를 추정한다.
            <ul>
              <li>그러나 탐색 시작의 가정 없이 현재 가치함수를 탐욕정책으로 개선할 수 없다. (탐욕스럽지 않은 행동의 추가 탐색을 방지할 수 있음.)</li>
            </ul>
          </li>
          <li>GPI 는 정책이 탐욕정책으로 완전히 전환되는 것을 요구하지 않고, 탐욕정책의 바향으로 이동하기만 하면 됨.
            <ul>
              <li>On-policy 방식에서는 $\varepsilon - greedy$ 정책으로 이동할 것임.</li>
              <li>$q_\pi$ 에 대한 $\varepsilon - soft$ 정책, $\varepsilon - greedy$ 정책 은 $\pi$ 와 같거나 더 나음을 보장한다.</li>
            </ul>
          </li>
          <li>아래는 전체 알고리즘에 대한 내용이다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_4_1_on_policy_first_visit_mc_control.png" alt="5_4_1_on_policy_first_visit_mc_control" /></p>

        <ul>
          <li>$q_\pi$ 에 기반한 어떠한 $\varepsilon - greedy$ 정책도 $\varepsilon - soft$ 정책보다 개선된다는 점은 policy improvement theorem 에 의해 보장된다.</li>
          <li>$\pi’$ 를 $\varepsilon - greedy$ 정책이라 가정하면 아래의 수식이 성립된다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_4_2_e_greedy_e_soft_policy_improvement_theorem.png" alt="5_4_2_e_greedy_e_soft_policy_improvement_theorem" /></p>

        <ul>
          <li>policy improvement theorem 에 따라, $\pi’ \geq \pi$ (i.e., $v_{\pi’} (s) \geq v_\pi (s)$, for all $s \in S$).
            <ul>
              <li>이때 등식은 $\pi’$ 와 $\pi$ 모두 $\varepsilon - soft$ 정책 내에서 최적이며, 동시에 모든 타 $\varepsilon - soft$ 정책보다 좋거나 같을 때 성립한다.</li>
            </ul>
          </li>
          <li>$\varepsilon - soft$ 정책이 환경 내부로 이동한, 원래 환경과 동일한 새 환경을 고려한다.
            <ul>
              <li>새 환경은 원 환경과 동일한 행동, 동일한 상태 설정을 가진다.</li>
              <li>만약 상태 s 에서 행동 a 를 취하는 경우, 확률 $1 - \varepsilon$ 으로 새 환경은 이전 환경과 동일하게 작동한다.</li>
              <li>$\varepsilon$ 의 확률로, 같은 확률분포로 임의의 행동을 재선택하면, 새로운 행동을 선택한 이전의 환경처럼 동작한다.</li>
              <li>보통의 정책으로 이 새로운 환경에서 할 수 있는 최선은, $\varepsilon - soft$ 정책으로 원래 환경에서 할 수 있는 최선과 동일함.</li>
              <li>$\tilde{v_*}$ 는 새로운 환경에 대한 최적 가치함수를 나타냄.
                <ul>
                  <li>$v_\pi = \tilde{v_*}$ 인 경우에만 정책 $\pi$ 가 $\varepsilon - soft$ 정책 중에서 최적임.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/5_4_3_e_soft_policy_tilde_optimal_state_value_function.png" alt="5_4_3_e_soft_policy_tilde_optimal_state_value_function" /></p>

        <ul>
          <li>이 방정식은 $\tilde{v_*}$ 를 $v_\pi$ 로 치환한 것을 제외하고 이전 것과 동일하다.
            <ul>
              <li>$\tilde{v_*}$ 는 고유 솔루션이다.</li>
              <li>따라서 $v_\pi = \tilde{v_*}$ 여야만 한다.</li>
            </ul>
          </li>
          <li>지금까지 $\varepsilon - soft$ 정책에도 정책 반복이 작동한 다는 것을 살펴봄.
            <ul>
              <li>탐욕 정책의 자연스러운 개념을 사용하면, $\varepsilon - soft$ 정책 중에서 최상의 정책이 발견된 경우를 제외하고 모든 단계에서 개선이 보장됨.</li>
              <li>이 분석은 각 단게에서 행동가치함수가 결정되는 방식과 별개이나, 정확하게 계산된다고 가정하였다.</li>
              <li>한편으로 이는 탐색 시작이라는 가정을 없앤 것임.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Off-policy Prediction via Importance Sampling</p>

    <ul>
      <li>Off-policy learning 의 정의
        <ul>
          <li>모든 제어(Control) 에 대한 학습방법은 딜레마에 직면함.
            <ul>
              <li>그들은 후속 최적 행동에 대해 조건부로 행동값을 학습하려고 함.</li>
              <li>모든 행동을 탐색하기 위해 최적이 아닌 행동을 해야 함.</li>
            </ul>
          </li>
          <li>탐색적 정책에 따라 행동하는데 최적의 정책에 알수 있는 방법
            <ul>
              <li>On-policy 방식 : 절충안
                <ul>
                  <li>최적의 정책이 아니라 여전히 탐색하는 최적에 가까운 정책에 대한 작업 값을 학습함.</li>
                </ul>
              </li>
              <li>Off-policy 방식 : 보다 직접적인 접근방식
                <ul>
                  <li>학습되어 최적의 정책이 되는 정책 (대상정책 : target policy) 과 탐험을 하며 행동을 생성하는 정책 (행동정책 : behavior policy)</li>
                  <li>학습이 대상 정책에서 떨어진 (off) 데이터에서 발생한다 : Off-policy learning</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>On-policy 와 Off-policy
        <ul>
          <li>이 책의 나머지 부분에서는 On-policy 방식과 Off-policy 방식 모두를 고려함.</li>
          <li>On-policy 방식
            <ul>
              <li>일반적으로 더 간단</li>
              <li>먼저 고려되는 방식</li>
            </ul>
          </li>
          <li>Off-policy 방식
            <ul>
              <li>추가 개념과 표기법이 필요하며, 데이터가 다른 정책의 것이기 때문에 분산이 더 크고, 수렴 속도가 느림.</li>
              <li>더 강력하고 일반적임.</li>
              <li>특이 케이스로 대상 정책 (target policy) 과 행동 정책 (behavior policy) 이 같으면 On-policy 방식이 된다. (포함)</li>
              <li>응용 프로그램에서 더 많은 쓰임새가 있음.
                <ul>
                  <li>기존의 비학습 컨트롤러 또는 인간 전문가가 생성한 데이터를 학습할 수도 있음.</li>
                </ul>
              </li>
              <li>일부 사람들에게는 세상의 동적 모델을 학습하는 다단계 예측 모델을 배우는 데 중요한 역할을 하는 것으로 간주됨.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Off-policy 방식
        <ul>
          <li>대상정책 (target policy) 과 행동정책 (behavior policy) 이 모두 고정된 예측 문제를 가정
            <ul>
              <li>즉, $v_\pi$ 또는 $q_\pi$ 를 추정하려 하나, 우리가 가진 데이터는 정책 $b$ 를 따르는 에피소드 뿐이라고 가정. ($b \ne \pi$)</li>
              <li>이 경우, $\pi$ 는 대상정책, $b$ 는 행동정책이며, 두 정책 모두 고정 및 주어진 것으로 간주한다.</li>
            </ul>
          </li>
          <li>커버리지 가정 (assumption of coverage)
            <ul>
              <li>$b$ 의 에피소드를 사용하여 $\pi$ 의 값을 추정하려면, $\pi$ 에서 취한 모든 작업이 $b$ 에서도 적어도 가끔씩 수행되어야 함.</li>
              <li>즉, $\pi (a | s) &gt; 0$ 이면 $b (a | s)$ 를 요구한다.</li>
              <li>커버리지 가정의 적용 범위 내에서 $b$ 는 $\pi$ 와 동일하지 않은 상태에서 확률론적이어야 함.</li>
              <li>반면 목표정책 $\pi$ 는 결정론적일 수 있으며, 제어 응용프로그램에서 특히 중요한 사항임.</li>
              <li>제어 측면에서, 대상정책은 일반적으로 행동가치함수의 현재 추정치의 결정론적 탐욕 정책임.</li>
              <li>대상정책이 결정론적 최적 정책이 되는 반면, 행동정책은 확률론적이고 탐색적임. (예 : $\varepsilon - greedy$ 정책)</li>
              <li>이 섹션에서는, $\pi$ 가 변하지 않고, 주어진 예측 문제를 고려함.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>중요도 샘플링 (importance sampling)
        <ul>
          <li>거의 모든 Off-policy 방식은 중요도 샘플링 (importance sampling) 을 활용함.</li>
          <li>중요도 샘플링은 다른 분포에서의 샘플을 통해 하나의 분포에서의 기댓값을 추정하는 일반적인 기법임.</li>
          <li>중요도 샘플링 비율 (imporatnce-sampling ratio)
            <ul>
              <li>대상정책 (target policy) 과 행동정책 (behavior policy) 에 따른 경로 발생의 상대적 확률에 따라 리턴 값에 부여하는 가중치</li>
            </ul>
          </li>
          <li>$S_t$ 에서 시작하여, 정책 $\pi$ 하에 후속 상태-행동 궤적 ($A_t, S_{t+1}, A_{t+1}, \cdots , S_T$) 의 확률은 아래와 같다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_5_1_importance_sampling_trajectory.png" alt="5_5_1_importance_sampling_trajectory" /></p>

        <ul>
          <li>여기에서 $p$ 는 상태전이확률 함수이다.</li>
          <li>따라서, 대상정책과 행동정책 간 궤적의 상대적 확률 (importance-sampling ratio) 은 아래와 같다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_5_2_importance_sampling_ratio.png" alt="5_5_2_importance_sampling_ratio" /></p>

        <ul>
          <li>궤적 확률은 일반적으로 알려지지 않은 MDP의 전이확률에 의존하지만, 분자와 분모 모두에 동일하게 나타나 상쇄된다.</li>
          <li>중요도 샘플링 비율은 MDP 가 아닌, 두 정책과 시퀀스에만 의존하게 된다.</li>
          <li>우리가 원하는 추정값은 대상정책(traget policy) 의 기대되는 리턴값이다.
            <ul>
              <li>실제로 가진 값은 행동정책(behavior policy) 의 리턴값 $G_t$ 이다.</li>
              <li>즉, $\mathbb{E}[G_t | S_t = s] = v_b(s)$ 이며, 이를 이용해 평균값을 구하여 $v_\pi$ 를 얻는 것은 불가능하다.</li>
            </ul>
          </li>
          <li>이 부분에서 중요도 샘플링 (importance sampling) 개념이 필요하다.
            <ul>
              <li>즉, ratio $p_{t:T-1}$ 값이 리턴값을 변환시켜, 올바른 예측값을 구하게 된다.</li>
              <li>$\mathbb{E}[p_{t:T-1}G_t | S_t = s] = v_\pi (s)$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>중요도 샘플링 (importance sampling) 을 활용한 가치함수추정
        <ul>
          <li>이제 우리는 $v_\pi(s)$ 를 추정하기 위해 정책 $b$ 의 관찰된 에피소드 배치의 평균 리턴값을 계산하는 몬테카를로 알고리즘이 준비되었다.</li>
          <li>여기에서 에피소드의 경계를 넘어 증가하는 방식으로 타임스텝 (time step) 번호를 지정하는 것이 편리하다.
            <ul>
              <li>즉, 배치의 첫 에피소드가 time step = 100 에서 종료했다면, 다음 에피소드는 t = 101 에서 시작됨.</li>
              <li>이를 통해 특정 에피소드의 특정 타입스텝을 참조하기 위해 time-step number 를 사용할 수 있다.</li>
            </ul>
          </li>
          <li>수식 내 문자의 의미
            <ul>
              <li>$\mathcal{T} (s)$ : 상태 s 를 방문하는 모든 time-step number 집합
                <ul>
                  <li>이것은 every-visit 방식에 해당하는 의미이며, first-visit 방식의 경우 $\mathcal{T} (s)$ 는 에피소드 내 s를 처음 방문한 time-step number 만 포함한다.</li>
                  <li>$T(t)$ : 시간 t 이후 첫 번째 종료를 의미</li>
                  <li>$G_t$ : t 이후 $T(t)$ 까지의 리턴값</li>
                  <li>${G_t}_{t \in \mathcal{T}(s)}$ : 상태 s 에 속하는 리턴값</li>
                  <li>${p_{t:T(t)-1}}_{t \in \mathcal{T}(s)}$ : 위에 상응하는 중요도 샘플링 비율</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Ordinary Importance Sampling : $v_\pi (s)$ 를 추정하기 위해, 간단히 리턴 값을 비율로 조정하고 평균값을 구하는 수식은 아래와 같다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_5_3_ordinary_importance_sampling.png" alt="5_5_3_ordinary_importance_sampling" /></p>

        <ul>
          <li>Weighted Importance Sampling : 0이 아닌 평균의 가중치 (weighted average) 를 사용한다.</li>
        </ul>

        <p><img src="/assets/images/posts/5_5_4_weighted_importance_sampling.png" alt="5_5_4_weighted_importance_sampling" /></p>

        <ul>
          <li>Ordinary Importance Sampling 과 Weighted Importance Sampling
            <ul>
              <li>두 가지 종류의 중요도 샘플링을 이해하기 위해, 한 번의 리턴을 관찰한 후 first visit 방법 의 추정치를 고려해보자.</li>
              <li>평균의 가중치 추정 (weighted-average estimate) (Weighted Importance Sampling의 경우) 에서는 단일 리턴에 대한 비율 $p_{t:T(t)-1}$ 이 분자와 분모에서 상쇄되어, 비율과 관계없이 추정치는 관찰된 리턴과 동일하게 됨. (비율이 0이 아닌 경우를 가정)</li>
              <li>이 관찰된 리턴이 유일하게 관찰된 경우에는 합리적인 추정치이지만, 기대값은 $v_b(s)$ 가 아닌 $v_\pi (s)$ 이며, 이 통계적 의미에서 편향된 추정치이다.</li>
              <li>반면 일반 중요도 샘플링 (Ordinary Importance Sampling) 추정의 first visit 버전은 항상 기대값이 $v_\pi (s)$ 이다. (편향이 없는 추정치)
                <ul>
                  <li>하지만 이는 극단적일 수 있음.</li>
                  <li>예를 들어 가중치 비율이 10이라고 가정해보면, 이는 관측된 경로가 행동정책에 비해 대상정책에서 10배 더 가능성이 높다는 의미이다.</li>
                  <li>이 경우 일반 중요도 샘플링 (Ordinary Importance Sampling) 의 추정치는 관측된 리턴의 10배가 된다.</li>
                  <li>즉, 관측된 보상과는 상당히 멀어질 것이며, 이는 해당 에피소드의 경로가 대상정책을 매우 잘 대표한다고 생각되더라도 그렇다.</li>
                </ul>
              </li>
              <li>형식적으로, 두 종류의 중요도 샘플링의 첫 방문 방법의 차이는 편향과 분산으로 표현된다.
                <ul>
                  <li>일반 중요도 샘플링은 편향이 없지만, 가중 중요도 샘플링은 편향이 존재한다. (편향은 점진적으로 0으로 수렴함.)</li>
                  <li>일반 중요도 샘플링의 분산은 일반적으로 무한대일 수 있다.
                    <ul>
                      <li>비율의 분산이 무한대일 수 있기 때문</li>
                      <li>반면 가중 샘플링 방식의 추정치에서는 단일 리턴값의 가장 큰 가중치는 1이다.</li>
                      <li>실제로 유한한 리턴값을 가정하면, 가중 중요도 샘플링 추정치의 분산은 비율의 분산 자체가 무한대일지라도 0으로 수렴함 (Precup, Sutton, and Dasgupta 2001)</li>
                    </ul>
                  </li>
                  <li>가중 추정치의 분산은 일반적으로 매우 낮고 강력하게 선호된다.</li>
                </ul>
              </li>
              <li>함수 근사를 사용한 근사 방법을 더 쉽게 확장할 수 있기 때문에, 일반 중요도 샘플링 또한 학습할 것이다.</li>
              <li>일반 중요도 샘플링과 가중 중요도 샘플링의 every visit 방식은 모두 편향이 있으며, 샘플의 수가 증가함에 따라 편향은 0으로 수렴한다.</li>
              <li>every visit 방식은 방문한 상태를 추적할 필요가 없으며, 근사화에 쉽게 확장할 수 있기 때문에 선호되는 편이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>부연설명 (가치함수 V(s)를 구하는 Ordinary Importance Sampling과 Weighted Importance Sampling의 수식)
        <ul>
          <li>Ordinary Importance Sampling
            <ul>
              <li>$V(s) = \frac{1}{N} * \sum [ W(i) * R(i) ]$
                <ul>
                  <li>N : 수집된 샘플의 개수</li>
                  <li>$W(i)$ : i 번째 샘플의 가중치</li>
                  <li>$R(i)$ : i 번째 샘플의 보상</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Weighted Importance Sampling
            <ul>
              <li>$V(s) = \sum [ \frac{(W(i) * R(i))}{\sum W(i)} ]$
                <ul>
                  <li>$W(i)$ : i 번째 샘플의 가중치</li>
                  <li>$R(i)$ : i 번째 샘플의 보상</li>
                  <li>$\sum W(i)$ : 가중치의 합</li>
                </ul>
              </li>
              <li>Ordinary Importance Sampling 은 모든 샘플이 동일한 중요도 ($\frac{1}{N}$) 를 가짐 (평균을 구함)</li>
              <li>Weighted Importance Sampling 은 각 샘플마다 개별적인 중요도를 가짐. ($\frac{W(i)}{\sum W(i)}$)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="introduction-to-monte-carlo-methods">Introduction to Monte Carlo Methods</h2>

<ul>
  <li>What is Monte Carlo?
    <ul>
      <li>소개
        <ul>
          <li>몬테카를로 라는 용어는 반복적으로 무작위 샘플링에 의지하는 추측방식을 광범위하게 일컷는 용어로 많이 쓰임.</li>
          <li>강화학습에서는 상태의 시퀀스, 행동, 보상등의 경험으로부터 직접적으로 추정값에 접근하는 방식임.
            <ul>
              <li>경험으로부터 직접 학습한다는 것은 강력한 이점인데, 환경역학에 대한 사전지식 없이 정확한 가치함수를 추정할 수 있기 때문</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>학습 목표
        <ul>
          <li>몬테카를로 방식이 샘플링된 상호작용을 통한 가치함수 예측에 어떻게 쓰이는지 이해하기</li>
          <li>몬테카를로 방식을 통해 풀 수 있는 문제 식별하기</li>
        </ul>
      </li>
      <li>강화학습에서 DP 학습의 한계점
        <ul>
          <li>에이전트는 환경의 상태변이확률을 알고 있어야 함.
            <ul>
              <li>예를 들어, 기상학자가 미래의 기상 예측을 할 때, 환경의 상태변이확률을 알 수가 없음.</li>
            </ul>
          </li>
          <li>상태변이확률을 계산하는 것도 어렵고 에러 발생이 큰 지루한 작업임.
            <ul>
              <li>12개의 주사위를 던지는 문제일 때, DP의 계산은 코딩 혹은 계산 정확성에서 지루하고 오류가 발생하기 쉬운 작업임.
                <ul>
                  <li>합계 12가 나올 확률, … , 합계 72가 나올 확률을 계산하는 것</li>
                  <li>몬테카를로 방식에서는 많은 무작위 샘플을 이용해 평균을 구하고, 값을 추정함.</li>
                </ul>

                <p><img src="/assets/images/posts/monte_carlo_12_dices.png" alt="monte_carlo_12_dices" /></p>

                <ul>
                  <li>12개의 주사위의 기대 합 42와 근접한 수치의 추측</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>몬테카를로 방식</p>

        <ul>
          <li>몬테카를로 방식을 이용한 가치함수 추정</li>
        </ul>

        <p><img src="/assets/images/posts/monte_carlo_for_policy_evaluation.png" alt="monte_carlo_for_policy_evaluation" /></p>

        <ul>
          <li>동일한 상태에서 여러 리턴값을 관찰</li>
          <li>이 다수의 리턴값의 평균을 구해 해당 상태의 추정 리턴값을 구함.</li>
          <li>샘플의 수가 많아질수록, 실제 값에 가까워짐.</li>
          <li>
            <p>이러한 리턴 값은 에피소드가 끝나야 알 수 있음. (Episodic Tasks 라 가정)</p>
          </li>
          <li>몬테카를로 방식과 bandit 방식</li>
        </ul>

        <p><img src="/assets/images/posts/monte_carlo_and_bandits.png" alt="monte_carlo_and_bandits" /></p>

        <ul>
          <li>몬테카를로와 bandit 방식은 유사한데, badnit 에서도 arm 을 여러번 당겨 평균값을 구해 추정값을 구했었음.</li>
          <li>
            <p>몬테카를로의 차이점은 arm 이 아닌 정책을 고려한다는 점이다.</p>
          </li>
          <li>몬테카를로 방식의 리턴 값 계산</li>
        </ul>

        <p><img src="/assets/images/posts/monte_carlo_computing_returns_efficiently.png" alt="monte_carlo_computing_returns_efficiently" /></p>

        <ul>
          <li>효율적인 계산을 위해 에피소드 종료시점 부터 거꾸로 계산해야 함.</li>
        </ul>

        <p><img src="/assets/images/posts/monte_carlo_prediction_psuedo_code.png" alt="monte_carlo_prediction_psuedo_code" /></p>

        <ul>
          <li>평균을 구할 때, 이전의 값들을 모두 저장하는 것이 아닌 증분 업데이트의 사용이 가능하다.
            <ul>
              <li>$NewEstimate \gets OldEstimate + StepSize [Target - OldEstimate]$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Using Monte Carlo for Prediction
    <ul>
      <li>학습 목표
        <ul>
          <li>몬테카를로 예측을 통해 주어진 정책의 가치 함수를 예측하기</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="monte-carlo-for-control">Monte Carlo for Control</h2>

<ul>
  <li>
    <p>Using Monte Carlo for Action Values</p>
  </li>
  <li>학습 목표
    <ul>
      <li>몬테카를로 방식을 통해 행동 가치함수 (action value function) 추정하기</li>
      <li>몬테카를로 알고리즘에서 탐색의 유지의 중요성을 이해하기</li>
    </ul>
  </li>
  <li>Learning Action Values
    <ul>
      <li>상태의 값을 추정하는 방식과 동일 (특정 상태에서 행동을 선택했을 때의 리턴값의 평균을 구함)\</li>
      <li>행동 가치를 학습하는 이유
        <ul>
          <li>하나의 상태에서 각 행동이 가지는 가치비교를 할 수 있음.</li>
          <li>더 나은 행동으로의 정책 변경이 가능
            <ul>
              <li>이는 다른 행동을 하여 가치를 추정했을 때 가능함.</li>
              <li>이 부분이 어려운 부분인데, 결정론적인 정책을 따를 경우 다른 행동을 탐색하지 않는다. (정책을 따름)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>탐색의 유지 문제
        <ul>
          <li>에이전트는 값들을 학습을 위해 각 상태의 모든 행동을 시도해봐야 한다.</li>
          <li>exploring starts (탐색 시작)
            <ul>
              <li>모든 상태-행동 쌍에서 첫 시작을 해보는 것을 보증해야한다.</li>
              <li>그 뒤로 에이전트는 정책을 따라 움직인다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Using Monte Carlo methods for generalized policy iteration</p>

    <ul>
      <li>학습 목표
        <ul>
          <li>몬테카를로 방식을 사용하여 GPI (Generalized Policy Iteration) 알고리즘 구현하는 방법 이해하기</li>
        </ul>
      </li>
      <li>
        <p>Monte Carlo Generalized Policy Iteration</p>

        <p><img src="/assets/images/posts/monte_carlo_generalized_policy_iteration.png" alt="monte_carlo_generalized_policy_iteration" /></p>

        <ul>
          <li>정책 평가를 Monte Carlo Prediction 으로 진행 (샘플링된 에피소드의 평균을 통한 행동가치함수의 추정)
            <ul>
              <li>탐색 유지를 위한 여러 방법 중 하나를 사용 (여기에서는 탐색시작 방법을 사용한다.)</li>
            </ul>
          </li>
          <li>정책 개선을 $q_{\pi_k}$ 의 $\arg\max$ 함수를 이용해 탐욕적으로 개선</li>
        </ul>

        <p><img src="/assets/images/posts/monte_carlo_es_psuedo_code.png" alt="monte_carlo_es_psuedo_code" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Solving the Blackjack Example</p>

    <ul>
      <li>학습 목표
        <ul>
          <li>MDP 해결을 위한 탐색 시작과 함께 몬테 카를로의 적용</li>
        </ul>
      </li>
      <li>몬테카를로 방식의 블랙잭 적용 예시
        <ul>
          <li>문제의 정의
            <ul>
              <li>각 블랙잭 게임을 하나의 에피소드로 보고, 할인되지않은 MDP 를 적용한다.</li>
              <li>보상 : 패배시 -1, 비길 시 0, 이길 시 1</li>
              <li>행동 : 힛, 스틱</li>
              <li>상태 : 에이스가 있는지 여부, 플레이어 카드의 총합, 딜러의 카드 1장
                <ul>
                  <li>위의 경우 총 200개의 상태가 존재한다.</li>
                </ul>
              </li>
              <li>덱의 모든 카드를 교체나 버리는 것 없이 사용한다고 가정한다. (카드카운팅 불가 : 현 상태로 마르코브 속성을 가짐.)</li>
            </ul>
          </li>
          <li>몬테카를로 적용
            <ul>
              <li>에이전트는 하나의 에피소드가 끝나고 학습이 가능</li>
              <li>Discount factor 가 1이기 때문에, 승리했을 경우 해당 에피소드의 각 상태에서의 보상값은 1이다.</li>
            </ul>
          </li>
          <li>몬테카를로의 장점
            <ul>
              <li>환경의 거대한 모델을 저장할 필요가 없음</li>
              <li>특정 상태에 대한 값을 개별적으로 측정할 수 있다. (타 상태의 값과 관계없이)</li>
              <li>값의 업데이트 계산에 MDP의 상태집합의 크기가 영향을 주지 않는다. (에파소드의 길이에 영향을 받음)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>블랙잭의 조건
        <ul>
          <li>Exploring Starts 를 사용하기 좋은 조건 (에피소드 시작을 무작위 상태에서 무작위 행동을 하는 것으로 시작한다.)
            <ul>
              <li>현재의 블랙잭 게임 조건은 자연스럽게 랜덤한 상태에서 시작하게 된다. (플레이어 손패 카드 2장 (에이스 유무와 합계), 딜러 패 2장중 1장 오픈)</li>
              <li>즉, 첫 행동을 무작위로 선택하면 되는데, 정책을 따르는 것은 그 이후에 하게 됨. (합이 20이 넘으면 스틱)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="exploration-methods-for-monte-carlo">Exploration Methods for Monte Carlo</h2>

<ul>
  <li>
    <p>Epsilon-soft Policy</p>

    <ul>
      <li>학습 목표
        <ul>
          <li>왜 탐색 시작이 실 문제에서 적용이 어려운지 이해하기</li>
          <li>몬테카를로 제어를 위한 대안적 탐색유지 방법의 설명</li>
        </ul>
      </li>
      <li>탐색 시작을 사용할 수 없는 경우
        <ul>
          <li>탐색 시작 알고리즘은 모든 가능한 상태와 행동 쌍에서 시작할 수 있어야 한다.
            <ul>
              <li>그렇지 않은 경우 충분한 탐색을 하지 못하므로, 최적 정책이 아닌 차선의 다른 정책에 수렴하게 될 수 있다.</li>
            </ul>
          </li>
          <li>대부분의 문제는 시작 상태-행동 쌍으로 무작위 샘플을 구할 수 없다.
            <ul>
              <li>예를 들어 자율주행 문제의 경우 초기 상태-행동 쌍을 무작위 샘플링할 수 없다. (닥칠 수 있는 모든 상황-행동으로 초기화 시작)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>e-Greedy 탐색</p>

        <p><img src="/assets/images/posts/e_greedy_exploration.png" alt="e_greedy_exploration" /></p>

        <ul>
          <li>$\varepsilon-Greedy$ 정책
            <ul>
              <li>Bandits 에서 사용했던 $\varepsilon-Greedy$ 을 몬테카를로에 적용</li>
              <li>$\varepsilon-Greedy$ 정책은 확률론적인 정책임
                <ul>
                  <li>보통 탐욕적인 행동을 취하나 때로는 무작위 행동을 선택함.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>$\varepsilon-Soft$ 정책
            <ul>
              <li>$\varepsilon-Greedy$ 정책 을 포괄하는 정책</li>
              <li>각 행동에 $\frac{\epsilon}{ | \mathcal{A} |}$ (nonzero) 이상의 확률을 부여
                <ul>
                  <li>필연적으로 모든 행동을 시도하게</li>
                </ul>
              </li>
              <li>$\varepsilon-Soft$ 정책은 에이전트로 하여금 계속 탐색하도록 강제한다.
                <ul>
                  <li>즉, 탐색 시작의 요구사항을 제거 (탐색 시작을 대체) 할 수 있음.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>$\varepsilon-Soft$ 정책의 한계
  <img src="/assets/images/posts/e_soft_policies_not_be_optimal.png" alt="e_soft_policies_not_be_optimal" /></p>

        <ul>
          <li>결정론적 최적 정책에 수렴할 수 없다.
            <ul>
              <li>탐색 시작의 방식은 최적 정책에 도달함과 다르게, 최적 $\varepsilon-Soft$ 정책 에만 도달할 수 있음.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/e_soft_policy_psuedo_code.png" alt="e_soft_policy_psuedo_code" /></p>

        <ul>
          <li>위 코드는 몬테카를로 $\varepsilon-Soft$ 정책에 대한 내용으로, 탐색시작과 차이가 생기는 부분을 표시한 것이다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="off-policy-learning-for-prediction">Off-policy Learning for Prediction</h2>

<ul>
  <li>
    <p>Why does off-policy learning matter?</p>

    <ul>
      <li>학습 목표
        <ul>
          <li>off-policy learning 이 탐색 문제를 해결하는 데 어떻게 도움이 되는지 이해하기</li>
          <li>목표 정책의 예시 및 행동 정책의 예시를 생성</li>
        </ul>
      </li>
      <li>
        <p>$\varepsilon - soft$ 정책의 한계점</p>

        <p><img src="/assets/images/posts/epsilon_soft_policy_limitation.png" alt="epsilon_soft_policy_limitation" /></p>

        <ul>
          <li>탐색에도, 행동에도 최적 정책이 아니다.</li>
          <li>학습을 통해 차선의 최적 정책으로 수렴함.</li>
        </ul>
      </li>
      <li>On-Policy 와 Off-Policy
        <ul>
          <li>On-Policy Learning: 에이전트가 학습할 데이터의 생성에 기여한 정책을 발전시킴.</li>
          <li>Off-Policy Learning: 에이전트가 다른 정책으로부터 학습할 데이터를 생성하여 이와 별개의 정책을 학습
            <ul>
              <li>예를들어 uniform random 정책으로 생성한 데이터로 최적 정책을 학습함.</li>
            </ul>

            <p><img src="/assets/images/posts/off_policy_target_policy.png" alt="off_policy_target_policy" /></p>

            <ul>
              <li>
                <p>대상정책(target policy) : 에이전트가 개선해나아가며 최종적으로 획득하길 원하는 정책
    + 에이전트가 학습할 가치 함수는 대상정책(target policy) 를 기반으로 함.</p>

                <p><img src="/assets/images/posts/off_policy_behavior_policy.png" alt="off_policy_behavior_policy" /></p>

                <ul>
                  <li>행동정책(behavior policy) : 에이전트가 행동을 선택하는 기준이 되는 정책</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Off-Policy 를 사용하는 이유
        <ul>
          <li>지속 탐색의 또다른 전략을 제공하기 때문
            <ul>
              <li>대상정책을 따라 학습하게 되면, 전체 상태가 아닌 소수의 상태만을 탐색하게 된다.</li>
            </ul>
          </li>
          <li>Off-policy 에 대한 몇가지 유효한 어플리케이션이 존재함 (참고용 자료)
            <ul>
              <li>learning from demonstartion
                <ul>
                  <li>따라하기 학습</li>
                  <li>전문가의 행동을 따라하거나 전문가가 제공한 상태-행동 쌍 데이터를 이용하여 학습을 진행</li>
                  <li>초기에 정확한 행동을 배우기 위해 유용함</li>
                  <li>전문ㅏ의 경험을 통해 학습 과정을 가속화, 좋은 품질의 행동을 습득</li>
                </ul>
              </li>
              <li>parallel learning
                <ul>
                  <li>여러 에이전트가 동시에 병렬로 학습을 수행하는 방법 (여러 에이전트가 동시에 다른 환경 또는 다른 부분 문제를 처리)</li>
                  <li>학습 속도를 높이고 효율성을 향상시키는 데 도움이 됨</li>
                  <li>다양한 경험을 공유하고 전체 학습 성능을 향상</li>
                  <li>분산 시스템에서 사용되는 경우가 많으며, 학습 속도와 성능 향상을 위해 활용될 수 있음</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Off-Policy 의 중요 요건
        <ul>
          <li>행동정책 (behavior policy) 이 대상정책 (target policy) 을 포괄해야 함.
            <ul>
              <li>즉, $\pi (a | s) &gt; 0$ 이면 $b(a | s ) &gt; 0$ 이어야 한다.
                <ul>
                  <li>수학적인 이유는 다음 섹션에서 살펴봄</li>
                  <li>직관적인 이유는 아래의 그림과 같을 경우, 우측으로 가는 행동에 대한 행동 가치를 알 수가 없기 때문</li>
                </ul>

                <p><img src="/assets/images/posts/off_policy_behavior_policy_must_contain_target_policy.png" alt="off_policy_behavior_policy_must_contain_target_policy" /></p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Importance Sampling</p>

    <ul>
      <li>학습 목표
        <ul>
          <li>중요도 샘플링을 이용하여 다른 분포의 샘플을 통한 대상 분포도에 대한 예상 값을 추정한다.</li>
        </ul>
      </li>
      <li>Importance Sampling 유도
        <ul>
          <li>개요
            <ul>
              <li>Sample : $x \sim b$
                <ul>
                  <li>~ : 분포에 의해 생성되는 것을 의미함.</li>
                  <li>x : 생성된 데이터</li>
                  <li>b : 정책</li>
                </ul>
              </li>
              <li>Estimate : $\mathbb{E}_\pi [X]$
                <ul>
                  <li>target 정책에 의해 추정되어지는 예측값</li>
                </ul>
              </li>
              <li>$b$ 에서 유도된 데이터 $x$ 이므로, 이 값을 간단히 평균내어 $\pi$ 에 대한 예측치를 구할 수 없다.</li>
            </ul>
          </li>
          <li>
            <p>기대값에 대한 정의</p>

            <p><img src="/assets/images/posts/importance_sampling_derivation.png" alt="importance_sampling_derivation" /></p>

            <ul>
              <li>$E_\pi [X] \doteq \sum_{x \in X} x \pi (x)$
                <ul>
                  <li>모든 가능한 출력값 $x$ 에 대해 $\pi$ 에 따른 확률값을 곱해 합계를 구함.</li>
                </ul>
              </li>
              <li>$=\sum_{x \in X} x \pi (x) \frac{b(x)}{b(x)}$
                <ul>
                  <li>뒤의 $\frac{b(x)}{b(x)}$ 는 값이 1이므로 수식에 곱할 수 있다.</li>
                  <li>$b(x)$ : b 정책 하의 관측된 결과값 $x$ 에 대한 확률값</li>
                </ul>
              </li>
              <li>$=\sum_{x \in X} x \frac{\pi (x)}{b(x)} b(x)$
                <ul>
                  <li>여기에서 $\frac{\pi (x)}{b(x)}$ 를 Importance Sampling Ratio 라고 한다.</li>
                  <li>우리는 보통 Importance Sampling Ratio 를 $\rho (x)$ 로 표기한다.</li>
                  <li>즉, $\sum_{x \in X} x \rho (x) b(x)$</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>기대값을 $b$ 정책에 대한 식으로 치환</p>

            <p><img src="/assets/images/posts/importance_sampling_derivation_to_b_1.png" alt="importance_sampling_derivation_to_b_1" /></p>

            <ul>
              <li>위 공식에서 유도된 $E_\pi [X] = \sum_{x \in X} x \rho (x) b(x)$</li>
              <li>$= E_b [X \rho (X)]$ : $\pi$ 에 대한 식을 $b$ 에 대한 식으로 치환함.</li>
            </ul>

            <p><img src="/assets/images/posts/importance_sampling_derivation_to_b_2.png" alt="importance_sampling_derivation_to_b_2" /></p>

            <ul>
              <li>$\mathbb{E} [X] \approx \frac{1}{n} \sum_{i=1}^n x_i$
                <ul>
                  <li>$\approx$ : 거의 동일</li>
                  <li>위 식은 몬테카를로에서 기대값을 구하는 방식으로 평균을 취한 값임.</li>
                </ul>
              </li>
              <li>$\approx \frac{1}{n} \sum_{i=1}^n x_i \rho (x_i)$
                <ul>
                  <li>즉, 위 식을 풀기 위해 weighted sample average 를 구하면 되는데, 이는 importance sampling ratio 를 weightings 로 사용한 수식이다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/importance_sampling_derivation_to_b_3.png" alt="importance_sampling_derivation_to_b_3" /></p>

            <ul>
              <li>우리는 식에서 $\pi$ 가 아닌 $b$ 의 분포를 이용하여 계산하고, 실제로 이 값은 sample average 를 이용해 $\pi$ 의 분포 아래 값을 구하는 식이 된다.</li>
            </ul>
          </li>
          <li>
            <p>샘플링을 이용한 예측치를 구하는 예시</p>

            <p><img src="/assets/images/posts/importance_sampling_example.png" alt="importance_sampling_example" /></p>

            <ul>
              <li>위의 예시는 $b$ 의 확률분포 내에서 몬테카를로 방식으로 진행했을 때 보상값이 1, 3, 1 이 나왔을 시 importance sampling ratio 를 이용해 $\pi$ 의 분포도로 치환하여 계산한 식을 의미하며 $\pi$ 의 실제 예측값인 2.2 에 가까운 근사값이 나오는 것을 볼 수 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Off-policy Monte Carlo Prediction
    <ul>
      <li>학습 목표
        <ul>
          <li>중요도 샘플링을 이용하여 리턴값을 수정하는 방법을 이해하기</li>
          <li>몬테카를로 예측 알고리즘을 변형하여 off-policy learning 에 적용하는 방법 이해하기.</li>
        </ul>
      </li>
      <li>
        <p>몬테카를로 방식에 대한 복기</p>

        <p><img src="/assets/images/posts/monte_carlo_recall.png" alt="monte_carlo_recall" /></p>

        <ul>
          <li>상태 $s$ 에서 수집된 리턴값의 평균을 구하는 방식</li>
        </ul>
      </li>
      <li>
        <p>Off-Policy Monte Carlo</p>

        <p><img src="/assets/images/posts/off_policy_montecarlo.png" alt="off_policy_montecarlo" /></p>

        <ul>
          <li>
            <p>정책 $b$ 로부터 수집된 리턴값에 importance sampling ratio weight ($\rho$) 값을 곱한 값으로 평균을 구함.</p>
          </li>
          <li>$\rho = \frac{\mathbb{P} ( \texttt{trajectory under } \pi )}{\mathbb{P} ( \texttt{trajectory under } b )}$</li>
          <li>여기에서 $\rho$ 는 전체 궤적의 분포도를 수정하여 리턴값의 분포를 수정한다.</li>
          <li>위 수정 방식을 이용해 $\pi$ 의 기대 리턴값을 구한다.</li>
          <li>$V_\pi (s) = \mathbb{E}_b [ \rho G_t | S_t = s ]$</li>
        </ul>

        <p><img src="/assets/images/posts/off_policy_trajectories.png" alt="off_policy_trajectories" /></p>

        <ul>
          <li>궤적의 확률 값은 시점 $t$ 에서 종료상태 $T$ 까지의 정책 상 행동 $a$ 의 선택 확률과 상태 $s$ 로의 전이확률 (환경역학) 값의 연속이다.
            <ul>
              <li>$b(A_t | S_t) p (S_{t+1} | S_t, A_t) b(A_{t+1} | S_{t+1}) p (S_{t+2} | S_{t+1}, A_{t+1}) \cdots p (S_T | S_{T-1}, A_{T-1} )$</li>
              <li>위의 식을 $\Pi_{k=t}^{T-1} b(A_k | S_k) p (S_{k+1} | S_k, A_k)$ 로 표기할 수 있다.</li>
            </ul>
          </li>
          <li>위의 식에서 환경역학 $p$ 는 동일한 값이기에 상쇄 (약분) 된다.</li>
          <li>위의 식에서 남은 앞 부분은 importance sampling ratio 인 $\rho$ 이다.
            <ul>
              <li>$\rho_{t:T-1} \doteq \Pi_{k=t}^{T-1} \frac {\pi (A_k | S_k)} {b (A_k | S_k)}$</li>
            </ul>
          </li>
          <li>Off-Policy Value : $E_b [ \rho_{t:T-1} G_t | S_t = s] = v_\pi (s)$</li>
        </ul>
      </li>
      <li>
        <p>Off-Policy Monte Carlo 의 구현</p>

        <ul>
          <li>
            <p>Off-policy every-visit MC</p>

            <p><img src="/assets/images/posts/off_policy_every_visit_MC_psuedo_code.png" alt="off_policy_every_visit_MC_psuedo_code" /></p>

            <ul>
              <li>에피소드를 생성할 때 $\pi$ 가 아닌 $b$ 를 통해 생성한다.</li>
              <li>에피소드의 리턴 값이 $G \gets \gamma G + R_{t+1}$ 이 아닌 $G \gets \gamma W G + R_{t+1}$ 로 계산된다.</li>
              <li>$W \gets W \frac {\pi (A_t | S_t)}{b (A_t | S_t)}$ 로 매 스텝마다 누적곱이 된다.</li>
            </ul>
          </li>
          <li>
            <p>$\rho_{t:T-1}$ 의 증분계산</p>

            <p><img src="/assets/images/posts/importance_sampling_calculate.png" alt="importance_sampling_calculate" /></p>

            <ul>
              <li>몬테카를로 알고리즘 루프가 마지막 타임스텝에서부터 거꾸로 계산을 누적하는 방식이므로, $\rho$ 의 계산 또한 동일하다.</li>
              <li>위의 식과 같이 이전 단계의 값의 누적 곱을 구하는 방식이기에, 이전 단계의 모든 $\rho$ 값을 따로 저장할 필요가 없다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Emma Brunskill - Batch Reinforcement Learning
    <ul>
      <li>배치 강화학습의 개요
        <ul>
          <li>현재 강화학습의 놀라운 성공에 대해 생각해보면 로봇 공학이나 게임플에이와 같은 영역이며, 시뮬레이터에서 액세스 할 수 있는 케이스임</li>
          <li>에이전트가 오랫동안 학습에 실패하더라도 결국 학습하게 됨.</li>
          <li>반대로 사람과 상호작용하는 영역의 경우 사람들이 학습하거나 행동하는 방식에 대한 훌륭한 시뮬레이터를 얻을 수 없음
            <ul>
              <li>실제 데이터에 의존해야 함 (실제 인간과 상호작용 하는 것과 관련이 있기에 어려운 일임)</li>
              <li>에이전트가 학습해야 하는 데이터의 양을 최소화하기 위한 기술과 학습량의 근본적인 한계는 무엇일까?</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>실 생활 데이터의 활용 어려움
        <ul>
          <li>실제 행하지 않은 행동의 결과 추론 문제</li>
          <li>과거에 행했던 하나의 행동이 아닌 일련의 행동에 대한 조합 가능성</li>
        </ul>
      </li>
      <li>실 생활 데이터의 활용 가능성
        <ul>
          <li>특정한 행동순서로 진행하는 것의 결과값을 알고 싶을 때, 데이터가 10만건이라면 이 중 해당 행동순서를 가지는 경우는 100건이 될 수 있다.</li>
        </ul>
      </li>
      <li>중요도 샘플링의 문제점
        <ul>
          <li>중요도 샘플링은 편향되지 않은 추정량을 제공하지만 일반적으로 분산이 매우 높을 수 있음
            <ul>
              <li>즉, 데이터가 많지 않거나 에피소드가 길면 열악한 근사값을 구할 확률이 큼</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>중요도 샘플링의 대안 : MDP 의 Parametric Models
        <ul>
          <li>상태 전이확률과 보상 함수를 파라미터화한 모델
            <ul>
              <li>파라미터를 사용하여 상태 전이 확률과 보상함수를 추정하거나 근사하는 방식으로 동작</li>
            </ul>
          </li>
          <li>실제 환경의 동작을 정확히 모델링하지 않고도 예측과 학습이 수행 가능, 복잡한 환경에서도 효율적인 학습을 할 수 있음.</li>
          <li>하지만 실제 환경과 다름 - 편향을 가질 수 있음</li>
          <li>그러나 약간의 편향에 대한 대가로 낮은 분산을 가질 수 있음</li>
        </ul>
      </li>
      <li>Doubly Robust Estimator 방법
        <ul>
          <li>이 방법은 오프-폴리시(Off-Policy) 학습에서 사용되며, 동일한 데이터를 사용하여 가치 함수를 추정하는 데에 효과적</li>
          <li>Doubly Robust Estimators의 핵심 개념은 두 가지 보정 요소를 조합하는 것
            <ul>
              <li>가치 함수 추정에 사용되는 모델 또는 추정기(estimator) : Parametric Models</li>
              <li>가치 함수 추정에서 발생하는 편향을 보정하는 가중치 : Importance Sampling</li>
            </ul>
          </li>
          <li>첫 번째 방법은 모델 또는 추정기를 사용하여 행동 가치 함수 또는 상태 가치 함수를 추정하는 것. 이 추정은 일부 편향을 가질 수 있지만, 적은 데이터에서도 추정이 가능.</li>
          <li>두 번째 방법은 경험 데이터를 사용하여 행동 가치 함수 또는 상태 가치 함수를 보정하는 것. 이 보정은 가중치를 사용하여 편향을 보정하는 것으로, 정책 평가의 불일치 문제를 해결하는 데 도움이 됨.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="chapter-summary">Chapter Summary</h2>

<ul>
  <li>몬테카를로 방식의 장점
    <ul>
      <li>본 장의 몬테카를로 방식 : 샘플 에피소드의 형태에서의 경험으로 가치함수와 최적의 정책을 학습</li>
      <li>환경 역학의 모델 없이, 환경과의 상호작용에서 직접 최적의 정책을 학습함.</li>
      <li>시뮬레이션 또는 샘플 모델과 함께 사용할 수 있음.
        <ul>
          <li>많은 응용프로그램의 경우 DP 방식에서 요구하는 전환 확률의 명시적 모델을 구성하는 것은 어렵지만, 샘플 에피소드를 시뮬레이션 하는 것은 쉬움.</li>
        </ul>
      </li>
      <li>몬테카를로 방식을 상태 중 작은 하위 집합에 집중시킬 수 있다.
        <ul>
          <li>나머지 상태 집합을 정확하게 평가하는 데 비용을 들이지 않고 특별한 관심 영역을 정확히 평가 가능 (8장에서 살펴볼 내용)</li>
        </ul>
      </li>
      <li>Markov 속성 위반으로 인한 피해가 적음 (이후에 다룰 내용)
        <ul>
          <li>이는 후속 상태의 가치 추정치를 기반으로 가치 추정치를 업데이트 하지 않기 때문 (부트스트랩을 하지 않기 때문)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>몬테카를로의 제어 방법 설계
    <ul>
      <li>이 장에서는 일반화된 정책 반복 (Generalized Policy Iteration, GPI) 의 전체 스키마를 따랐음.
        <ul>
          <li>GPI는 정책 평가 및 정책 개선의 상호 작용 프로세스를 포함함.</li>
          <li>몬테카를로 방식은 정책 평가 프로세스를 대체함
            <ul>
              <li>모델을 사용하여 각 상태의 가치를 계산하는 대신, 상태에서 시작되는 많은 샘플의 리턴값 평균을 구함.</li>
            </ul>
          </li>
          <li>제어 방법에서 행동가치함수를 근사화 하는데 집중함. (환경의 전환 역학 모델을 요구하지 않고 정책을 개선하는데 사용 가능)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>충분한 탐색을 유지하기 위한 몬테카를로 제어 방법의 문제
    <ul>
      <li>현재 최선이라고 추정되는 행동만을 선택하는 것으로는 다른 행동을 선택했을 때의 리턴값을 알 수 없고 실제로 더 나은 조치를 학습할 수 없게 됨.</li>
      <li>한가지 접근 방식은 에피소드가 모든 가능성을 다루기 위해 무작위로 선택된 상태-행동 쌍으로 시작한다고 가정하여 이 문제를 무시하는 것
        <ul>
          <li>탐색 시작은 시뮬레이션된 에피소드가 있는 응용프로그램에서 사용될 수 있지만, 실제 경험에서 학습할 가능성은 낮음.</li>
        </ul>
      </li>
      <li>On-Policy 방식
        <ul>
          <li>에이전트는 항상 탐색하고, 탐색이 포함된 최상의 정책을 찾으려고 노력함.</li>
        </ul>
      </li>
      <li>Off-Policy 방식
        <ul>
          <li>에이전트는 탐색하지만 탐색 정책과 관련이 없을 수 있는 결정론적 최적 정책을 학습함.
            <ul>
              <li>중요도 샘플링의 일부 형태를 기반으로 함.</li>
              <li>두 정책에서 관찰된 행동을 취할 확률의 비율로 구해, 리턴값에 가중치를 부여하여 행동정책에서 대상정책으로 기대치를 변환함.</li>
            </ul>
          </li>
          <li>일반 중요도 샘플링 : 가중 수익율의 단순 평균을 사용
            <ul>
              <li>편향되지 않은 추정치를 생성하지만 분산이 더 크고 무한할 수 있음</li>
            </ul>
          </li>
          <li>가중 중요도 샘플링 : 가중 평균을 사용
            <ul>
              <li>유한한 분산을 가짐</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>개념적 단순성에도 불구하고 예측 및 제어 모두에 대한 정책 외 몬테카를로 방법은 여전히 불안정하며 지속적인 연구 대상임</li>
    </ul>
  </li>
  <li>몬테카를로 방식과 DP 방식의 차이점
    <ul>
      <li>샘플 경험을 기반으로 작동하므로 모델 없이 직접 학습에 사용할 수 있음</li>
      <li>부트스트랩을 수행하지 않음 (다른 가치 추정치를 기반으로 가치 추정치를 업데이트하지 않음)</li>
      <li>다음 장에서는 몬테카를로 방법과 같이 경험에서 배우는 방법과 DP 방법과 같은 부트스트랩 방법을 고려함.</li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Monte Carlo Methods" /><summary type="html"><![CDATA[관련 자료 (RLbook2018 Pages 91-104)]]></summary></entry><entry><title type="html">Sample-based Learning Methods - 00. 강좌소개</title><link href="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_00_About-this-course/" rel="alternate" type="text/html" title="Sample-based Learning Methods - 00. 강좌소개" /><published>2023-06-09T14:00:00+09:00</published><updated>2023-06-09T14:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_00_About%20this%20course</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Sample_based_Learning_Methods_00_About-this-course/"><![CDATA[<h1 id="강좌에-대한-설명">강좌에 대한 설명</h1>

<p><img src="/assets/images/posts/coursera_sampleBasedLearningMethods.png.png" alt="강좌에 대한 설명" /></p>

<ul>
  <li>환경과의 trial and error 상호작용을 통한 최적정책</li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html"><![CDATA[강좌에 대한 설명]]></summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 04. Week 4. Dynamic Programming</title><link href="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_04_Week4/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 04. Week 4. Dynamic Programming" /><published>2023-04-20T15:00:00+09:00</published><updated>2023-04-20T15:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_04_Week4</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_04_Week4/"><![CDATA[<h2 id="관련-자료-rlbook2018-pages-73-88">관련 자료 (RLbook2018 Pages 73-88)</h2>

<ul>
  <li>Dynamic Programming
    <ul>
      <li>동적 프로그래밍이란?
        <ul>
          <li>Markov decision process (MDP) 형태의 완벽한 환경 모델이 제공될 때 최적 정책을 계산하기 위한 알고리즘의 집합</li>
          <li>완벽한 모델이라는 가정과 비싼 컴퓨팅 비용 때문에 전통적 동적 프로그램 (DP) 은 강화학습에서 활용성이 떨어짐</li>
          <li>하지만 이론적으로 여전히 중요함 (필수적인 기초 지식)
            <ul>
              <li>모든 방식들이 위의 2가지 제약을 벗어나 DP (Dynamic Programming) 와 동일한 효과를 내는 것을 시도하는 것이라 볼 수 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>주로 사용하는 동적 프로그래밍의 환경 조건
        <ul>
          <li>a finite MDP (유한 MDP)
            <ul>
              <li>state sets $S$, action sets $A$, reward sets $R$ 은 유한함</li>
              <li>환경의 역학 (dynamics) 은 확률의 집합 $p(s’,r|s,a)$ 로 제공됨 ($s \in S, a \in A(s), r \in R, s’ \in S^+$)
                <ul>
                  <li>$S^+$ 는 $S$ 에 terminal state 를 포함한 것 (episodic task 일 경우)</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>비록 DP 가 continuous state, action spaces 에서도 적용할 수 있다 해도 그것은 특이 케이스인 경우 뿐임.
            <ul>
              <li>continuous state 에서 DP를 적용하는 통상의 방법은 state 와 action을 quantize (근사) 하여 유한 상태의 DP 방식을 적용하는 것이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>DP, 그리고 일반적인 강화학습의 핵심 아이디어는 가치 함수를 사용하여 좋은 정책을 찾기 위한 구조화를 하는 것이다.
        <ul>
          <li>우리는 최적 정책을 벨만 최적 방정식을 만족하는 최적 가치함수를 구함으로서 쉽게 찾을 수 있다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_0_1_bellman_optimality_equations.png" alt="4_0_1_bellman_optimality_equations" /></p>

        <ul>
          <li>for all $s \in S$, $a \in A(s)$, $s’ \in S^+$</li>
          <li>DP 알고리즘은 벨만 방정식을 원하는 가치함수 (최적가치함수) 의 근사값을 개선하는 업데이트 규칙으로 전환한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Policy Evaluation (Prediction)
    <ul>
      <li>첫번째로 임의의 정책 $\pi$ 의 상태 가치 함수 $v_{\pi}$ 를 계산하는 방법을 고려한다.
        <ul>
          <li>이것을 DP (Dynamic Programming) 용어로 정책 평가 (Policy evaluation) 이라 한다.
  <img src="/assets/images/posts/4_1_1_state_value_bellman_equation.png" alt="4_1_1_state_value_bellman_equation" /></li>
          <li>$v_\pi$ 의 고유성은 $\gamma &lt; 1$ 혹은 정착 $\pi$ 아래 모든 상태에서 필연적인 종료 (termination) 가 보장될 경우 보장된다.</li>
          <li>만일 환경의 역학을 완전히 알 경우 (4.4) 는 $|S|$ 개의 미지수를 가진 선형 방정식으로 풀릴 수 있다.
            <ul>
              <li>이는 직관적이지만 지루한 연산 과정이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>우리의 목적을 달성하기 위해 반복 솔루션 방법이 가장 적합하다.
        <ul>
          <li>$S^+$ 와 $R$ (실제 숫자 값) 을 매핑하는 근사 가치함수 $v_0, v_1, v_2, …$ 를 가정한다.</li>
          <li>최초의 근사, $v_0$ 는 임의로 선택 (단, 최종 상태가 있는 경우 0을 지정)</li>
          <li>각 연속적인 근사치는 벨만 방정식의 업데이트 규칙을 사용하여 얻음.
  <img src="/assets/images/posts/4_1_2_bellman_equation_update_rule.png" alt="4_1_2_bellman_equation_update_rule" /></li>
          <li>$v_\pi$ 에 대한 벨만 방정식이 등치를 보장하기 때문에, 모든 $s \in S$ 에 대해 $v_k = v_\pi$ 는 위 업데이트 규칙의 고정점이다.</li>
          <li>사실 시퀀스 $\lbrace v_k \rbrace$ 는 일반적으로 $k \to \infty$ 에 따라 $v_\pi$ 로 수렴한다.</li>
          <li>위 알고리즘을 반복 정책 평가 (iterative policy evaluation) 라 한다.</li>
        </ul>
      </li>
      <li>iterative policy evaluation
        <ul>
          <li>expected update
            <ul>
              <li>$v_k$ 로부터 $v_{k+1}$ 연속 근사를 생성하기 위해, iterative policy evaluation 은 각 $s$ 상태에 같은 연산을 적용한다.</li>
              <li>평가 중인 정책의 가능한 모든 한 단계의 전환에 대하여…</li>
              <li>이전 값 $s$ 를 새로운 값으로 교체하기 위해 $s$ 의 후속상태의 이전 값과 즉각적인 보상을 이용</li>
              <li>우리는 이러한 연산을 expected update 라 한다.</li>
              <li>위의 각 반복은 모든 상태의 값을 한번 업데이트 하여 $v_k$ 에서 $v_{k+1}$ 을 생성한다.</li>
            </ul>
          </li>
          <li>several different kinds of expected updates
            <ul>
              <li>state 를 업데이트 하거나, state-action pair 를 업데이트 할 수 있음</li>
              <li>후속 상태의 추정 값이 결합되는 방법에 따라 달라짐</li>
              <li>DP 에서 사용하는 모든 업데이트 방식을 expected update 라 한다.
                <ul>
                  <li>모든 가능한 다음 상태의 추정값에 기반하기 때문 (다음 상태를 샘플링 하는 것이 아닌)</li>
                  <li>방정식이나 백업 다이어그램 등으로 표현이 가능함.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>순차 컴퓨터 프로그램으로 iterative policy evaluation 을 구현하는 방법
            <ul>
              <li>two-array version
                <ul>
                  <li>두 개의 array 를 사용 (old values $v_k (s)$, new values $v_{k+1} (s)$)</li>
                  <li>두 개의 array 를 사용함으로써, 이전 값의 변경 없이 새로운 값을 계산할 수 있음.</li>
                </ul>
              </li>
              <li>in-place algorithm
                <ul>
                  <li>하나의 array 를 사용하여 제자리에서 값을 업데이트 한다.</li>
                  <li>새로운 값이 즉각적으로 이전 값을 덮어 쓴다.</li>
                  <li>각 상태의 업데이트 순서에 따라, 이전 값이 아닌 새로운 값을 업데이트에 쓰기도 한다.</li>
                  <li>이 알고리즘 또한 $v_\pi$ 로 수렴한다.
                    <ul>
                      <li>two-array version 보다 더 빠르게 수렴한다.</li>
                      <li>사용 가능한 새로운 데이터를 즉각 사용하기 때문</li>
                      <li>업데이트 되는 상태의 순서가 수렴율에 큰 영향을 끼친다.</li>
                    </ul>
                  </li>
                  <li>DP 알고리즘에서의 알고리즘은 주로 in-place version 이라 생각하면 된다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>in-place version of iterative policy evaluation (pseudocode)
  <img src="/assets/images/posts/4_1_3_iterative_policy_evaluation.png" alt="4_1_3_iterative_policy_evaluation" />
            <ul>
              <li>종료 상태를 관리함 (수렴은 극한 값에서 이루어지지만, 이보다 짧아야 함.)</li>
              <li>의사코드는 $\max_{s \in S} | v_{k+1} (s) - v_k (s) |$ 의 값을 매 sweep 마다 체크하고, 값이 충분히 작아지면 중지한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>예제 4.1 : 4 x 4 gridworld
  <img src="/assets/images/posts/4_1_4_example_4_1_gridworld.png" alt="4_1_4_example_4_1_gridworld" />
        <ul>
          <li>종료상태 미포함 상태 $S= { 1,2,…,14 }$</li>
          <li>각 상태에서 가능한 행동 $A = { up, down, right, left }$</li>
          <li>행동에 따라 결정론적으로 상태의 전이가 일어남 (그리드를 벗어나는 행동에 대해서는 상태가 변하지 않음)
            <ul>
              <li>$p(6,-1 | 5,right) = 1$</li>
              <li>$p(7,-1 | 7,right) = 1$</li>
              <li>$p(10,r | 5,right) = 0$</li>
            </ul>
          </li>
          <li>이것은 할인이 없는, episodic task 이다.</li>
          <li>모든 전이에 대한 보상은 -1 이며, terminal state 에 닿을 때까지 지속된다.</li>
          <li>terminal state 는 해당 그림에서 음영으로 표시된 상태이다.</li>
          <li>모든 상태 $s, s’$ 와 행동 $a$ 에 대한 기대 보상 함수는 $r(s,a,s’) = -1$ 이다.</li>
          <li>에이전트가 equprobable random policy (모든 action 이 동등 확률) 을 따른다고 가정한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Policy Improvement</p>

    <ul>
      <li>
        <p>예제 4.1 gridworld 의 iterative policy evaluation 의 수렴
  <img src="/assets/images/posts/4_2_1_example_4_1_gridworld_convergence.png" alt="4_2_1_example_4_1_gridworld_convergence" /></p>
      </li>
      <li>정책의 변경
        <ul>
          <li>임의의 결정론적 정책 $\pi$ 에 대한 가치함수 $v_\pi$ 를 결정했다고 가정할 경우,</li>
          <li>일부 상태의 경우 결정론적으로 행동 $a \neq \pi (s)$ 인 a 를 선택하도록 정책을 변경해야 하는지 여부를 알고 싶음</li>
          <li>우리는 $s$ 상태에서 현 정책을 따르는 것이 얼마나 좋은지 ($v_\pi (s)$) 를 알고 있지만, 새 정책으로 변경하는 것이 더 나은지 알고 싶음</li>
          <li>위 질문에 대한 하나의 해법은 $s$ 상태에서 $a$ 를 선택하고, 그 뒤 기존 정책 $\pi$ 를 따라보는 것이다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_2_2_q_pi_s_a.png" alt="4_2_2_q_pi_s_a" /></p>

        <ul>
          <li>그 값은 위와 같다.</li>
          <li>위의 값이 $v_\pi (s)$ 보다 큰지 작은지가 중요하다.
            <ul>
              <li>클 경우, $s$ 에서 $a$ 를 선택한 뒤 $\pi$ 정책을 따르는 것이 $\pi$ 를 계속 따르는 것보다 낫다는 뜻임.</li>
              <li>즉, $s$ 에서 $a$ 를 선택하는 정책이 $\pi$ 정책보다 더 나은 정책임.</li>
              <li>이것을 정책 개선 정리 (policy improvement theorem) 라 한다.</li>
            </ul>
          </li>
          <li>$\pi$ 와 $\pi’$ 를 결정론적 정책이라 가정, $\textrm{all s} \in S$ 인 경우</li>
        </ul>

        <p><img src="/assets/images/posts/4_2_3_policy_improvement_q_pi.png" alt="4_2_3_policy_improvement_q_pi" /></p>
        <ul>
          <li>정책 $\pi’$ 는 $\pi$ 와 같거나 보다 좋은 정책이다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_2_4_policy_improvement_v_pi.png" alt="4_2_4_policy_improvement_v_pi" /></p>
        <ul>
          <li>모든 상태에서 위 4.7이 성립할 경우 4.8 또한 성립한다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_2_5_policy_improvement_reapplying.png" alt="4_2_5_policy_improvement_reapplying" /></p>
        <ul>
          <li>위와 같이 하나의 상태에서 특정 행동을 변경하는 정책을 어떻게 평가하는지 알아봤음.</li>
          <li>이 방식을 모든 상태에 모든 선택가능한 행동에 적용하는 것으로 확장하는 것은 자연스러운 방법이다.</li>
          <li>즉, 새로운 탐욕 정책 $\pi’$ 를 고려해본다.</li>
        </ul>
      </li>
      <li>
        <p>Policy Improvement</p>

        <p><img src="/assets/images/posts/4_2_6_new_greedy_policy_pi_quote.png" alt="4_2_6_new_greedy_policy_pi_quote" /></p>
        <ul>
          <li>$\arg\max_a$ 는 뒤의 식이 최대값이 되는 $a$ 를 뜻한다. (동률일 경우 임의로 선택)</li>
          <li>greedy policy 는 $v_\pi$ 하의 단기적으로 가장 좋아보이는 행동을 선택한다. (한 번의 스텝 진행만을 고려)</li>
          <li>greedy policy 는 policy improvement theorm (4.7) 의 조건을 만족하므로, 기존 정책과 같거나 더 좋은 정책이라는 것을 알 수 있다.</li>
          <li>
            <p>기존 정책의 가치 함수를 사용하여 기존의 정책보다 더 나은 정책을 만드는 것을 policy improvement 라 한다.</p>
          </li>
          <li>new greedy policy $\pi’$ 가 old policy $\pi$ 만큼 좋지만, 더 좋지는 않은 상태를 가정한다.
            <ul>
              <li>for $\textrm{all s} \in S$</li>
              <li>$v_\pi = v_{\pi’}$</li>
              <li>4.9 의 식을 따름
  <img src="/assets/images/posts/4_2_7_bellman_optimality_equation.png" alt="4_2_7_bellman_optimality_equation" /></li>
            </ul>
          </li>
          <li>결국 위의 식은 Bellman optimality equation 이 된다.</li>
          <li>따라서 $v_{\pi’}$ 는 $v_*$ 와 같고 $\pi$ 와 $\pi’$ 는 최적 정책 (optimal policy) 이 된다.</li>
          <li>정책 개선 (Policy Improvement) 는 기존 정책이 이미 최적이 아닌 경우, 엄격하게 더 나은 정책을 제공한다.</li>
        </ul>
      </li>
      <li>확률론적 정책 (stochastic policy) 의 경우
        <ul>
          <li>지금까지 결정론적 정책 (deterministic policy) 의 경우를 고려하였음.</li>
          <li>일반적인 경우 확률론적 정책 (stochastic policy) $\pi$ 는 확률에 특화되어 있다.</li>
          <li>$\pi (a|s)$ : $s$ 상태에서 $a$ 행동을 선택할 확률</li>
          <li>policy improvement theorem 은 사실 확률론적 정책으로 쉽게 확장이 가능하다.
            <ul>
              <li>최선의 행동이 복수개일 경우 특정 확률의 배분을 차지할 수 있다.</li>
              <li>차선의 행동은 확률이 0 가 된다.</li>
              <li>(예제 4.1 gridworld 의 iterative policy evaluation 의 수렴 참조)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Policy Iteration</p>

    <ul>
      <li>Policy Iteration
        <ul>
          <li>정책 $\pi$ 에서 $v_\pi$ 를 이용해 더 나은 정책 $\pi’$ 를 도출했다면..</li>
          <li>$v_{\pi’}$ 를 계산하여 더 나은 정책 $\pi’’$ 를 도출할 수도 있다.</li>
          <li>따라서, 우리는 단조롭게 개선되는 일련의 정책과 가치 함수를 얻을 수 있다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_3_1_policy_iteration.png" alt="4_3_1_policy_iteration" /></p>

        <ul>
          <li>$\overset{E}{\rightarrow}$ 는 policy evaluation 을,</li>
          <li>$\overset{I}{\rightarrow}$ 는 policy improvement 를 뜻한다.</li>
          <li>각 정책은 이전의 정책보다 엄격히 개선되었다는 것을 보장한다. (이미 최적이 아닌 경우)</li>
          <li>유한 MDP 는 유한한 수의 정책을 가지고 있기 때문에, 이 프로세스는 반드시 최적 정책과 최적 가치함수에 유한한 반복 진행으로 수렴한다.</li>
          <li>위의 방법으로 최적 정책을 찾는 방법을 policy iteration 이라 한다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_3_2_policy_iteration_algorithm.png" alt="4_3_2_policy_iteration_algorithm" /></p>

        <ul>
          <li>반복 계산인 각 정책 평가 (policy evaluation) 은 이전 정책에 대한 가치 함수로 시작된다.</li>
          <li>이것은 수렴의 속도를 크게 향상시키는데, 이전 정책에서 다음 정책으로 변화 할때, 가치 함수가 크게 바뀌지 않기 때문으로 추정된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>예제 4.2 : Jack’s Car Rental
    <ul>
      <li>문제의 설정
        <ul>
          <li>Jack 은 렌터카 회사의 두 지점을 관리한다.</li>
          <li>매일 일정 수의 고객이 자동차를 렌트하기 위해 각 지점에 도착한다.
            <ul>
              <li>사용할 수 있는 자동차가 있으면 그것을 임대하고 $10 을 정립한다.</li>
              <li>지점에 차가 없으면 사업을 잃게 됨</li>
            </ul>
          </li>
          <li>차량은 반납한 다음날부터 대여가 가능함</li>
          <li>차량이 필요한 곳에서 사용될 수 있도록, 두 지점간 차량을 밤 사이에 이동시킬 수 있고, 차량 당 $2 가 든다.
            <ul>
              <li>한 위치에서 차량을 최대 5대 까지 이동할 수 있음</li>
            </ul>
          </li>
          <li>각 위치에서 요청되고 반환되는 자동차의 수는 푸아송 확률 변수라고 가정한다.
            <ul>
              <li>즉, 수량이 $n$ 일 확률은 $\frac{n!}{\lambda^n} e^{-\lambda}$ 이며, $\lambda$ 는 예상치이다.</li>
              <li>$\lambda$ 를, 첫 번째 위치와 두 번째 위치 각각, 3과 4 (렌탈 수량), 3과 2 (반납수량) 으로 가정한다.</li>
            </ul>
          </li>
          <li>각 지점에 차량이 최대 20대까지 있을 수 있음 (추가되는 차량은 사라짐)</li>
          <li>할인율 $\gamma = 0.9$
            <ul>
              <li>할인율을 이용해 연속적인 유한 MDP 로 가정</li>
            </ul>
          </li>
          <li>하나의 step 기준은 하루</li>
          <li>States 는 하루가 끝날 때 각 위치의 자동차 수</li>
          <li>Actions 는 숫자 (밤 새 두 장소 사이를 오가는 차량의 수)</li>
        </ul>
      </li>
      <li>하기 이미지는 차를 전혀 움직이지 않는 정책부터 시작하여, 정책 반복으로 찾은 정책의 순서를 보여준다.</li>
    </ul>

    <p><img src="/assets/images/posts/4_3_3_Example_4_2_Jacks_Car_rental.png" alt="4_3_3_Example_4_2_Jacks_Car_rental" /></p>

    <ul>
      <li>Jack 의 자동차 임대 문제에 대한 정책 반복으로 찾은 정책의 순서와 최종 상태가치 함수</li>
      <li>각 상태에 대해 몇 대의 차를 보내야 하는지를 보여줌 (음수는 두번째 위치에서 첫 번째 위치로의 이동을 나타냄)</li>
      <li>각 정책은 이전 정책에서의 엄격한 개선을 한 것이며, 마지막 정책이 최적 정책임.</li>
    </ul>
  </li>
  <li>Value Iteration
    <ul>
      <li>정책 반복의 문제점
        <ul>
          <li>정책 반복의 한 가지 단점은, 각 반복에 정책 평가가 포함된다는 점이다.</li>
          <li>정책 평가는 상태 세트를 여러 번 스윕하는 장 기간의 반복 계산일 수 있다.</li>
          <li>반복적인 정책 평가를 하면 $v_\pi$ 로의 수렴은 극한에서만 일어난다.</li>
          <li>그림 4.1 의 예는 정책 평가를 생략하는 것이 가능할 수 있음을 시사한다. (3번의 반복 이후에 greedy policy 에 영향을 주지 않음.)</li>
        </ul>
      </li>
      <li>가치 반복 (Value Iteration)
        <ul>
          <li>정책 반복 (Policy iteration) 의 정책 평가 (Policy evaluation) 단계는 정책 반복의 수렴 보장을 유지한 채 여러 가지 방법으로 축소될 수 있음.</li>
          <li>한 가지 중요한 특수 사례는 정책 평가 (Policy evaluation) 가 단 한번의 스윕 (각 상태의 업데이트 1회) 후에 중지되는 경우임.</li>
          <li>위와 같은 사례 (알고리즘) 를 Value Iteration 이라 한다.</li>
          <li>정책 개선 (Policy improvement) 과 축소된 정책 평가 (Policy evaluation) 는 단순한 업데이트 연산으로 결합할 수 있다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_4_1_value_iteration_1.png" alt="4_4_1_value_iteration_1" /></p>

        <ul>
          <li>정책 평가와 같이, Value Iteration 도 정확한 $v_*$ 로의 수렴을 위해 무한히 반복을 해야 한다.</li>
          <li>실제로는, 가치 함수가 한 번의 스윕에서 매우 적은 양만 변화할 경우 중지한다.</li>
          <li>아래의 박스는 완성된 알고리즘 (중지 상태를 포함) 을 보여준다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_4_2_value_iteration_2.png" alt="4_4_2_value_iteration_2" /></p>

        <ul>
          <li>가치 반복 (Value Iteration) 은 각 스윕에서 한 번의 정책 평가 스윕과 한 번의 정책 개선 스윕을 효과적으로 결합한다.</li>
          <li>종종 각 정책 개선 (Policy Improvement) 스윕 사이에 여러 정책 평가 (Policy evaluation) 스윕을 삽입하여 달성된다.</li>
          <li>일반적으로 축소된 정책 반복 알고리즘의 전체 클래스는 일부는 정책 평가 (Policy evaluation) 업데이트, 일부는 가치 반복 (Value Iteration) 업데이트 를 사용하는 스윕 시퀀스로 생각할 수 있다.
            <ul>
              <li>4.10 수식의 최대 (max) 연산이 유일한 차이점이기 때문</li>
              <li>즉, 정책 평가의 일부 스윕에 최대 (max) 연산 작업이 추가됨을 의미함</li>
            </ul>
          </li>
          <li>이러한 모든 알고리즘은 할인된 유한 MDP 에 대한 최적의 정책으로 수렴함.</li>
        </ul>
      </li>
      <li>예제 4.3 : Gambler’s Problem
        <ul>
          <li>문제
            <ul>
              <li>도박꾼은 일련의 동전 던지기 결과에 대해 베팅할 기회가 있다.</li>
              <li>동전이 앞면이 나오면 그는 걸었던 만큼의 달러를 얻고, 뒷면이면 지분을 잃는다.</li>
              <li>도박꾼이 $100 의 목표에 도달하면 이기고, 돈을 다 잃으면 끝나게 된다.</li>
              <li>매번 도박꾼은 자신의 자본 중 어느 정도를 걸어야 할 지 정해야 한다.</li>
              <li>이 문제는 할인되지 않은, 에피소딕한 유한한 MDP 로 공식화될 수 있다.</li>
            </ul>
          </li>
          <li>정의
            <ul>
              <li>상태 : 도박꾼의 자본, $s \in \lbrace 1,2,…,99 \rbrace$</li>
              <li>행동 : 베팅 금액, $a \in \lbrace 0, 1, …, min(s, 100-s) \rbrace$</li>
              <li>보상 : 도박꾼이 목표에 도달하였을때 +1, 그 외 모든 전환에서 0</li>
              <li>상태가치 함수는 위의 경우 각 상태에서 승리할 확률을 제공한다.</li>
              <li>정책은 자본과 베팅 금액 간 매핑이다.</li>
              <li>최적의 정책은 목표에 도달할 확률을 최대화한다.</li>
              <li>$p_h$ : 동전이 앞면이 나올 확률</li>
              <li>$p_h$ 를 알게 되면, 전체 문제를 알 수 있고, 가치 반복 (Value Iteration) 등으로 풀 수 있다.</li>
              <li>그림 4.3 은 가치 반복의 연속적인 스윕에 대한 가치 함수의 변화를 보여준다.</li>
              <li>그림 4.3 은 $p_h = 0.4$ 인 경우 최적 정책을 보여준다. 이 정책은 최적이지만 고유하지는 않다.
                <ul>
                  <li>최적 가치 함수 상 값이 동률일 경우…</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/4_4_3_example_4_3_gamblers_problem.png" alt="4_4_3_example_4_3_gamblers_problem" /></p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Asynchronous Dynamic Programming
    <ul>
      <li>기존 DP 모델의 한계
        <ul>
          <li>MDP 의 전체 상태 집합에 대한 연산을 포함한다.</li>
          <li>즉, 상태 집합 전체에 대한 스윕이 필요함.</li>
          <li>상태 세트가 매우 크면 한 번의 스윕도 엄청난 비용일 수 있다.</li>
          <li>백게먼 게임에서 상태의 수는 $10^{20}$ 개 이다.</li>
          <li>이는 초당 백만 개의 상태의 가치 반복 업데이트를 수행할 수 있더라도, 단일 스윕을 완료하는 데 천 년 이상이 걸린다.</li>
        </ul>
      </li>
      <li>비동기식 DP 알고리즘
        <ul>
          <li>비동기식 DP 알고리즘은 상태 세트의 체계적 스윕이 구성되지 않은 내부 반복 DP 알고리즘이다.</li>
          <li>이 알고리즘은 사용 가능한 다른 상태의 값을 사용하여 순서에 관계없이 상태 값을 업데이트한다.
            <ul>
              <li>일부 상태 값은 다른 상태 값이 업데이트 되기 전에 여러 번 업데이트 될 수 있다.</li>
              <li>그러나 올바른 수렴을 위해 모든 상태의 값을 계속 업데이트 해야한다.</li>
              <li>일정 시점 이후에는 어떤 상태도 무시할 수 없다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>비동기식 DP 알고리즘의 이점
        <ul>
          <li>비동기 DP 알고리즘은 업데이트할 상태를 선택하는 데 큰 유연성을 제공한다.
            <ul>
              <li>예를 들어 가치 반복 업데이트 (4.10) 를 이용하여, 매 스탭 $k$ 에 대해 하나의 상태인 $s_k$ 에 대해서만 업데이트 하는 것이다.</li>
              <li>만약, $0 \leq \gamma &lt; 1$ 일 경우, $v_*$ 에 대한 점근적인 수렴은 모든 상태가 시퀀스 $\lbrace s_k \rbrace$ 에서 무한한 횟수로 발생하는 경우에만 보장된다.
                <ul>
                  <li>시퀀스가 확률적일 수도 있음</li>
                  <li>할인되지 않는 에피소드 케이스의 경우, 수렴되지 않는 일부 업데이트 순서가 있을 수 있지만 이는 상대적으로 피하기 쉬움)</li>
                </ul>
              </li>
              <li>마찬가지로 정책 평가와 가치 반복 업데이트를 혼합하여 일종의 비동기식 축소된 정책 반복을 생성할 수 있음</li>
              <li>이것과 다른 더 특이한 DP 알고리즘에 대한 세부 사항은 이 책의 범위를 벗어남</li>
              <li>몇 가지 다른 업데이트가 다양한 Sweepless DP 알고리즘에서 유연하게 사용할 수 있는 빌딩 블록을 형성한다는 것은 분명함</li>
              <li>스윕을 피한다고 해서 반드시 적은 계산으로 벗어날 수 있다는 의미는 아님.
                <ul>
                  <li>이는 알고리즘이 정책을 개선하기 전에 절망적으로 긴 스윕에 갇힐 필요가 없음을 의미함</li>
                </ul>
              </li>
              <li>알고리즘의 진행률을 향상시키기 위해 업데이트를 적용할 상태를 선택하여 이러한 유연성을 활용할 수 있음.
                <ul>
                  <li>값 정보가 상태에서 상태로 효율적으로 전파되도록 업데이트 순서를 정할 수 있음</li>
                  <li>일부 상태는 다른 상태만큼 자주 값을 업데이트할 필요가 없을 수 있음</li>
                  <li>최적의 동작과 관련이 없는 경우 일부 상태를 완전히 업데이트 하지 않을 수도 있음</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>비동기식 알고리즘을 사용하여 실시간 상호작용 내용을 더 쉽게 혼합하여 계산할 수 있다.
            <ul>
              <li>MDP 를 풀기 위해 에이전트가 실제로 MDP 를 경험하는 동안, 동시에 반복 DP 알고리즘을 수행할 수 있다.</li>
              <li>에이전트의 경험은 DP 알고리즘이 업데이트를 적용할 상태를 결정하는 데 사용할 수 있다.</li>
              <li>동시에 DP 알고리즘의 최신 값과 정책 정보를 이용해 에이전트의 의사 결정에 활용할 수 있다.
                <ul>
                  <li>예를 들어 에이전트가 특정 상태를 방문할 때 상태에 업데이트를 적용할 수 있다.</li>
                  <li>이를 통해 에이전트와 가장 관련성 높은 상태 세트 부분의 업데이트에 집중할 수 있다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Generalized Policy Iteration
    <ul>
      <li>정책 반복 (Policy Iteration) 에 대한 내용 정리
        <ul>
          <li>정책 반복은 두 개의 동시 상호 작용 프로세스로 구성됨
            <ul>
              <li>현재 정책과 일치하는 가치 함수를 만들기 (정책 평가)</li>
              <li>현재 가치함수와 관련하여 정책을 탐욕스럽게 만들기 (정책 개선)</li>
            </ul>
          </li>
          <li>정책 반복 : 이 두 프로세스는 서로 번갈아가며 다른 프로세스가 시작되기 전에 완료됨</li>
          <li>가치 반복 : 각 정책 개선 사이에 정책 평가의 단일 반복만 수행됨</li>
          <li>비동기식 DP 방법 : 평가 및 개선 프로세스가 훨씬 더 세밀하게 끼워넣어짐
            <ul>
              <li>예 : 다른 프로세스로 돌아가기 전에 한 프로세스에서 단일 상태가 업데이트됨</li>
            </ul>
          </li>
          <li>두 프로세스가 계속해서 모든 상태를 업데이트 하는 한, 궁극적인 결과는 일반적으로 최적 가치 함수와 최적 정책으로 수렴됨.</li>
        </ul>
      </li>
      <li>Generalized Policy Iteration (GPI) : 일반화된 정책 반복
        <ul>
          <li>우리는 일반화된 정책 반복 (GPI) 이라는 용어를 사용하여 정책 평가 및 정책 개선 과정이 서로 상호작용하는 일반 아이디어를 나타낸다.
            <ul>
              <li>두 과정의 세부 사항과 세분화와는 독립적인 개념임.</li>
            </ul>
          </li>
          <li>거의 모든 강화학습 방법은 GPI로 잘 설명되어 있음.</li>
        </ul>

        <p><img src="/assets/images/posts/4_4_4_gpi_diagram.png" alt="4_4_4_gpi_diagram" /></p>

        <ul>
          <li>즉, 모든 것들은 식별 가능한 정책과 가치 함수를 가지며, 정책은 항상 가치 함수에 대해 개선되고, 가치 함수는 항삭 정책에 대한 가치 함수로 향하도록 조정된다.</li>
          <li>만약 평가 과정과 개선 과정이 모두 안정화 되면, 즉 더 이상 변화가 발생하지 않으면, 가치 함수와 정책은 최적이어야 한다.</li>
          <li>가치 함수는 현 정책과 일치할 때에만 안정화 되고, 정책은 현 가치함수에 대해 탐욕스러울 경우에만 안정화 된다.</li>
          <li>따라서 두 과정이 모두 안정화되려면, 자신의 평가 함수에 대해 탐욕스러운 정책을 찾아야 한다.</li>
          <li>이는 벨만 최적 방정식 (4.1) 이 성립하며, 정책과 가치함수가 최적임을 의미한다.</li>
        </ul>

        <p><img src="/assets/images/posts/4_4_5_gpi_diagram_2.png" alt="4_4_5_gpi_diagram_2" /></p>

        <ul>
          <li>GPI 의 평가 및 개선 프로세스는 경쟁과 협력으로 볼 수 있음
            <ul>
              <li>정책을 가치 함수에 대해 탐욕스럽게 만들면 일반적으로 가치 함수가 변경된 정책에 대해 부정확해짐</li>
              <li>가치 함수를 정책과 일치하게 만들면 해당 정책이 더 이상 탐욕스러운 정책이 아니게 됨.</li>
              <li>장기적으로 이 두 과정은 하나의 공통해결책인 최적의 가치 함수와 최적의 정책을 찾기 위해 상호작용함.</li>
            </ul>
          </li>
          <li>위 다이어그램에서 제안된 대로 2차원 공간의 두 개의 선으로 생각해볼 수 있음.
            <ul>
              <li>실제 기하학은 이보다 훨씬 복잡하나, 이 다이어그램은 실제 상황이 어떤 식으로 일어나는지를 시사함.</li>
              <li>각 과정은 가치함수, 정책을 두고 두 목표중 하나의 해결책을 나타내는 선으로 이끈다.</li>
              <li>두 목표는 직교하지 않기 때문에 목표 간 상호작용이 발생 (한 목표로 직접적으로 나아가면 다른 목표에서 멀어지는 움직임)</li>
              <li>그러나 결국 공동 프로세스는 최적의 목표에 가까워지게 된다.</li>
              <li>GPI 에서 각 목표에 대해 작고 불완전한 단계를 취할 수도 있으나, 어느 경우에도 두 과정은 전반적인 최적의 목표를 달성하기 위해 함께 작동한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Efficiency of Dynamic Programming
    <ul>
      <li>MDP 문제에서 DP 방식이 가지는 효율성
        <ul>
          <li>DP(Dynamic Programming)는 매우 큰 문제에는 적합하지 않을 수 있지만, MDP(Markov Decision Processes)를 해결하기 위한 다른 방법과 비교하면 DP 방법은 실제로 꽤 효율적임.</li>
          <li>일부 기술적인 세부 사항을 무시한다면, DP 방법이 최적 정책을 찾는 데 걸리는 (최악의 경우) 시간은 상태와 액션의 수에 다항식으로 표현됨.
            <ul>
              <li>n과 k가 상태와 액션의 수를 나타낸다면, DP 방법은 n과 k의 다항식 함수보다 적은 계산 작업을 필요로 함.</li>
              <li>DP 방법은 전체 정책의 수가 $k^n$ 인 것과는 상관없이 다항 시간 내에 최적 정책을 찾을 것을 보장함.</li>
              <li>(직접 탐색 (모든 경우의 수, 가능한 정책을 검사하고 평가) 은 동일한 보장을 제공하기 위해 각 정책을 철저히 검사해야 함)</li>
            </ul>
          </li>
          <li>일부의 경우 선형 프로그래밍 방법 (문제를 선형 제약 조건과 목적함수를 활용해 수학적으로 리모델링) 도 MDP를 해결하는 데 사용될 수 있으며, DP 방법보다 수렴 보장이 더 좋을 수 있음.
            <ul>
              <li>예를 들어, MDP 문제에서 상태 공간이 크고 액션 수가 상대적으로 작은 경우</li>
              <li>혹은 작은 상태 수</li>
            </ul>
          </li>
          <li>그러나 가장 큰 문제에 대해서는 DP 방법만이 실현 가능함.
            <ul>
              <li>DP(Dynamic Programming)은 종종 차원의 저주(curse of dimensionality)라는 이유로 적용 가능성이 제한적으로 생각되기도 한다. 이는 상태 변수의 수에 따라 상태의 수가 지수적으로 증가하기 때문.</li>
              <li>큰 상태 집합은 어려움을 일으킬 수 있지만, 이는 문제의 본질적인 어려움이며 DP 자체의 해결 방법의 어려움은 아니다.</li>
              <li>실제로 DP는 직접 탐색이나 선형 프로그래밍과 같은 경쟁적 위치의 방법들보다 큰 상태 공간을 처리하는 데 더 적합함.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>비동기 DP 의 활용방식
        <ul>
          <li>실제로 DP(Dynamic Programming) 방법은 오늘날의 컴퓨터를 사용하여 수백만 개의 상태를 가진 MDP(Markov Decision Process)를 해결하는 데에 사용될 수 있다.</li>
          <li>정책 이터레이션(policy iteration)과 가치 이터레이션(value iteration) 모두 널리 사용되며, 일반적으로 어떤 것이 더 우수한지 명확하지 않음.</li>
          <li>이러한 방법들은 보통 좋은 초기 값 함수 또는 정책으로 시작되었을 때, 이론적인 최악의 경우 실행 시간보다 훨씬 빠르게 수렴함.</li>
          <li>상태 공간이 큰 문제에서는 비동기적인 DP 방법이 종종 선호됨.</li>
          <li>동기적인 방법의 경우 하나의 전체 순회를 완료하기 위해서는 모든 상태에 대한 계산과 메모리가 필요함.</li>
          <li>일부 문제에서는 심지어 이 정도의 메모리와 계산도 실용적이지 않을 수 있지만, 최적 해결 경로에는 상대적으로 적은 수의 상태가 발생할 수 있으므로 문제는 여전히 해결이 가능함.</li>
          <li>비동기적인 방법과 GPI의 다른 변형들은 이러한 경우에 적용될 수 있으며, 동기적인 방법보다 좋거나 최적의 정책을 훨씬 빠르게 찾을 수 있음.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="policy-evaluation-prediction">Policy Evaluation (Prediction)</h2>

<ul>
  <li>
    <p>Policy Evaluation vs. Control</p>

    <ul>
      <li>학습 목표
        <ul>
          <li>정책 평가 (policy evaluation) 와 제어 (control) 의 구분</li>
          <li>동적 프로그래밍 (Dynamic Programming) 이 적용될 수 있는 설정(환경) 과 제한을 이해</li>
        </ul>
      </li>
      <li>Policy Evaluation 과 Control 의 정의
        <ul>
          <li>Policy Evaluation : 주어진 정책에 대한 stable 한 가치 함수를 구하는 것 (얼마나 좋은지 평가)
            <ul>
              <li>$\pi \to v_\pi$</li>
              <li>$v_\pi (s) \doteq E_\pi [ G_t | S_t = s]$</li>
              <li>$G_t \doteq \sum_{k=0}^\infty \gamma^k R_{t+k+1}$
                <ul>
                  <li>리턴 값은 미래의 보상에 대한 할인된 합계이다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>Control : 가치 함수를 통해 가장 많은 보상을 얻는 정책을 찾는 것 (정책을 발전시키는 것)</p>

            <p><img src="/assets/images/posts/policy_control_1.png" alt="policy_control_1" /></p>

            <ul>
              <li>모든 상태에서의 가치가 같거나 더 나은 정책을 찾는 것</li>
              <li>반복적으로 찾다 보면 최적의 정책을 찾게 됨.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Dynamic Programming Algorithms</p>

        <ul>
          <li>벨만 방정식을 사용해 가치 평가와 제어의 반복적인 알고리즘을 정의하는 것</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/linear_solver_vs_dynamic_programming_1.png" alt="linear_solver_vs_dynamic_programming_1" /></p>

    <ul>
      <li>Linear equations 와 Dynamic Programming 비교
        <ul>
          <li>Linear equations
            <ul>
              <li>가치함수 $v_\pi$ 를 찾기 위해, 각 상태별로 위의 하나의 방정식을 갖게 된다.</li>
            </ul>
          </li>
          <li>Dynamic Programming
            <ul>
              <li>통상 MDP 의 문제에서 DP 방식이 보다 더 적절한 방식이다.</li>
            </ul>

            <p><img src="/assets/images/posts/dynamic_programming_1.png" alt="dynamic_programming_1" /></p>

            <ul>
              <li>DP 에서는 다양한 형태의 벨만 방정식을 사용한다.</li>
              <li>위의 경우 환경역학 $p$ 에 대한 지식을 기반으로 한다.</li>
              <li>고전적인 DP 에서는 환경과의 상호작용을 포함하지 않는다. (대신 주어진 MDP 모델을 활용 / 함수 $p$ 에 접근할 수 있다는 가정.)</li>
              <li>대부분의 강화학습 알고리즘은 모델이 없는 DP 의 근사화된 프로그래밍이라고 볼 수 있다.</li>
              <li>(이러한 특징은 이후에 소개할 Temporal different space dynamic planning algorithm 에서 두드러진다.)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Iterative Policy Evaluation</p>

    <ul>
      <li>학습목표
        <ul>
          <li>주어진 정책에서 상태값 평가를 위한 반복 정책 평가 (iterative policy evaluation) 알고리즘의 개요</li>
          <li>반복 정책 평가 (iterative policy evaluation) 를 적용하여 가치함수 계산</li>
        </ul>
      </li>
      <li>벨만 방정식과 Iterative Policy Evaluation 간의 관계
        <ul>
          <li>DP 알고리즘은 벨만 방정식을 업데이트 룰로 변경함으로써 얻을 수 있다.</li>
          <li>iterative policy evaluation 알고리즘 또한 이러한 알고리즘 중 하나이다.</li>
        </ul>

        <p><img src="/assets/images/posts/iterative_policy_evaluation_update_rule_1.png" alt="iterative_policy_evaluation_update_rule_1" /></p>

        <ul>
          <li>벨만 방정식 중 $v_\pi$ 에 대핸 재귀적 표현을 활용한다.</li>
          <li>업데이트 룰에서는 참 가치함수가 아닌 예측 값을 활용한다.
            <ul>
              <li>이 방식은 점진적으로 보다 나은 대략적인 가치 함수를 제공하게 된다.</li>
            </ul>

            <p><img src="/assets/images/posts/iterative_policy_evaluation_update_rule_2.png" alt="iterative_policy_evaluation_update_rule_2" /></p>

            <ul>
              <li>각각의 반복은 모든 상태 집합에 대해 업데이트를 적용하는데, 이를 스윕 (sweep) 이라고 한다.</li>
              <li>만약 모든 상태에 대해, 가치 함수 근사값 $v_k$ 와 $v_{k+1}$ 의 값이 같을 경우 정책에 대한 참 가치 함수를 찾았다고 한다.</li>
              <li>${v_0}$ 가 어떤 값이여도, $k$ 가 무한대에 수렴하면, $v_k$ 또한 $v_\pi$ 로 수렴하게 된다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>구현의 방식
        <ul>
          <li>
            <p>2개의 배열 사용</p>

            <p><img src="/assets/images/posts/iterative_policy_evaluation_two_array.png" alt="iterative_policy_evaluation_two_array" /></p>

            <ul>
              <li>모든 상태 세트에 대해 업데이트를 진행한다.</li>
              <li>Old value V 를 이용해 New value V’ 를 갱신한다.</li>
              <li>Old value V 는 업데이트 중에 변동이 없다.</li>
              <li>모든 상태를 순회, 업데이트 후에 V’ 를 V 에 할당하고, V’ 에 다시 업데이트를 진행한다.</li>
            </ul>
          </li>
          <li>
            <p>1개의 배열 사용</p>

            <p><img src="/assets/images/posts/iterative_policy_evaluation_one_array.png" alt="iterative_policy_evaluation_one_array" /></p>

            <ul>
              <li>V 배열만을 이용해 업데이트를 진행한다.</li>
              <li>경우에 따라 특정 상태의 값을 참조할 때, Old value 가 아닌 New value 를 참조하기도 한다.</li>
              <li>이러한 한 개의 배열 버전 또한 수렴을 보장하며, 사실 보통의 경우 최신 값을 사용하기에 더 빠르게 수렴한다.</li>
            </ul>
          </li>
          <li>
            <p>여기에서는 단순성을 위해 2개의 배열 버전에 집중한다.</p>
          </li>
        </ul>
      </li>
      <li>Iterative Policy Evaluation 의 예시
        <ul>
          <li>4 x 4 의 grid world 를 가정</li>
          <li>좌측 상단과 우측 하단에 Terminate State 가 있는 Episodic MDP 로 정의</li>
          <li>모든 상태이동의 보상은 -1</li>
          <li>할인 값은 없다고 가정 ($\gamma = 1$)</li>
          <li>각 상태별로 4개의 방향으로 이동할 수 있음 (up, down, left, right). 각 행동은 결정론적임 (확률=100%)</li>
          <li>그리도 밖으로의 이동은 에이전트가 해당 상태에 그대로 머물도록 함</li>
          <li>정책은 uniform random policy (각 확률 25%).</li>
        </ul>

        <p><img src="/assets/images/posts/iterative_policy_evaluation_example_1.png" alt="iterative_policy_evaluation_example_1" /></p>

        <ul>
          <li>스윕은 좌에서 우로, 위에서 아래로 진행된다.</li>
          <li>첫 스윕 결과는 위의 식에 의해 Terminal State 를 제외하고 모두 -1 이 된다.</li>
          <li>첫 풀 스윕 이후 V’ 을 V 로 카피하고, 위 과정을 반복한다.</li>
        </ul>

        <p><img src="/assets/images/posts/iterative_policy_evaluation_psuedo.png" alt="iterative_policy_evaluation_psuedo" /></p>

        <ul>
          <li>위는 iterative policy evaluation 의 전체 알고리즘이다.</li>
          <li>각 상태의 이전과 이후의 차이 ($\delta$) 가 정의한 작은 숫자 ($\theta$) 보다 작을 경우 루프를 중지한다.</li>
          <li>$\theta$ 가 충분히 작을 경우, V 는 $v_\pi$ 에 가까운 값이라 할 수 있다.</li>
        </ul>

        <p><img src="/assets/images/posts/iterative_policy_evaluation_example_2.png" alt="iterative_policy_evaluation_example_2" /></p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="policy-iteration-control">Policy Iteration (Control)</h2>

<ul>
  <li>
    <p>Policy Improvement</p>

    <ul>
      <li>학습목표
        <ul>
          <li>정책 개선 이론 (policy improvement theorem) 이해하기</li>
          <li>주어진 MDP 에서 더 나은 정책의 생성을 위해 정책에 가치함수 적용하기</li>
        </ul>
      </li>
      <li>
        <p>Policy Improvement</p>

        <p><img src="/assets/images/posts/policy_improvement_1.png" alt="policy_improvement_1" /></p>

        <ul>
          <li>최적 가치함수 $v_*$ 에 대해 greedy action 을 취한 정책을 최적 정책 (Optimal Policy) 이라 한다.</li>
          <li>임의의 정책 $\pi$ 를 따르는 가치 함수 $v_\pi$ 에 대해 greedy action 을 $v_\pi$ 에 대한 탐욕 정책이라 했을 때…</li>
          <li>현재의 정책 $\pi$ 와 위의 정책이 차이가 없을 경우 $\pi$ 는 이미 $v_\pi$ 에 대한 탐욕 정책이며,</li>
          <li>이 경우 $v_\pi$ 가 벨만 최적성 방정식을 따른다면, $\pi$ 는 최적 정책이다.</li>
        </ul>
      </li>
      <li>Policy Improvement Theorem
        <ul>
          <li>$\pi$ 가 최적정책이 아니라면, $\pi$ 보다 엄격한 개선이 이루어진 새로운 정책이 존재한다.</li>
          <li>$q_\pi (s, \pi’(s)) \geq q_\pi (s, \pi (s))$ for all $s \in S$  $\to \pi’ \geq \pi$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_improvement_theorem_1.png" alt="policy_improvement_theorem_1" /></p>

        <ul>
          <li>uniform random policy 를 따르는 $v_\pi$ 가치함수에 대해, greedy policy $\pi’$ 이 Policy Improvement Theorem 에 의해 더 개선된 정책임.</li>
          <li>Policy Improvement Theorem 은 새로운 정책이 이전 정책보다 개선된 정책임 만을 보장한다.
            <ul>
              <li>개선된 정책이 최적 정책임은 보장하지 않음 (가치함수가 최적 가치함수가 아님)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Policy Iteration</p>

    <ul>
      <li>학습목표
        <ul>
          <li>최적 정책을 찾기 위한 정책 반복 알고리즘 (policy iteration algorithm) 을 정의하기</li>
          <li>“the dance of policy and value” (평가와 개선을 반복하여 최적 정책을 찾는 것) 이해하기</li>
          <li>정책 반복 (policy iteration) 을 적용하여 최적 정책과 최적 가치 함수 계산하기</li>
        </ul>
      </li>
      <li>
        <p>Policy Iteration</p>

        <p><img src="/assets/images/posts/policy_iteration_1.png" alt="policy_iteration_1" /></p>

        <ul>
          <li>더 이상 가치 함수를 통한 정책이 변경되지 않으면 그것이 최적 정책이다.</li>
          <li>결정론적 정책을 사용하기에, 필연적으로 최적 정책에 도달한다.</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_2.png" alt="policy_iteration_2" /></p>

        <ul>
          <li>$\pi_1$ 에 대한 가치함수 $v_{\pi_1}$ 을 구하면, $\pi_1$ 은 더이상 greedy policy 가 아니게 되고…</li>
          <li>greedy policy $\pi_2$ 를 구하면 더 이상 가치함수 $v_{\pi_1}$ 이 정확한 가치함수가 아니게 된다.</li>
          <li>위의 과정을 반복하면 필연적으로 변하지 않는 정책 $\pi_*$ 와 정확한 가치함수 $v_*$ 를 구하게 된다.</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_psuedo_code.png" alt="policy_iteration_psuedo_code" /></p>

        <ul>
          <li>4x4 gridworld 예제의 정의</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_1.png" alt="policy_iteration_example_4by4_gridworld_1" /></p>

        <ul>
          <li>$\pi_0$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_2.png" alt="policy_iteration_example_4by4_gridworld_2" /></p>

        <ul>
          <li>$v_{\pi_0}$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_3.png" alt="policy_iteration_example_4by4_gridworld_3" /></p>

        <ul>
          <li>$\pi_1$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_4.png" alt="policy_iteration_example_4by4_gridworld_4" /></p>

        <ul>
          <li>$v_{\pi_1}$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_5.png" alt="policy_iteration_example_4by4_gridworld_5" /></p>

        <ul>
          <li>$\pi_2$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_6.png" alt="policy_iteration_example_4by4_gridworld_6" /></p>

        <ul>
          <li>$v_{\pi_2}$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_7.png" alt="policy_iteration_example_4by4_gridworld_7" /></p>

        <ul>
          <li>$\pi_3$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_8.png" alt="policy_iteration_example_4by4_gridworld_8" /></p>

        <ul>
          <li>$\pi_*$</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_example_4by4_gridworld_9.png" alt="policy_iteration_example_4by4_gridworld_9" /></p>

        <ul>
          <li>The Power of Policy Iteration</li>
        </ul>

        <p><img src="/assets/images/posts/policy_iteration_power_of_p_i.png" alt="policy_iteration_power_of_p_i" /></p>

        <ul>
          <li>최적 정책에 도달하기 전까지, 계속적으로 정책이 개선됨을 볼 수 있다.</li>
          <li>최적 정책이 선형적이지 않을 때도 정책에 도달할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2>

<ul>
  <li>
    <p>Flexibility of the Policy Iteration Framework</p>

    <ul>
      <li>학습목표
        <ul>
          <li>일반화된 정책 반복 프레임워크 (framework of generalized policy iteration) 이해하기</li>
          <li>가치 반복 (value iteration) 과 일반화된 정책 반복 (generalized policy iteration) 의 주요 예시</li>
          <li>동기 (synchronous) / 비동기 (asynchronous) 동적 프로그래밍 방법 간 차이점을 이해하기</li>
        </ul>
      </li>
      <li>
        <p>유연한 Policy Iteration</p>

        <p><img src="/assets/images/posts/flexible_policy_iteration_1.png" alt="flexible_policy_iteration_1" /></p>

        <ul>
          <li>현 정책에 가까운 가치 함수 예측치를 사용</li>
          <li>좀 더 탐욕적인 정책을 사용하나, 완전히 탐욕적인 정책은 아닌 정책을 사용</li>
          <li>이러한 진행 또한 최적 정책과 최적 가치함수를 향해 나아간다.</li>
          <li>이러한 Policy Iteration 을 Generalized Policy Iteration 이라고 함.</li>
        </ul>
      </li>
      <li>Value Iteration
        <ul>
          <li>Generalized Policy Iteration 중 하나</li>
          <li>모든 상태를 sweep 하고, 현 가치 함수에 대해 탐욕 정책을 사용하는 것은 같음.</li>
          <li>그러나, 완전한 정책 평가를 하는 것은 아님
            <ul>
              <li>모든 상태에 대해 단 한번의 스윕만을 진행</li>
              <li>스윕 진행 후 다시 탐욕 정책을 사용함.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/value_iteration_psuedo_code.png" alt="value_iteration_psuedo_code" /></p>

        <ul>
          <li>이러한 업데이트 룰을 상태 가치함수에 바로 적용한다.
            <ul>
              <li>$V(s) \gets \max_a \sum_{s’,r} p(s’,r|s,a) [r + \gamma V(s’) ]$</li>
              <li>업데이트가 어떠한 특정 정책을 참조하지 않기 때문에, 이 방식을 value iteration 이라 한다.</li>
            </ul>
          </li>
          <li>이 방식은 iterative policy evaluation 과 매우 유사한데, 고정된 정책을 이용해 업데이트 하는 것이 아닌, 현재의 추정 값을 이용해 최대화 하는 것이 특징이다.</li>
          <li>value iteration 또한 극한 값에서 $v_*$ 에 수렴한다.</li>
          <li>우리는 극한으로 진행될 때까지 기다릴 수 없으므로, 종료 조건을 둔다.</li>
          <li>최종적으로 구해진 최적 가치 함수에 대해 $\arg\max$ 를 취함으로서 최적 정책을 얻게 된다.</li>
        </ul>
      </li>
      <li>Avoiding full sweeps
        <ul>
          <li>Synchronous DP
            <ul>
              <li>value iteration 또한 policy evaluation iteration 과 마찬가지로 모든 상태를 (순차적으로) 스윕한다.</li>
              <li>시스템 적으로 스윕을 하는 방식을 synchronus (동기 방식) 라 한다.</li>
              <li>만약 상태공간이 크다면, 이러한 방식은 문제가 된다.
                <ul>
                  <li>모든 스윕 단계에 매우 긴 시간을 소모함</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Asynchronous DP
            <ul>
              <li>상태의 값을 특정 순서 없이 업데이트 한다. (시스템 적인 스윕이 아님).</li>
              <li>다른 상태값이 한번 업데이트 되는 동안 특정 상태값을 여러 번 업데이트 할 수 있음.</li>
              <li>수렴을 보장하기 위해서는, 계속하여 모든 상태의 값을 업데이트 해야함.
                <ul>
                  <li>예를 들어 다른 상태를 무시하고 3개의 상태값만 계속 업데이트 한다면, 다른 상태의 값이 옳을 리가 없기 때문에 수렴할 수가 없음.</li>
                </ul>
              </li>
              <li>이러한 선택적 업데이트 덕분에, 값 정보를 빠르게 전파할 수 있음.
                <ul>
                  <li>어떠한 경우에서는 시스템적인 스윕보다 더 효율적일 수 있음.
                    <ul>
                      <li>예를 들어 최근 값이 변한 상태의 주변 값들을 집중적으로 업데이트.</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Efficiency of Dynamic Programming</p>

    <ul>
      <li>학습목표
        <ul>
          <li>최적 정책을 찾기 위한 대안으로서의 무작위 탐색 방법 (brute force search) 설명</li>
          <li>가치 함수 학습을 위한 대안으로서의 몬테카를로 (Monte Carlo) 방식 설명</li>
          <li>최적 정책 탐색에 있어서 동적 프로그래밍 (Dynamic programming) 과 부트스트래핑 (bootstrapping) 방식이 대안 전략과 비교하여 가지는 이점을 이해하기</li>
        </ul>
      </li>
      <li>A Sampling Alternative for Policy Evaluation (Monte Carlo)
        <ul>
          <li>Dynamic Programming 의 policy evaluation iteration 의 대안</li>
          <li>
            <p>Monte Carlo 방식</p>

            <p><img src="/assets/images/posts/policy_evaluation_monte_carlo_1.png" alt="policy_evaluation_monte_carlo_1" /></p>

            <ul>
              <li>정책 $\pi$ 에 대한 많은 리턴값을 수집하여 평균 값을 구하는 방식</li>
              <li>결국 값에 수렴하게 됨</li>
            </ul>

            <p><img src="/assets/images/posts/policy_evaluation_monte_carlo_2.png" alt="policy_evaluation_monte_carlo_2" /></p>

            <ul>
              <li>수렴을 위해서는 각 상태에 대한 많은 리턴 수집값이 필요
                <ul>
                  <li>이 값들은 $\pi$ 에 의해 선택된 random action, 환경 역학에 의한 random state transition 등 많은 random 성을 띄게 됨.</li>
                </ul>
              </li>
              <li>이러한 과정을 모든 상태에 대해 별개로 진행해야 함.</li>
            </ul>
          </li>
          <li>
            <p>Dynamic Programming 의 이점</p>

            <p><img src="/assets/images/posts/policy_evaluation_monte_carlo_2.png" alt="policy_evaluation_dp_bootstrapping_1" /></p>

            <ul>
              <li>Dynamic Programming 의 핵심은 각 상태의 평가를 별개의 문제로 취급할 필요가 없다는 점이다.
                <ul>
                  <li>이미 계산해 놓은 다른 상태의 값을 이용할 수 있음.</li>
                </ul>
              </li>
              <li>이렇게 이후 상태의 추측값을 사용해 현재의 추측값을 개선하는 것을 부트스르래핑 (bootstrapping) 이라 한다.
                <ul>
                  <li>이 방식이 각각의 상태를 별개로 계산하는 몬테카를로 방식보다 훨씬 효율적임.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Brute-Force Search
        <ul>
          <li>Dynamic Programming 의 policy iteration 의 대안</li>
          <li>
            <p>Brute-Force Search 방식</p>

            <p><img src="/assets/images/posts/policy_improvement_brute_force_search_1.png" alt="policy_improvement_brute_force_search_1" /></p>

            <ul>
              <li>이 방식은 단순히 모든 결정론적인 정책을 하나하나 평가하여 가장 높은 값의 정책을 선택하는 것임.</li>
              <li>정책의 수는 유한하고, 언제나 최적 결정론적 정책은 존재함으로 최적 정책을 찾을 수 있음.</li>
              <li>그러나 결정론적인 정책의 수가 너무 많을 수도 있음.
                <ul>
                  <li>각각의 상태에 대해 하나의 행동을 선택해야 함.
                    <ul>
                      <li>$| \mathscr{A} | * | \mathscr{A} | * \cdots * | \mathscr{A} |$</li>
                    </ul>
                  </li>
                  <li>즉, 결정론적 정책의 수는 지수적이다.
                    <ul>
                      <li>${| \mathscr{A} |}^{| \mathscr{S} |}$</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>따라서 이 프로세스는 시간이 매우 오래 걸린다.</li>
            </ul>
          </li>
          <li>Policy Improvement Theorem 의 이점
            <ul>
              <li>점점 더 나은 정책을 찾게 된다.</li>
              <li>이 점은 모든 정책에 대한 검색보다 훨씬 효율적이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Efficiency of Dynamic Programming
        <ul>
          <li>Policy Iteration : $| \mathscr{S} |$ 와 $| \mathscr{A}|$ 에 대한 다항식 곱의 복잡도</li>
          <li>Brute-Force Search : ${| \mathscr{A} |}^{| \mathscr{S} |}$ 개의 정책</li>
          <li>Dynamic Programming 은 Brute-Force Search 에 비해 지수적으로 빠름
            <ul>
              <li>예를 들어 4x4 Grid World 의 경우 DP 는 위의 예시에서 약 5번의 스윕을 통해 최적 정책을 찾아냈으나, Brute-Force Search 의 경우 $4^{16}$ 개의 정책을 확인해야 함.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>The Curse of Dimensionality (차원의 저주)
        <ul>
          <li>관계된 요소의 수가 늘어날 수록 상태 공간의 크기가 지수적으로 늘어남</li>
          <li>MDP 문제는 상태의 크기가 커질 수록 풀기 어려워짐</li>
          <li>하나의 에이전트가 Grid World 를 탐험하는 것은 괜찮지만, 대중교통을 설계하기 위해 몇천 명의 운전자가 수백 개의 지역을 돌아다니는 상태를 가정하면 어떻게 될까?</li>
          <li>사실 이는 Dynamic Programming 의 문제가 아닌 문제 자체의 내제된 복잡성이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Warren Powell: Approximate Dynamic Programming for Fleet Management(Short)</li>
  <li>
    <p>Warren Powell: Approximate Dynamic Programming for Fleet Management(Long)</p>
  </li>
  <li>
    <p>Week 4 Summary</p>

    <ul>
      <li>Policy evaluation : 특정한 정책 $\pi$ 로부터 상태가치함수 $v_\pi$ 를 구하는 것
        <ul>
          <li>Iterative Policy Evaluation
            <ul>
              <li>$v_\pi (s) = \sum_a \pi (a|s) \sum_{s’} \sum_r p(s’,r | s,a) [ r+\gamma v_\pi (s’) ]$</li>
              <li>$v_{k+1} (s) \gets \sum_a \pi (a|s) \sum_{s’} \sum_r p(s’,r | s,a) [ r+\gamma v_k (s’) ]$</li>
              <li>$v_\pi$ 에 대한 벨만 방정식을 업데이트 룰로 바꾼 것</li>
              <li>반복 과정을 거치며 점점 더 근사하는 가치함수를 찾을 수 있음</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Control : 정책을 발전시키는 과정
        <ul>
          <li>Policy improvement theorem
            <ul>
              <li>$\pi’ (s) \doteq \arg\max_a \sum_{s’} \sum_r p(s’,r | s,a) [ r+\gamma v_\pi (s’) ]$</li>
              <li>새로운 정책 $\pi’$ 는 현 가치함수에서 단순히 탐욕화한 정책이다.</li>
              <li>$\pi’$ 은 $\pi$ 보다 엄격히 개선된 정책임을 보장한다. ($\pi$ 가 최적정책이 아닐 경우)</li>
            </ul>
          </li>
          <li>Policy Iteration
            <ul>
              <li>$E \to I \to \cdots \to E \to I \to v_* , \pi_*$</li>
            </ul>
          </li>
          <li>Generalized Policy Iteration
            <ul>
              <li>Policy Iteration 과 달리 Evaluation 과 Improvement step 을 끝까지 진행하지 않고 반복하는 것
                <ul>
                  <li>value iteration : Generalized Policy Iteration 의 한 종류로, 모든 상태를 단 한번 스윕하고 정책을 개선시키는 것</li>
                </ul>
              </li>
              <li>asynchronous DP
                <ul>
                  <li>모든 상태를 시스템적으로 스윕하는 것이 아닌, 불규칙적인 방식으로 상태를 업데이트 하는 것</li>
                  <li>모든 상태를 지속적으로 업데이트한다는 가정 하에 최적 정책으로 수렴하게 됨</li>
                  <li>특정 상황에서 더 빠르게 수렴할 수 있으며, 상태 공간이 큰 문제에 효율적임</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Chapter Summary (RLbook2018 Pages 88-89)</li>
</ul>

<h2 id="weekly-assessment">Weekly Assessment</h2>

<h2 id="course-wrap-up">Course Wrap-up</h2>

<ul>
  <li>
    <p>Bandits</p>

    <p><img src="/assets/images/posts/wrap_up_1_bandits_1.png" alt="wrap_up_1_bandits_1" /></p>

    <ul>
      <li>각 레버의 보상의 분포를 모르기 때문에, 각각의 arm 을 많이 시도하여 평균을 구해야 했었음.</li>
      <li>Exploration - Exploitation Trade-Off
        <ul>
          <li>지금 당장의 최선의 arm 을 당길 것인지, 다른 arm 을 탐험할 것인지?</li>
        </ul>
      </li>
      <li>Bandit 문제는 늘 같은 state 에서 action 을 선택하는 문제였음.
        <ul>
          <li>불변하는 하나의 최상의 action 이 존재</li>
          <li>보상은 지연되지 않고 즉시 지급되었음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>MDP</p>

    <p><img src="/assets/images/posts/wrap_up_2_mdp_1.png" alt="wrap_up_2_mdp_1" /></p>

    <ul>
      <li>Bandits 보다 복잡한 현실 문제를 더 잘 반영한 모델</li>
      <li>환경은 action 을 선택하였을 때 즉각적인 보상 뿐만 아니라 다음의 상태도 제공해 줌
        <ul>
          <li>이 상태는 미래의 보상에 잠재적인 영향을 준다.</li>
        </ul>
      </li>
      <li>보상은 미래에 받을 잠재적 보상값의 할인된 합계이다.</li>
    </ul>
  </li>
  <li>Basic Concepts of Reinforcement learning
    <ul>
      <li>정책 (policy) : 에이전트가 각 상태 (state) 에서 어떤 action 을 취할 것인지를 말함</li>
      <li>가치함수 (value function)
        <ul>
          <li>$v_\pi (s) \doteq E_{\pi} [ G_t | S_t = s ]$</li>
          <li>특정 정책 하에 각 상태에 대해 미래의 예상되는 리턴 값을 측정해줌</li>
          <li>혹은 특정 정책 하에  상태-행동 쌍에 대한 미래의 예상되는 리턴 값을 측정해줌</li>
        </ul>
      </li>
      <li>벨만 방정식
        <ul>
          <li>각 상태 혹은 상태-행동 쌍의 값을 가능한 다음 값과 연결 시켜주는 방정식 (부트스트래핑)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Dynamic Programming</p>

    <p><img src="/assets/images/posts/wrap_up_3_dp_1.png" alt="wrap_up_3_dp_1" /></p>

    <ul>
      <li>prediction (Policy Evaluation)</li>
      <li>control (Policy Improvement)</li>
      <li>Dynamic Programming 의 경우 환경 역학 (Environment dynamics) 에 접근할 수 있어야 한다.</li>
      <li>강화학습 문제, 혹은 현실의 문제에서는 이 환경역학에 접근할 수 없다. (시도해 보기 전엔 어떤 영향을 줄 지 알 수 없음.)</li>
      <li>Dynamic Programming 은 강화학습 알고리즘의 핵심 기초가 된다.</li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Dynamic Programming" /><summary type="html"><![CDATA[관련 자료 (RLbook2018 Pages 73-88)]]></summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 03. Week 3. Value Functions &amp;amp; Bellman Equations</title><link href="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_03_Week3/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 03. Week 3. Value Functions &amp;amp; Bellman Equations" /><published>2023-03-16T15:00:00+09:00</published><updated>2023-03-16T15:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_03_Week3</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_03_Week3/"><![CDATA[<h2 id="관련-자료-rlbook2018-pages-58-67">관련 자료 (RLbook2018 Pages 58-67)</h2>

<ul>
  <li>용어설명
    <ul>
      <li>휴리스틱 : 정립된 공식이 아닌 정보가 온전하지 않은 상황에서의 노력, 시행착오, 경험 등을 통해서 지식을 알게 되는 과정.
        <ul>
          <li>주먹구구식의 규칙 (Rule of Thumb) 을 통해 지식을 습득하게 되는 과정</li>
          <li>잘 추측하는 기술 (art of good guessing)</li>
          <li>알고리즘과 달리 휴리스틱은 해결책의 발견을 보장하지 않는다.</li>
          <li>그러나 휴리스틱은 알고리즘보다 효율적이다. (쓸모없는 대안책들을 실제 시도하지 않고도 배제 가능)</li>
          <li>출처 : <a href="http://www.aistudy.com/heuristic/heuristic.htm">http://www.aistudy.com/heuristic/heuristic.htm</a></li>
        </ul>
      </li>
      <li>휴리스틱 서치
        <ul>
          <li>깊이우선 탐색이나 너비우선 탐색 등의 blind search method 는 goal 까지 의 경로를 찾는 상당히 소모적인 (exhaustive) 방법임.</li>
          <li>즉 문제에 대한 해를 제공하지만 너무 많은 노드를 확장시키므로 실용적이지 못하다.</li>
          <li>탐색작업을 축소시키기 위해 항상 옳은 해를 제공하지는 못하지만 대부분의 경우에 잘맞는 경험에 의한 규칙 (rules of thumb) 을 이용</li>
          <li>이렇게 그래프로써 표현된 문제에 대한 특별한 정보를 이용하여 탐색 (Search) 작업을 빠르게 진행시키는 방식</li>
          <li>출처 : <a href="http://www.aistudy.co.kr/heuristic/heuristic_search.htm">http://www.aistudy.co.kr/heuristic/heuristic_search.htm</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Policies and Value Functions</p>

    <ul>
      <li>거의 대부분의 강화학습 알고리즘은 가치함수 (value function) 을 추정하는 것을 포함한다.
        <ul>
          <li>상태 (States) 혹은 상태-액션 쌍(State-action pairs) 에 대한 함수</li>
          <li>에이전트에게 주어진 상태 (혹은 주어진 상태에서 주어진 액션) 이 얼마나 좋은지 (how good) 추정하는 것</li>
          <li>얼마나 좋은지 (how good) 란 기대되는 미래 보상값 혹은 기대되는 결과값에 대한 이야기임</li>
        </ul>
      </li>
      <li>미래의 보상 값은, 에이전트가 어떤 액션을 취할지에 달려있음
        <ul>
          <li>따라서 가치함수 (value function) 는 특정한 행동 방식 ( = Policy) 에 따라 정의된다.</li>
        </ul>
      </li>
      <li>일반적으로 정책 (Policy) 은 환경 (States) 에 따른 선택 가능한 행동 (Action) 을 선택하는 확률의 매핑 정보이다.
        <ul>
          <li>$\pi ( a \mid s )$
            <ul>
              <li>에이전트가 정책(policy) $\pi$ 를 따른다 가정할 경우</li>
              <li>time t 시점에서 $S_t = s$ 일 때 $A_t = a$ 일 확률을 가리킨다.</li>
              <li>mdp 함수 $p$ 와 같이 $\pi$ 또한 평범한 함수이다.
                <ul>
                  <li>$\mid$ 는 각 $s \in S$ 에 대한 $a \in A(s)$ 의 확률 분포를 나타낸다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>강화 학습 함수는 에이전트의 경험 결과에 따라 어떻게 정책이 바뀔지를 지정한다.</li>
        </ul>
      </li>
      <li>$v_\pi (s)$ : policy $\pi$ 를 따를 때 state $s$ 일 때 가치 함수 (value function)
        <ul>
          <li>$\pi$ 를 따를 때 $s$ 상태에서 시작할 경우 기대되는 결과값</li>
        </ul>

        <p><img src="/assets/images/posts/3_5_1_state_value_function.png" alt="3_5_1_state_value_function" /></p>
        <ul>
          <li>$E_\pi [\cdot]$ : 에이전트가 policy $\pi$ 를 따르고, $t$ 는 임의의 time step 일 때 에이전트가 제공하는 무작위 변수에 대한 기대 리턴값</li>
          <li>Terminal state 에 대한 위의 값은 0</li>
          <li>우리는 $v_\pi$ 를 정책 $\pi$ 에 대한 state-value function 이라 한다.</li>
        </ul>

        <p><img src="/assets/images/posts/3_5_2_action_value_function.png" alt="3_5_2_action_value_function" /></p>
        <ul>
          <li>위와 유사하게 policy $\pi$ 아래에서 state $s$ 에 action $a$ 를 취할 때의 value 를 측정할 경우 이를 $q_\pi (s,a)$ 로 정의할 수 있다.</li>
          <li>이 때 $E_\pi [\cdot]$ 는 정책 $\pi$ 를 따를 경우 상태 $s$ 에서 시작하여 행동 $a$ 를 행했을 때 기대되는 리턴값을 뜻한다.</li>
          <li>우리는 $q_\pi$ 를 정책 $\pi$ 에 대한 action-value function 이라 한다.</li>
        </ul>
      </li>
      <li>가치함수 $v_\pi$ 와 $q_\pi$ 는 경험을 통해 추정할 수 있다.
        <ul>
          <li>예를 들어 정책 $\pi$ 를 따르는 에이전트가 각각의 상태를 마주하고 이에 따른 리턴 값의 평균을 보관할 경우, 해당 상태에 직면하는 횟수가 무한에 가까워지면 평균 값은 상태의 가치, $v_\pi (s)$ 로 수렴한다.</li>
          <li>마찬가지로 각 상태에 보관된 평균 값을 행동에 따라 분리한다면 이는 행동의 가치, $q_\pi (s,a)$ 로 수렴한다.</li>
          <li>우리는 이러한 추정 방식을 Monte Carlo methods 라 한다. (많은 랜덤한 샘플의 실제 리턴값을 평균내는 것을 포함하고 있기 때문)</li>
          <li>물론 이러한 방식은 상태값이 많으면 각 상태 별로 분리된 평균값을 각각 가지는 것에 어려움이 있다.</li>
          <li>대신, 에이전트는  $v_\pi$ 와 $q_\pi$ 를 파라미터화된 함수로 유지하고 이 파라미터를 조정하는 방식을 사용한다.
            <ul>
              <li>이 또한 정확한 추정치를 구할 수 있다.</li>
              <li>이 경우 파라미터화 된 function approximator (함수 근사) 에 달려있다. (이는 이후 과정에서 설명)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>가치함수의 핵심요소들은 강화학습 이나 다이나믹 프로그래밍 전반에 걸쳐 사용되며, 재귀적인 표현법으로 표현될 수 있다.
        <ul>
          <li>다음 조건은 $s$ 값과 그 값의 가능한 후속 states 사이에서 일관성있게 유지된다.</li>
        </ul>

        <p><img src="/assets/images/posts/3_5_3_recursive_state_value_function.png" alt="3_5_3_recursive_state_value_function" /></p>

        <ul>
          <li>위의 재귀식은 아래의 조건을 따른다.
            <ul>
              <li>$a \in A(s)$</li>
              <li>$s’ \in S$ (Episodic problem 일 경우 $S^+$)</li>
              <li>$r \in R$</li>
            </ul>
          </li>
          <li>위 식은 실제로 모든 변수 $a, s’, r$ 에 대한 합계이다.</li>
          <li>예상값을 구하기 위해 $\pi(a|s)p(s’,r|s,a)$ 확률을 계산하여 [ ] 안의 값에 가중치를 부여하고 이 모든 확률에 대한 합계를 구한다.</li>
          <li>위의 3.14 방정식은 Bellman equation for $v_\pi$ 이다.
            <ul>
              <li>이것은 상태의 가치, 그리고 그 상태의 후속 상태의 가치에 대한 관계를 나타낸다.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/3_5_4_backup_diagram_for_v_pi.png" alt="3_5_4_backup_diagram_for_v_pi" /></p>

        <ul>
          <li>에이전트는 state $s$ 에서 policy $\pi$ 에 기반한 행동들 ($a \in A(s)$) 을 할 수 있다.</li>
          <li>환경은 함수 $p$ 에 따라 주어진 역학 (dynamic) 을 통해 보상 $r$ 과 함께 여러 다음 상태 중 하나인 $s’$ 로 응답할 수 있음.</li>
          <li>벨만 방정식 (The Bellman equation (3.14.)) 은 모든 가능성에 대해 평균을 내며, 발생 확률에 따라 가중치를 부여함.</li>
          <li>시작 상태의 값은 다음 상태의 (할인된) 값과 보상값의 합과 반드시 같다.</li>
          <li>위의 다이어그램을 backup diagrams 라 한다.</li>
        </ul>

        <p><img src="/assets/images/posts/3_5_4_1_backup_diagram_v_pi_q_pi.png" alt="3_5_4_1_backup_diagram_v_pi_q_pi" />
  <img src="/assets/images/posts/3_5_4_2_backup_diagram_q_pi_v_pi.png" alt="3_5_4_2_backup_diagram_q_pi_v_pi" /></p>

        <ul>
          <li>위의 다이어그램은 $v_\pi (s)$ 와 $q_\pi (s)$ 의 관계를 나타낸다.</li>
        </ul>
      </li>
      <li>
        <p>Example 3.5 : Gridworld</p>

        <p><img src="/assets/images/posts/3_5_5_Gridworld_example.png" alt="3_5_5_Gridworld_example" /></p>

        <ul>
          <li>위 gridworld 는 단순한 유한 MDP 를 나타낸다.</li>
          <li>그리드의 각 셀은 환경의 state 를 나타낸다.</li>
          <li>각 셀에서 동,서,남,북 의 action 이 가능하며, 해당 방향의 셀로 이동하게 된다.</li>
          <li>가장자리에서의 이동은 이동 없이 해당 셀에 머물게 되나 보상으로 -1 을 얻게 된다.</li>
          <li>특별한 상태인 A, B 를 제외하고 다른 곳에서의 이동은 보상으로 0 을 얻는다.</li>
          <li>A 에서의 이동은 action 의 방향과 관계없이 A’ 로 이동하게 되며 +10의 보상을 얻는다.</li>
          <li>B 에서의 이동은 action 의 방향과 관계없이 B’ 로 이동하게 되며 +5의 보상을 얻는다.</li>
          <li>모든 상태에서 동일한 확률로 랜덤한 action 을 취하는 정책을 취할 때 우측의 그림은 그에따른 가치함수 (discounted reward $\gamma=0.9$)를 나타낸다.</li>
          <li>위 가치함수는 (3.14) 의 선형방정식 (linear equations) 을 푼 결과이다.
            <ul>
              <li>가장자리는 값 이 낮은데, 이는 가장자리에 부딪힐 확률이 높기 때문이다.</li>
              <li>State A 는 보상 값 +10 보다 낮은 가치를 가지는데 전이된 A’ 에서 감점될 확률이 높기 때문이다.</li>
              <li>B 는 B’ 로 전이되었을 때 A’ 보다 감점될 확률이 적다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Example 3.6 : Golf</p>

        <p><img src="/assets/images/posts/3_5_6_Golf_example.png" alt="3_5_6_Golf_example" /></p>

        <ul>
          <li>그림 중 위의 부분에 대한 설명임.</li>
          <li>공을 홀에 넣을 때까지 각 스트로크에 대해 -1 의 페널티 (마이너스보상).</li>
          <li>공의 위치가 상태이며, 상태 값은 홀로부터 상태 위치 까지의 스트로크 횟수 (음수) 값이다.</li>
          <li>행동은 공을 조준, 스윙하는 방법과 어떤 클럽을 선택하는가 등이 있지만 전자는 주어진 것으로 받아들이고 후자의 선택에만 집중한다.</li>
          <li>홀의 값은 0</li>
          <li>그린의 어느 곳에서든 퍼팅을 할 수 있다고 가정하며, 이러한 상태의 값은 -1</li>
          <li>그린 밖에서는 퍼팅으로 홀에 도달할 수 없으며, 퍼팅을 통해 그린에 도달할 수 있다면 -2</li>
          <li>마찬가지로 -2 등고선으로 퍼팅이 가능한 모든 위치는 -3 값을 가지며, 위와 같은 방식으로 등고선이 그려짐</li>
          <li>퍼팅으로 모래 함정에서 벗어날 수 없으므로 $-\infty$ 의 값을 가지게 됨</li>
          <li>전반적으로 퍼팅으로 티에서 홀 까지 가는데 6타가 걸림.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Optimal Policies and Optimal Value Functions</p>

    <ul>
      <li>강화학습 문제를 푼다는 것은, 간단히 말하면 긴 흐름 속에서 많은 보상을 획득하는 정책을 찾는 것이다.</li>
      <li>유한 MDP 문제에서 우리는 Optimal policy 를 아래와 같이 정의할 수 있다.
        <ul>
          <li>가치함수는 전체 정책에서 부분적인 순서를 결정한다.</li>
          <li>정책 $\pi$ 가 정책 $\pi’$ 와 비교하여 모든 상태에서 예상되는 리턴 값이 크거나 같을 경우 정책 $\pi$ 는 정책 $\pi’$ 보다 좋거나 동등하다고 본다.</li>
          <li>$\pi \ge \pi’$, if and only if $v_\pi (s) \ge v_\pi’ (s)$ for all $s \in S$.</li>
        </ul>
      </li>
      <li>항상 최소한 하나의, 타 정책보다 나은 정책이 있는데 이를 최적 정책 (optimal policy) 라 한다.
        <ul>
          <li>우리는 optimal policies 를 $\pi_*$ 로 표기한다.</li>
        </ul>

        <p><img src="/assets/images/posts/3_6_1_optimal_state_value_function.png" alt="3_6_1_optimal_state_value_function" /></p>

        <ul>
          <li>이는 optimal state-value function $v_*$ (for all $s \in S$) 를 공유한다.</li>
        </ul>

        <p><img src="/assets/images/posts/3_6_2_optimal_action_value_function.png" alt="3_6_2_optimal_action_value_function" /></p>

        <ul>
          <li>이는 또한 optimal action-value function $q_*$ (for all $s \in S$ and $a \in A(s)$) 를 공유한다.</li>
        </ul>

        <p><img src="/assets/images/posts/3_6_3_optimal_action_state_value_function_relation.png" alt="3_6_3_optimal_action_state_value_function_relation" /></p>

        <ul>
          <li>위 두 함수의 정의에 따라 위와같이 표현할 수 있다.</li>
        </ul>
      </li>
      <li>
        <p>Example 3.7 : Optimal Value Functions for Golf</p>

        <p><img src="/assets/images/posts/3_5_6_Golf_example.png" alt="3_5_6_Golf_example" /></p>

        <ul>
          <li>그림 중 아래 부분에 대한 설명임. (optimal action-value function $q_*(s,driver)$)</li>
          <li>드라이버로 먼저 스트로크를 한 다음 나중에 드라이버나 퍼터 중 더 나은 쪽을 선택하는 경우 각 상태의 값</li>
          <li>드라이버를 사용하면 공을 더 멀리 칠 수 있지만 정확도는 떨어져, 홀에서 가까운 부분만 -1 이 됨.</li>
          <li>스트로크가 두번 있는 경우 -2 등고선에서 볼 수 있듯 더 먼 곳에서 홀까지 도달할 수 있음.</li>
          <li>즉, 그린에만 떨어지면 퍼터를 사용할 수 있음.</li>
          <li>Optimal action-value function 은 처음 특정 행동을 한 이후에 값을 제공 (위의 경우 우선 드라이버를 사용하고 그 뒤에 무엇을 사용할지를 결정)</li>
          <li>-3 등고선은 더 멀리 있으며 시작 티를 포함함. 티에서 가장 좋은 순서는 드라이버2개, 퍼트1개로 공을 홀에 넣는 것임.</li>
        </ul>
      </li>
      <li>벨만 최적 방정식
        <ul>
          <li>$v_*$ 는 정책에 대한 가치함수이기 때문에 벨만 방정식(3.14) 에 의해 주어진 상태 값에 대한 자기 일관성 조건을 충족해야 한다.</li>
          <li>하지만 최적 가치 함수이기 때문에 $v_*$ 의 일관성 조건은 특정 정책을 참조하지 않고 특별한 형태로 작성할 수 있음.</li>
          <li>이것은 $v_*$ 에 대한 벨만 방정식 또는 벨만 최적 방정식 (Bellman Optimality equation) 이라 함.</li>
          <li>
            <p>직관적으로 Bellman 최적 방정식은 최적 정책 하의 상태 값이 해당 상태에서 최상의 조치에 대한 기대 수익과 같아야 한다는 사실을 나타냄.</p>
          </li>
          <li>
            <p>마지막 두 방정식은 벨만 최적 방정식 ($v_*$) 의 두가지 형태를 나타낸다.	
  <img src="/assets/images/posts/3_6_4_bellman_optimality_equation.png" alt="3_6_4_bellman_optimality_equation" /></p>
          </li>
          <li>
            <p>벨만 최적방정식 ($q_*$) 는 아래와 같이 표현할 수 있다.
  <img src="/assets/images/posts/3_6_5_bellman_optimality_equation.png" alt="3_6_5_bellman_optimality_equation" /></p>
          </li>
          <li>
            <p>아래는 벨만 최적방정식에 대한 backup diagrams 이다.
  <img src="/assets/images/posts/3_6_6_backup_diagrams_bellman_optimality_equation.png" alt="3_6_6_backup_diagrams_bellman_optimality_equation" /></p>
          </li>
          <li>이는 $v_\pi$ 와 $q_\pi$ 의 backup diagrams 와 같으나, 호 (arc) 가 추가되었음.
            <ul>
              <li>에이전트 선택 지점에 호가 추가되어 타 정책에 의해 주어진 예상 값이 아닌 해당 선택에 대한 최대값이 취해짐을 나타냄.</li>
            </ul>
          </li>
          <li>
            <p>왼쪽 backup diagram 은 3.19의 식이, 오른쪽은 3.20의 식이 표현된 것임.</p>
          </li>
          <li>유한 MDP 의 경우 $v_*$ (3.19) 에 대한 고유한 솔루션이 존재한다.</li>
          <li>벨만 최적방정식은 각각의 state 별로 존재하는 방정식이다.
            <ul>
              <li>따라서 n개의 state 가 있을 경우 n 개의 방정식과 n 개의 미지수가 존재하게 된다.</li>
            </ul>
          </li>
          <li>만약 환경의 역학 $p$ 를 알면 비선형 방정식 시스템을 풀기 위한 다양한 방법 중 하나를 선택하여 $v_*$ 에 대한 방정식을 풀 수 있다.</li>
          <li>$v_*$ 를 알게 되면, 최적 정책을 결정하기가 상대적으로 쉽다.
            <ul>
              <li>모든 상태 s 에 대해 벨만 최적 방정식의 최대값을 얻는 하나 또는 그 이상의 action 이 존재하고, 이런 action 에만 0 이 아닌 확률을 배분하는 정책이 최적 정책이다.</li>
              <li>즉 최적 가치 함수 $v_*$ 를 알면, one-step-ahead search 를 통해 어떤 action 이 최적의 action 인지 알게 된다.</li>
              <li>다른 용어로 (optimal evaluation function $v_*$) 의 측면에서 greedy policy 라 할 수 있음.
                <ul>
                  <li>greedy 함은 지역적 혹은 즉각적 고려를 통한 결정이며, 보다 나은 미래의 대안 가능성을 고려하지 않음.</li>
                </ul>
              </li>
              <li>$v_*$ 는 이미 미래 행동의 모든 가능성을 고려하고 반영한 결과값이므로, 긴 기간동안의 리턴값을 지역적, 즉각적인 수량으로 나타낸 값이 된다.</li>
            </ul>
          </li>
          <li>$q_*$ 를 알면, 최적의 행동을 선택하기 더 쉬워진다.
            <ul>
              <li>에이전트는 one-step-ahead search 를 할 필요 없이 s 상태에서 $q_*(s,a)$ 를 최대화 할 행동 a 를 선택하면 된다.</li>
              <li>$q_*$ 는 장기적인 기대 리턴값을 각각의 state-action pair 에 제한한다.</li>
            </ul>
          </li>
          <li>action-value function 은 더 효율적으로 모든 one-step-ahead search 의 결과값을 캐싱한다.
            <ul>
              <li>가능한 후속 상태 및 해당 값들에 대해 알 필요 없이 (즉, 환경의 역학에 대해 알 필요 없이) 최적 행동을 선택할 수 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Example 3.8: Solving the Gridworld</p>

        <p><img src="/assets/images/posts/3_6_7_solving_the_gridworld.png" alt="3_6_7_solving_the_gridworld" /></p>

        <ul>
          <li>Example 3.5 의 Gridworld 문제에 대한 벨만 방정식 (Bellman equation for $v_\pi$) 를 풀었다고 가정</li>
          <li>중간 이미지는 optimal value function $v_*$ 를 나타낸다.</li>
          <li>우측 이미지는 그에 상응하는 최적 정책 (optimal policies) 이다.</li>
        </ul>
      </li>
      <li>
        <p>Example 3.9: Bellman Optimality Equations for the Recycling Robot</p>

        <p><img src="/assets/images/posts/3_6_8_bellman_optimality_equations_for_the_recycling_robot.png" alt="3_6_8_bellman_optimality_equations_for_the_recycling_robot" /></p>

        <ul>
          <li>3.19 의 수식을 이용, 벨만 최적 방정식을 Recycling robot example 에 적용할 수 있다.</li>
          <li>states : high, row / actions : search, wait, recharge 를 각각 h, l, s, w, re 로 축약한다.</li>
          <li>상태가 2개이므로 벨만 최적 방정식은 2개의 방정식으로 구성된다.</li>
          <li>$v_* (h)$ 에 대한 방정식은 위와 같이 표기할 수 있다.</li>
          <li>$r_s, r_w, \alpha, \beta, \gamma$ ($0 \le \gamma &lt; 1, 0 \le \alpha,\beta \le 1$) 의 어떠한 케이스를 선택해도 정확히 한 쌍의 숫자, $v_* (h), v_* (l)$ 이 남는다. 이는 동시에 두 비선형 방정식을 만족함을 보여준다.</li>
        </ul>
      </li>
      <li>Bellman 최적 방정식의 한계
        <ul>
          <li>Bellman 최적 방정식의 해결은 최적의 정책을 찾고, 강화 학습 문제를 해결할 수 있음을 보여줌.</li>
          <li>그러나 이 솔루션은 실제로 유용하지 않음.
            <ul>
              <li>모든 가능성을 내다보고 예상되는 보상 측면에서 발생 가능성을 계산하는 검색과 유사함.</li>
              <li>실제로 거의 적용되지 않는 최소한 세 가지 가정에 의존함
                <ol>
                  <li>환경의 역학을 정확하게 알고 있음.</li>
                  <li>솔루션 계산을 완료하기에 충분한 계산 리소스가 있음</li>
                  <li>Markov 속성</li>
                </ol>
              </li>
              <li>예를 들어 첫 번째와 세 번째 가정은 백개먼 게임에 아무런 문제가 없지만 두 번째 가정은 큰 장애물이 됨.</li>
              <li>게임에는 약 $10^{20}$ 개의 상태가 있기 때문에</li>
              <li>오늘날의 가장 빠른 컴퓨터에서 $v_*$ 에 대한 Bellman 방정식을 푸는 데 수천 년이 걸리며</li>
              <li>$q_*$ 를 찾는 것도 마찬가지이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>강화 학습에서는 일반적으로 근사 솔루션 (approximate solutions) 으로 만족해야 함.
        <ul>
          <li>Bellman 최적 방정식을 근사로 푸는 방법으로 다양한 의사 결정 방법이 있음.
            <ul>
              <li>예를 들어 휴리스틱 검색 방법은 (3.19) 의 오른쪽을 여러 번 확장하여 어느 정도 깊이까지 형성함</li>
              <li>이를 통해 가능성에 대한 트리구조를 만들고, 휴리스틱 평가 함수를 사용하여 리프 노드에서 $v_*$ 를 근사함.
                <ul>
                  <li>$A^*$ 와 같은 휴리스틱 검색방법은 거의 항상 에피소드 사례를 기반으로 함.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>동적 계획법 (dynamic programming) 이 Bellman 최적 방정식과 훨씬 더 밀접한 관계가 있음.</li>
          <li>많은 강화 학습 방법은 transition 에 대한 지식 대신 transition 에 대한 경험을 토대로 근사치 해결을 함.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimality and Approximation
    <ul>
      <li>최적 가치 함수와 최적 정책에 대해 정의를 했고, 실제로 이를 학습한 에이전트는 수행을 잘 하였으나 이러한 경우는 매우 드물다.
        <ul>
          <li>이러한 종류의 작업은 엄청난 계산비용으로만 생성할 수 있다.</li>
        </ul>
      </li>
      <li>환경의 역학에 대해 완전하고 정확한 모델이 있더라도 벨만 최적 방정식을 풀어 최적의 정책을 계산하는 것은 일반적으로 불가능하다.
        <ul>
          <li>예를 들어 체스와 같은 보드게임도 여전히 최적의 수를 계산할 수 없음</li>
        </ul>
      </li>
      <li>에이전트가 직면하는 문제
        <ul>
          <li>단일 time-step 에서 수행할 수 있는 계산의 양</li>
          <li>사용 가능한 메모리 양 (가치함수, 정책, 모델 근사치의 구축 등에 필요)</li>
          <li>작고 유한한 상태집합이 있는 작업에서는 각 상태 혹은 상태-행동 쌍 에 대해 배열 또는 테이블을 사용하여 근사치를 형성할 수 있음
            <ul>
              <li>우리는 이런 경우를 tabular case, tabluar method (테이블 형식) 이라 하나 실질적인 문제들은 테이블 항목으로 표현할 수 없을 정도로 훨씬 더 많은 상태를 가지고 있음</li>
              <li>이러한 경우 더 간결한 형태의 매개 변수화된 함수 표현을 사용하여 근사화 해야 함</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>근사화
        <ul>
          <li>강화학습의 온라인 특성
            <ul>
              <li>자주 경험하는 상태에 대해 좋은 결정을 내릴 수 있도록 더 많은 노력을 기울임</li>
              <li>드물게 경험하는 상태는 상대적으로 더 적은 노력을 기울임
                <ul>
                  <li>최선의 선택이 아닌 선택을 할 확률이 매우 낮은 보상에 미미한 영향을 주는 상태들</li>
                  <li>예를 들어 백개먼 게임에서 실제로 거의 일어나지 않는 상황에서 악수를 둘 수 있지만, 전문가와 뛰어난 기술로 플레이 할 수 있음</li>
                  <li>수 많은 다양한 환경에서 실제로 잘못된 결정을 내릴 확률이 있음.</li>
                </ul>
              </li>
              <li>이는 MDP 를 근사화 하여 해결하는 다른 접근방식과 강화학습을 구별하는 핵심 속성중 하나임</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="policies-and-value-functions">Policies and Value Functions</h2>

<ul>
  <li>
    <p>Specifying Policies</p>

    <ul>
      <li>학습목표
        <ul>
          <li>정책 (Policy) 은 각 가능한 상태 (State)에 대한 행동 (Action) 의 분포임을 인지한다.</li>
          <li>확률론적 정책 (Stochastic Policies) 과 결정론적 정책 (Deterministic Policies) 의 유사점과 차이점을 설명한다.</li>
          <li>잘 정의된 정책의 특성을 식별한다.</li>
          <li>주어진 MDP 에 유효한 정책의 예시</li>
        </ul>
      </li>
      <li>Deterministic policy (결정론적 정책)
        <ul>
          <li>각 상태에 하나의 행동을 매핑</li>
          <li>$\pi(s) = a$</li>
          <li>
            <p>테이블로 표현 가능</p>

            <table>
              <thead>
                <tr>
                  <th style="text-align: left">State</th>
                  <th style="text-align: left">Action</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align: left">$s_0$</td>
                  <td style="text-align: left">$a_1$</td>
                </tr>
                <tr>
                  <td style="text-align: left">$s_1$</td>
                  <td style="text-align: left">$a_0$</td>
                </tr>
                <tr>
                  <td style="text-align: left">$s_2$</td>
                  <td style="text-align: left">$a_0$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>
            <p>예시</p>

            <p><img src="/assets/images/posts/example_deterministic_policy.png" alt="example_deterministic_policy" /></p>
          </li>
        </ul>
      </li>
      <li>Stochastic policy (확률론적 정책)
        <ul>
          <li>각 상태에서 행동이 가지는 확률을 표현</li>
          <li>하나의 상태에서도 각각 다른 행동이 선택될 수 있음 (확률이 0이 아닐 경우)</li>
          <li>$\pi(a|s)$</li>
          <li>
            <p>확률 그래프로 표현 가능</p>

            <p><img src="/assets/images/posts/example_stochastic_policy.png" alt="example_stochastic_policy" /></p>
          </li>
          <li>
            <p>예시</p>

            <p><img src="/assets/images/posts/example_stochastic_policy_2.png" alt="example_stochastic_policy_2" /></p>
          </li>
        </ul>
      </li>
      <li>정책은 오직 현재 상태에만 영향을 받는 것이 중요함.
        <ul>
          <li>시간이나 이전 상태와 같은 요소에 영향을 받지 않아야 함.
            <ul>
              <li>50% : 50% 의 행동 확률이라고 번갈아 가며 행동하지 않음 (이것은 현 상태 외의 영향을 받은 정책임.)</li>
              <li>이런 면을 상태의 요구사항이며, 에이전트의 제한은 아닌 것으로 여기는 편이 좋음.</li>
            </ul>
          </li>
          <li>현재 상태에 현재 행동을 결정할 모든 요소가 포함되어 있어야 함.</li>
        </ul>
      </li>
      <li>MDP 에서는 상태가 결정을 위한 모든 정보를 포함한 것으로 가정한다.
        <ul>
          <li>만약 번갈아 가며 하는 행동이 높은 보상값을 제공한다면, 상태값에 마지막 행동값이 포함되어야 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Value Functions</p>

    <ul>
      <li>
        <p>학습목표</p>

        <ul>
          <li>강화학습에서 상태가치함수 (state-value functions) 와 행동가치함수 (action-value functions) 의 역할에 대한 설명</li>
          <li>가치 함수 (value function) 와 정책 (policy) 간의 관계 설명</li>
          <li>주어진 MDP 에 대해 유효한 가치함수 생성</li>
        </ul>
      </li>
      <li>개념 설명
        <ul>
          <li>가치함수는 지연된 보상을 나타낸다.</li>
          <li>강화학습에서는 장기적으로 최대의 보상을 얻는 정책을 학습하는 것을 목표로 한다.</li>
        </ul>
      </li>
      <li>
        <p>State-value functions</p>

        <ul>
          <li>$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$</li>
          <li>$v(s) \doteq E [G_t|S_t=s]$</li>
          <li>주어진 환경에 대해 기대되는 보상 값을 의미</li>
          <li>이 의미에 의해 value function 은 주어진 policy (agent 가 어떤 action을 취할 것인지) 에 영향을 받는다는 것을 의미</li>
          <li>$v_\pi (s) \doteq E_\pi [G_t|S_t=s]$</li>
          <li>주어진 정책 하에 현 상태에 기대되는 리턴값</li>
        </ul>
      </li>
      <li>
        <p>Action-value functions</p>

        <ul>
          <li>$q_\pi (s,a) \doteq E_\pi [G_t|S_t=s,A_t=a]$</li>
          <li>s에서 a를 선택한 후 정책을 따랐을 시 기대되는 리턴값</li>
        </ul>
      </li>
      <li>Value function 의 의미
        <ul>
          <li>장기적인 결과를 관찰하기 위해 기다리는 대신</li>
          <li>현재 상황의 품질을 질의할 수 있음</li>
          <li>이 리턴 값은 즉시 사용할 수 없음</li>
          <li>정책 및 환경 역학의 확률로 인해 리턴 값이 무작위일 수 있음</li>
          <li>Value function 은 미래의 모든 기대되는 리턴값을 평균값으로 요약함</li>
          <li>이를 토대로 다른 정책들의 질을 판단할 수 있게 됨.</li>
        </ul>
      </li>
      <li>Value function 의 예시 : Chess
        <ul>
          <li>체스는 episodic MDP 이다.</li>
          <li>State : 모든 말의 현 위치</li>
          <li>Action : 합법적인 이동</li>
          <li>Reward : 게임의 승리(+1), 패배 또는 무승부(0)</li>
          <li>위의 보상으로 경기 중 에이전트가 얼마나 잘 플레이하는지에 대해 알 수 없음.</li>
          <li>또한 보상을 보려면 에피소드가 끝날 때 까지 기다려야 함.</li>
          <li>이 때 가치함수는 훨씬 더 많은 것을 알려줄 수 있음.
            <ul>
              <li>상태 가치함수 값은 단순히 현 Policy 를 따랐을 경우 이길 확률을 말함.</li>
              <li>상대방의 움직임은 상태 전이이다.</li>
              <li>action value function 은 policy 를 따랐을 경우 현 동작을 통해 이길 확률을 나타낸다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Rich Sutton and Andy Barto : A brief History of RL</p>
  </li>
</ul>

<h2 id="bellman-equations">Bellman Equations</h2>

<ul>
  <li>
    <p>Bellman Equation Derivation</p>

    <ul>
      <li>학습목표
        <ul>
          <li>상태가치함수 (state-value function) 에 대한 Bellman 방정식 유도</li>
          <li>행동가치함수 (action-value function) 에 대한 Bellman 방정식 유도</li>
          <li>Bellman 방정식이 현재와 미래 가치를 연관시키는 방법을 이해</li>
        </ul>
      </li>
      <li>State-value Bellman equation
        <ul>
          <li>$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$</li>
          <li>$v_\pi (s) \doteq E_\pi [G_t|S_t=s]$</li>
          <li>$=E_\pi [R_{t+1} + \gamma G_{t+1}|S_t=s]$</li>
          <li>$=\sum_a \pi(a|s) \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma E_\pi [G_{t+1}|S_{t+1}=s’]]$</li>
          <li>$=\sum_a \pi(a|s) \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma v_\pi (s’)]$</li>
        </ul>
      </li>
      <li>Action-value Bellman equation
        <ul>
          <li>$q_\pi (s,a) \doteq E_\pi [G_t|S_t=s,A_t=a]$</li>
          <li>$=\sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma E_\pi [G_{t+1}|S_{t+1}=s’]]$</li>
          <li>$=\sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma \sum_{a’} \pi(a’|s’)E_\pi[G_{t+1}|S_{t+1}=s’,A_{t+1}=a’]]$</li>
          <li>$=\sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma \sum_{a’} \pi(a’|s’)q_\pi (s’,a’)]$</li>
        </ul>
      </li>
      <li>현 state value 혹은 state/action value 는 미래의 state value 혹은 state/action value 표현법으로 재귀적 표현이 가능하다.</li>
    </ul>
  </li>
  <li>
    <p>Why Bellman Equations?</p>

    <ul>
      <li>학습목표
        <ul>
          <li>Bellman 방정식을 이용해 가치함수 (value functions) 를 계산</li>
        </ul>
      </li>
      <li>
        <p>예제 : Gridworld</p>

        <p><img src="/assets/images/posts/example_gridworld_bellman_equations_1.png" alt="example_gridworld_bellman_equations_1" /></p>

        <p><img src="/assets/images/posts/example_gridworld_bellman_equations_2.png" alt="example_gridworld_bellman_equations_2" /></p>

        <p><img src="/assets/images/posts/example_gridworld_bellman_equations_3.png" alt="example_gridworld_bellman_equations_3" /></p>

        <p><img src="/assets/images/posts/example_gridworld_bellman_equations_4.png" alt="example_gridworld_bellman_equations_4" /></p>

        <ul>
          <li>벨만 방정식은 가능한 모든 미래의 값을 무한히 더해가는 과정을 선형대수 문제로 치환시켜 준다.</li>
        </ul>
      </li>
      <li>벨만 방정식의 한계
        <ul>
          <li>체스게임과 같이 가능한 상태의 양이 많은 경우</li>
          <li>위의 예시의 경우 상태가 4개이기 때문에 4개의 선형 방정식을 풀면 되지만..</li>
          <li>체스 게임의 경우 $10^{45}$ 개의 선형 방정식을 풀어야 함.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="optimality-optimal-policies--value-functions">Optimality (Optimal Policies &amp; Value Functions)</h2>

<ul>
  <li>
    <p>Optimal Policies</p>

    <ul>
      <li>개요
        <ul>
          <li>정책 : 에이전트가 어떻게 행동할지를 나타내는 것</li>
          <li>정책이 결정된 후 value function 을 찾아볼 수 있다.</li>
          <li>강화학습의 목표는 특정 정책을 평가하는 것이 아닌 최적의 정책을 찾는 것이다.</li>
        </ul>
      </li>
      <li>학습목표
        <ul>
          <li>Optimal policy 에 대한 정의</li>
          <li>특정 정책이 어떻게 모든 상태에서 다른 모든 정책만큼 좋을 수 있는 것인지를 이해</li>
          <li>주어진 MDP 에 대한 최적의 정책 식별</li>
        </ul>
      </li>
      <li>
        <p>Optimal Policy 란?</p>

        <p><img src="/assets/images/posts/optimal_policy.png" alt="optimal_policy" /></p>

        <ul>
          <li>어떠한 상태에서도 타 정책과 같거나 더 좋은 경우</li>
          <li>최소한 하나 이상의 Optimal Policy 가 존재
            <ul>
              <li>특정 상황에 $\pi_2$ 가 $\pi_1$ 보다 결과가 좋을 경우</li>
              <li>해당 상황에서는 $\pi_2$ 정책을 사용하고 그 외의 경우 $\pi_1$ 을 사용하는 정책 $\pi_3$ 를 사용</li>
            </ul>
          </li>
          <li>작은 MDP 의 경우 직접적으로 풀 수 있지만…
            <ul>
              <li>2개의 결정론적 정책이 있을 경우 Brute-Force Search 로 문제 해결</li>
              <li>하지만 일반적인 MDP 의 경우 $|A|^{|S|}$ 개의 결정론적 정책이 존재하여 Brute-Force Search 로 문제 해결이 불가함.</li>
              <li>위의 경우 Bellman Optimality Equations 로 문제에 접근해야 함.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Optimal Value Functions</p>

    <ul>
      <li>학습목표
        <ul>
          <li>상태가치함수 (state-value functions) 에 대한 Bellman 최적 방정식 유도</li>
          <li>행동가치함수 (action-value functions) 에 대한 Bellman 최적 방정식 유도</li>
          <li>Bellman 최적 방정식이 이전에 소개된 Bellman 방정식과 어떻게 관련되는지 이해</li>
        </ul>
      </li>
      <li>Optimal Value Functions
        <ul>
          <li>$v_*$ : $v_{\pi_*} (s) \doteq E_{\pi_*} [G_t|S_t=s] = \underset{\pi}{\max} v_\pi (s)$ for all $s \in S$</li>
          <li>$v_* (s) = \sum_a \pi_* (a|s) \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma v_*(s’)]$</li>
          <li>$v_* (s) = \underset{a}{\max} \sum_{s’} \sum_r p(s’,r|s,a)[r+\gamma v_*(s’)]$
            <ul>
              <li>언제나 하나 이상의 결정론적인 최적 정책이 존재한다.</li>
              <li>모든 상태에서 하나의 최적 행동을 선택한다.</li>
              <li>즉, 가장 높은 리턴값을 가지는 하나의 행동의 확률이 1이고, 나머지 행동의 확률은 0이 된다.</li>
              <li>Bellman Optimality Equation for $v_*$</li>
            </ul>
          </li>
          <li>$q_*$ : $q_{\pi_*} (s,a) = \underset{\pi}{\max} q_\pi (s,a)$ for all $s \in S$ and $a \in A$</li>
          <li>$q_* (s,a) = \sum_{s’} \sum_r p(s’,r|s,a) [r+ \gamma \sum_{a’} \pi_{*} (a’|s’) q_* (s’,a’)]$</li>
          <li>$q_* (s,a) = \sum_{s’} \sum_r p(s’,r|s,a) [r+ \gamma \underset{a’}{\max} q_{*} (s’,a’)]$
            <ul>
              <li>Bellman Optimality Equation for $q_*$</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/bellman_optimality_equation_for_qastar_linear_system_solver.png" alt="bellman_optimality_equation_for_qastar_linear_system_solver" /></p>
        <ul>
          <li>위의 벨만 최적 방정식으로는 $v_{*}$ 를 풀어낼 수가 없는데, $\max$ 함수가 선형이 아니기 때문이다.</li>
          <li>$\pi_{*}$ 값을 이용해 같은 방식으로 $v_{*}$ 를 구할 수도 없는데, $\pi_{*}$ 값을 모를 뿐더러, $\pi_{*}$ 를 구하는 것 자체가 목적이기 때문이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Using Optimal Value Functions to Get Optimal Policies</p>

    <ul>
      <li>학습목표
        <ul>
          <li>최적가치함수 (Optimal value function) 과 최적정책 (Optimal Policy) 의 연관성 이해</li>
          <li>주어진 MDP 에 대한 최적가치함수 (Optimal value function) 확인</li>
        </ul>
      </li>
      <li>Optimal Policy 와 Optimal Value Function 의 관계
  <img src="/assets/images/posts/optimal_policy_and_optimal_value_function_1.png" alt="optimal_policy_and_optimal_value_function_1" />
        <ul>
          <li>$p$ 와 $v_*$ 에 접근할 수 있다고 가정</li>
          <li>한 단계 진행 시의 값을 구할 수 있을 경우, $A_2$ 가 최대의 값을 가짐을 알 수 있다.</li>
          <li>$\max$ 는 최대의 값을, $\arg\max$ 는 박스가 최대의 값을 가지게 하는 $a$ 값 자체를 나타낸다.</li>
        </ul>

        <p><img src="/assets/images/posts/optimal_policy_and_optimal_value_function_2.png" alt="optimal_policy_and_optimal_value_function_2" /></p>
        <ul>
          <li>$p$ 와 $v_*$ 에 접근할 수 있을 때 계산법</li>
        </ul>

        <p><img src="/assets/images/posts/optimal_policy_and_optimal_value_function_3.png" alt="optimal_policy_and_optimal_value_function_3" /></p>
        <ul>
          <li>$p$ 는 확률적 요소여서 알기 힘들지만, 충분히 많이 접근하면 위의 수식에 따라 최적 정책을 구할 수 있게 된다.</li>
          <li>$q_*$ 를 알 경우 최적 정책을 구하기 훨씬 쉬워지는데, 다음 스텝의 계산을 할 필요가 없기 때문이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Week 3 Summary</p>
  </li>
  <li>
    <p>Chapter Summary (RLbook2018 Pages 68-69)</p>
    <ul>
      <li>Reinforcement learning : Learning from interaction how to behave in order to achieve a goal.
        <ul>
          <li>interaction : 에이전트와 환경이 이산 스텝의 시퀀스에 따라 상호작용하는 것</li>
          <li>actions : 에이전트에 의해 이루어지는 선택</li>
          <li>states : 선택에 영향을 주는 요소</li>
          <li>rewards : 선택을 평가하는 요소</li>
          <li>모든 agent 내의 요소들은 완전히 알고 있고, 에이전트에 의해 컨트롤된다.</li>
          <li>모든 agent 밖의 요소는 불완전하게 제어되며, 완전히 알고 있는 것일수도, 그렇지 않은 것일수도 있다.</li>
          <li>policy : 에이전트가 상태값을 인자로 한 함수를 통해 행동을 선택하는 확률적 규칙</li>
          <li>agent 의 목적 : 전체 시간 동안 받을 수 있는 보상을 최대화 하는 것</li>
        </ul>
      </li>
      <li>Markov Decision Process (MDP)
        <ul>
          <li>위의 강화학습 설정이, 잘 정의된 전환 확률로 공식화되면 Markov Decision Process (MDP) 로 정의됨</li>
          <li>유한 MDP : 유한한 상태, 행동 및 보상 세트가 있는 MDP</li>
          <li>강화학습 이론의 대부분은 유한 MDP 로 제한하지만, 방법과 아이디어는 더 일반적으로 적용됨.</li>
        </ul>
      </li>
      <li>return : 미래의 보상에 대한 함수로 agent 가 최대화 하려는 기대값
        <ul>
          <li>작업의 특성과 지연된 보상의 할인 정도에 따라 다른 정의를 가질 수 있음</li>
          <li>할인이 적용되지 않은 return 식은 episodic tasks 에 맞는 방식
            <ul>
              <li>episodic tasks : 상호작용이 에피소드에 따라 자연스럽게 중지되는 형태</li>
            </ul>
          </li>
          <li>할인이 적용된 return 식은 continuing tasks 에 맞는 방식
            <ul>
              <li>continuing tasks : 상호작용이 자연스럽게 중단되지 않고 제한 없이 계속 이어지는 형태</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>value functions and optimal value functions
        <ul>
          <li>정책의 가치함수는 에이전트가 해당 정책을 사용하는 경우, 각 상태 혹은 상태-행동 쌍과 그에 예상되는 수익을 할당함</li>
          <li>최적 정책의 가치함수는 각 상태 혹은 상태-행동 쌍에 모든 정책 중 달성할 수 있는 최대의 기대 수익을 할당함</li>
          <li>최적 가치함수를 사용하는 정책을 최적 정책이라 한다.
            <ul>
              <li>최적 정책은 하나이거나 하나 이상일 수 있다. (예: 50 : 50 의 확률론적 최적 정책)</li>
              <li>최적 가치 함수와 관련하여 탐욕적인 모든 정책은 최적 정책임.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Bellman optimality equations
        <ul>
          <li>최적 가치 함수을 만족하는 특별한 일관성 조건</li>
          <li>이론적으로 최적 가치 함수를 풀 수 있는 방정식</li>
          <li>최적 정책을 상대적으로 쉽게 결정할 수 있음</li>
        </ul>
      </li>
      <li>강화학습은 주어진 조건에 따라 다양한 방식으로 제기될 수 있음
        <ul>
          <li>에이전트의 지식
            <ul>
              <li>환경의 역학 (역학 함수 $p$ 의 4개의 인자) 을 아는 경우와 모르는 경우</li>
            </ul>
          </li>
          <li>계산 퍼포먼스 및 메모리 이슈
            <ul>
              <li>테이블 방식의 접근을 할지, 근사함수를 사용할 지에 대한 사항</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>강화학습 문제는 최적 솔루션을 찾는 것 보다, 어떻게 근사해야 할지에 더 집중하는 것이 바람직하다.</li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Value Functions" /><category term="Bellman Equations" /><summary type="html"><![CDATA[관련 자료 (RLbook2018 Pages 58-67)]]></summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 02. Week 2. Markov Decision Processes</title><link href="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 02. Week 2. Markov Decision Processes" /><published>2023-02-07T18:00:00+09:00</published><updated>2023-02-07T18:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2/"><![CDATA[<h2 id="course-introduction">Course Introduction</h2>

<ul>
  <li>문제를 접하게 되었을 때, 가장 중요한 단계가 문제를 Markov Decision Process(MDP) 로 전환하는 것이다.</li>
  <li>솔루션의 질은 이 전환과 밀접한 관계가 있다.</li>
</ul>

<h2 id="관련-자료-rlbook2018-pages-47-56">관련 자료 (RLbook2018 Pages 47-56)</h2>

<ul>
  <li>
    <p>Finite Markov Decision Processes (유한한 마르코프 결정 프로세스)</p>

    <ul>
      <li>bandits 문제와 같이 피드백에 대한 평가를 포함</li>
      <li>또한 연관성 측면 (다른 상황에서 다른 액션을 선택하는 것) 의 성격도 가지고 있음
        <ul>
          <li>bandits 문제는 동일 상황에서 다른 액션을 선택하는 것</li>
        </ul>
      </li>
      <li>MDP 는 고전적인 정형화된 sequential decision making (순차 결정) 이다.
        <ul>
          <li>액션이 즉각적인 보상에만 영향을 주는 것이 아닌,</li>
          <li>후속 상황(situations), 에이전트의 상태(states), 미래의 보상 (future rewards) 에 영향을 줌.</li>
          <li>그러므로 MDP 는 지연된 보상, 그리고 지연된 보상과 즉각적인 보상 간의 tradeoff (거래합의) 가 필요하다.</li>
        </ul>
      </li>
      <li>MDP 수식 관련
        <ul>
          <li>bandit problems
            <ul>
              <li>estimate the value $q_*(a)$</li>
            </ul>
          </li>
          <li>MDP
            <ul>
              <li>estimate the value $q_*(s,a)$ : a : action, s : state</li>
              <li>or estimate the value $\upsilon_*(s)$ : 상태 기반 추정값</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>MDP 는 강화학습 문제에 있어 수학적으로 이상적인 형태
        <ul>
          <li>수학적 구조 상 핵심 요소들 : 보상, 가치함수, 벨먼 방정식 등</li>
        </ul>
      </li>
      <li>강화학습은 적용 가능성의 폭과 수학적 취급 용이성 사이에 있어 tradeoff (거래합의) 가 필요하다.</li>
    </ul>
  </li>
  <li>
    <p>The Agent-Environment Interface</p>

    <p><img src="/assets/images/posts/3_1_1_agent_environment_in_MDP.png" alt="3_1_1_agent_environment_in_MDP" /></p>

    <ul>
      <li>MDP 는 목표 달성을 위해 상호작용을 통한 학습 문제에 있어 직접적인 틀이다.</li>
      <li>학습자 (learner) 와 결정자 (decision maker) 는 에이전트 (agent) 라 한다.</li>
      <li>에이전트 (agent) 밖의 모든 구성으로 상호작용을 하는 것을 환경 (environment) 이라 한다.</li>
      <li>상호작용은 연속적으로 발생한다.
        <ul>
          <li>에이전트는 액션을 선택하고 환경은 액션에 반응한다.
            <ul>
              <li>환경은 새로운 상황을 에이전트에 제공한다.</li>
              <li>환경은 에이전트에 보상을 제공한다.</li>
              <li>보상은 특정한 수치값으로 에이전트가 액션을 선택하는 것을 통해 최대화하길 원하는 값이다.</li>
            </ul>
          </li>
          <li>특별히, 에이전트는 환경과 이산적 타임스텝 (0, 1, 2, 3) 의 순서로 상호작용한다고 가정한다.
            <ul>
              <li>많은 아이디어들이 시간 연속적인 것이 될 수 있어도, 단순성을 유지하기 위해 케이스를 제한한다.</li>
            </ul>
          </li>
          <li>각 타임스텝 t 에 따라…
            <ul>
              <li>agent 는 환경의 상태 (environment’s state) 값을 받음 : $S_t \in$ S</li>
              <li>이에 따라 agent 는 액션을 선택함 : $A_t \in$ A$(s)$</li>
              <li>액션에 따른 결과로 에이전트는 수치적 보상을 얻음 : $R_{t+1} \in R \subset$ R</li>
              <li>이후 다른 환경의 상태 값을 제공받음 : $S_{t+1}$</li>
            </ul>
          </li>
          <li>위에 따라 MDP 와 에이전트는 아래와 같은 시퀀스, 혹은 궤적 (trajectory) 를 남기게 됨
            <ul>
              <li>$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, …$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/3_1_2_MDP_function_p_1.png" alt="3_1_2_MDP_function_p_1" /></p>

    <ul>
      <li>유한 MDP (finite MDP) 에서 모든 상태, 액션, 보상의 집합은 유한한 수의 요소 (element) 이다.</li>
      <li>$R_t$ 와 $S_t$ 는 이전 상태와 액션값에만 영향을 받는 이산확률분포 (discrete probability distributions) 로 정의됨
        <ul>
          <li>이산확률분포 : 확률변수가 가질 수 있는 값이 명확하고 셀 수 있는 경우의 분포 (주사위의 눈 $1, …, 6$)</li>
          <li>연속확률분포 : 확률변수가 가질 수 있는 값이 연속적인 실수여서 셀 수 없는 경우의 분포 ($y = f(x)$)</li>
        </ul>
      </li>
      <li>우선 제공된 state 와 action 에 따른 time t 시점의 확률 값이 존재 (상단 수식)</li>
      <li>모든 $s’, s \in S, r \in R, a \in A(s)$ 에 대해 function $p$ 는 dynamics of the MDP 로 정의된다.</li>
      <li>일반적인 함수 표현, The dynamics function $p : S \times R \times S \times A \to [0,1]$</li>
      <li>| : 조건부 확률에 대한 표기법이지만, 여기서는 $p$ 가 $s, a$ 의 각 선택에 대한 확률분포를 지정한다는 것을 뜻함.</li>
    </ul>

    <p><img src="/assets/images/posts/3_1_3_MDP_function_p_2.png" alt="3_1_3_MDP_function_p_2" /></p>

    <ul>
      <li>MDP (Markov decision process) 에서 $p$ 로 주어진 확률은 환경의 역학을 완벽히 특성화함.
        <ul>
          <li>즉, 가능한 $S_t$, $R_t$ 값에 대한 가능성은 이전에 즉시 제공된 $S_{t-1}$, $A_{t-1}$ 값에만 의존한다.</li>
          <li>그 이전의 상태값과 액션은 영향을 주지 않는다.</li>
          <li>이것은 결정 과정에 대한 제한이 아닌 상태에 대한 제한을 의미한다.</li>
          <li>상태에 미래의 차이를 발생 시킬 수 있는 이전 에이전트-환경 간 상호작용에 대한 측면이 모두 정보로 포함되어 있어야 한다.</li>
          <li>만약 그렇다면, 이 상태를 마르코프 속성 (Markov property) 이라 한다.
            <ul>
              <li>이후 과정에서 Markov 속성에 의존하지 않는 근사 방법 (approximation methods) 을 고려할 것</li>
              <li>17장에서는 Markov 가 아닌 관찰값으로부터 어떻게 Markov 상태를 구성하고 학습할지를 고려할 것</li>
            </ul>
          </li>
          <li>4개의 인수를 가진 dynamic function $p$ 에서 state-transition probabilities (상태 전이 확률) 와 같은 알고싶은 모든 것을 계산할 수 있음.</li>
        </ul>

        <p><img src="/assets/images/posts/3_1_4_MDP_function_p_3.png" alt="3_1_4_MDP_function_p_3" /></p>

        <p>$p : S \times S \times A \to [0,1]$</p>

        <ul>
          <li>또한 예상되는 보상값을 2개 인자 (state,action) 의 r 함수로 계산할 수 있음.</li>
        </ul>

        <p><img src="/assets/images/posts/3_1_5_MDP_function_p_4.png" alt="3_1_5_MDP_function_p_4" /></p>

        <p>$r : S \times \ A \to R$</p>

        <ul>
          <li>예상되는 보상값을 3개 인자 (state, action, next state) 의 r 함수로 계산할 수 있음.</li>
        </ul>

        <p><img src="/assets/images/posts/3_1_6_MDP_function_p_5.png" alt="3_1_6_MDP_function_p_5" /></p>

        <p>$r : S \times \ A \times S \to R$</p>

        <ul>
          <li>
            <p>이 책에서 우리는 주로 4개 인자의 $p$ 함수 (3.2) 를 사용하지만 때때로 다른 함수가 편리할 경우도 있음.</p>
          </li>
          <li>MDP 프레임워크는 추상적이고 유연하여 다양한 문제에 적용할 수 있음
            <ul>
              <li>스텝 (Step) 은 고정된 시간 간격일 필요가 없음. 의사 결정과 행동의 임의적 연속 단계일 수 있음</li>
              <li>동작 (Action) 은 로봇 팔의 모터 전압과 같이 낮은 수준의 제어부터 대학원에 갈 것인지 여부를 정하는 높은 수준의 결정일 수 있음.</li>
              <li>상태 (State) 는 직접적인 센서의 데이터처럼 낮은 수준의 감각일 수 있고, 방에 있는 물체에 대한 상직적 설명과 같이 높은 수준의 추상일 수 있음.</li>
              <li>또한 상태 (State) 는 과거 감각의 기억이거나 정신적, 주관적인 것일 수 있음.
                <ul>
                  <li>예를 들어 에이전트는 물체가 어디있는지 확신하지 못할 수 있음</li>
                </ul>
              </li>
              <li>액션 (Action) 은 정신적일수도, 계산적일 수도 있음
                <ul>
                  <li>일부 동작은 에이전트가 어디에 집중할지 를 변경하는 것일 수 있음</li>
                </ul>
              </li>
              <li>즉 액션은 우리가 결정을 내리는 방법을 배우고자 하는 모든 결정, 상태는 결정을 내리는 데 유용한 모든 것이 될 수 있음.</li>
            </ul>
          </li>
          <li>MDP 에서 에이전트와 환경의 경계는 일반적인 물리적 경계와 동일하지 않음.
            <ul>
              <li>대게 경계는 중간지점보다 에이전트에 더 가깝게 그려짐
                <ul>
                  <li>예를 들어 로봇의 기계적 감지 하드웨어는 에이전트의 일부가 아닌 환경의 일부로 간주되어야 함.</li>
                  <li>생물체일 경우 근육, 골격 및 감각 기관은 환경의 일부로 간주되어야 함.</li>
                </ul>
              </li>
              <li>보상 또한 인공 학습 시스템 본체 내부에서 계산되지만, 에이전트 외부에 있는 것으로 간주됨.</li>
            </ul>
          </li>
          <li>즉, 에이전트가 임의로 변경할 수 없는 모든 것은 환경의 일부로 간주한다는 것임.
            <ul>
              <li>에이전트는 환경의 일부를 알고 있음
                <ul>
                  <li>에이전트는 자신의 행동과 상태에 대해 보상이 어떻게 함수로 계산되는지에 대해 알고 있음.</li>
                  <li>그러나 보상 계산은 에이전트 밖의 영역임.</li>
                  <li>우리가 루빅큐브가 어떻게 작동하는지 알지만, 여전히 해결할 수 없는 문제가 될 수 있는 것처럼</li>
                  <li>환경이 어떻게 작동하는 지에 모든 것을 알고있으면서도 여전히 어려운 강화 학습 작업에 직면할 수 있음.</li>
                </ul>
              </li>
              <li>즉, 에이전트와 환경의 경계는 에이전트의 지식이 아닌 제어의 한계를 나타냄.</li>
            </ul>
          </li>
          <li>에이전트와 환경의 경계는 서로 다른 목적을 위해 다르게 위치할 수 있음
            <ul>
              <li>복잡한 로봇을 예로 들면 각각 고유한 경계를 가진 여러 에이전트가 한번에 작동할 수 있음
                <ul>
                  <li>높은 수준의 결정을 구현하는 에이전트가 낮은 수준의 에이전트가 직면한 상태의 일부를 형성하는 높은 수준의 결정을 내릴 수 있음</li>
                </ul>
              </li>
              <li>에이전트와 환경 간의 경계는 상태, 액션, 보상을 결정한 뒤 결정을 위한 관심사를 식별한 뒤에 정해짐.</li>
            </ul>
          </li>
          <li>즉, MDP 프레임워크는 상호작용으로부터 목표 지향적 학습을 하는 문제를 추상화한 것이다.
            <ul>
              <li>감각, 기억, 제어 장치의 세부사항이 무엇이든</li>
              <li>달성하려는 목표가 무엇이든</li>
              <li>목표 지향적 행동을 학습하는 문제는 에이전트와 그 환경 사이를 오가는 3가지 신호로 축소될 수 있다고 제안하는 것.
                <ul>
                  <li>Actions : 에이전트의 선택을 나타내는 신호</li>
                  <li>States : 선택이 이루어지는 기준(상태) 를 나타내는 신호</li>
                  <li>Rewards : 에이전트의 목표를 정의하는 신호</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>이 프레임워크는 모든 의사결정 학습문제를 유용하게 나타내기에 충분하지 않을 수 있지만, 널리 이용 가능하고 적용 가능한 것으로 입증됨.
            <ul>
              <li>물론 states 와 actions 는 작업마다 크게 다르며, 이것을 표현하는 방식이 성능에 큰 영향을 줌</li>
              <li>다른 종류의 학습과 마찬가지로 강화학습에서 이러한 표현 선택은 과학보다 예술에 가까움.</li>
              <li>이 책에서 우리는 상태와 행동을 나타내는 좋은 방법에 관한 몇 가지 조언과 예를 제공하지만,</li>
              <li>우리의 주요 초점은 일반적인 원칙 (표현법이 선택되면 그것이 어떻게 동작하는지) 을 배우는 것에 있음.</li>
            </ul>
          </li>
          <li>예제 3.1 : Bioreactor (바이오 리액터 - 세포배양기)
            <ul>
              <li>강화 학습이 세포배양기의 순간 온도와 교반 속도 (뒤섞는 교반기의 회전 속도) 를 결정하기 위해 적용되어 있다고 가정</li>
              <li>Actions : 목표 온도와 목표 교반속도를 달성하기 위해 발열체와 모터를 직접 조종하는 하위 제어시스템에 전달되는 값</li>
              <li>States : 여과되고 지연된 열전대, 기타 센서 값과 대상 화학물질을 나타내는 상징적 입력값 등</li>
              <li>Rewards : 유용한 화합물이 생성되었는지에 대한 순간적인 측정값</li>
              <li>여기서 상태 값은 list 혹은 vector 값 (센서 측정값, 상징적 입력값)</li>
              <li>액션 값은 vector 값 (목표 온도와 교반속도로 이루어진)</li>
              <li>보상 값은 항상 단일 숫자값</li>
            </ul>
          </li>
          <li>예제 3.2 : Pick-and-Place Robot
            <ul>
              <li>반복적인 물체 이동 작업에서 로봇 팔의 동작을 제어하기 위해 강화학습을 사용한다고 가정</li>
              <li>빠르고 부드러운 동작을 학습하려면 학습 에이전트가 모터를 직접 제어하고, 기계 연결장치의 현재 위치와 속도에 대한 짧은 지연시간의 정보를 가져와야 함</li>
              <li>Actions : 각 관절의 모터에 적용되는 전압</li>
              <li>States : 관절의 각도와 속도의 최신 값</li>
              <li>Rewards : 잘 운반된 개체에 대해 + 1 값</li>
              <li>부드러운 움직임을 장려하기 위해 각 단계의 순간순간의 갑작스러운 움직임에 작은 처벌 (보상의 음수값) 을 부여</li>
            </ul>
          </li>
          <li>예제 3.3 : Recycling Robot (재활용 로봇)
            <ul>
              <li>모바일 로봇은 사무실 환경에서 빈 음료수 캔을 모으는 일을 함</li>
              <li>로봇은 캔을 감지하는 센서, 캔을 집어 일체형 통에 넣을 수 있는 팔과 그리퍼를 가지고 있음</li>
              <li>로봇은 충전식 배터리로 작동</li>
              <li>로봇의 제어 시스템에는 센서 정보를 해석하는 파츠, 탐색기능 파츠, 팔과 그리퍼를 컨트롤 하는 파츠 가 있음.</li>
              <li>높은 수준의 의사결정 (빈 캔을 어떻게 찾을지) 은 현재 배터리 충전 레벨에 따라 강화학습 에이전트가 내림.</li>
              <li>간단한 예제를 만들기 위해 충전정도는 2가지 레벨만 구분한다고 가정, 작은 상태 집합 $S = \lbrace high,low \rbrace$  으로 구성</li>
              <li>각 상태에서 에이전트는 다음 중 하나의 결정을 할 수 있음.
                <ul>
                  <li>(1) 일정 시간 동안 적극적으로 캔을 찾음</li>
                  <li>(2) 움직이지 않고 누군가가 캔을 가져올 때까지 기다림</li>
                  <li>(3) 배터리 충전을 위해 충전 장소로 돌아감</li>
                </ul>
              </li>
              <li>배터리가 많이 남았을 때 (high), 충전은 언제나 멍청한 선택이므로 해당 상태의 action set 으로 포함시키지 않는다.
                <ul>
                  <li>action set 은 $A(high) = \lbrace search,wait \rbrace$ 와 $A(low) = \lbrace search, wait, recharge \rbrace$ 이다.</li>
                </ul>
              </li>
              <li>대부분의 경우 보상은 0 이지만, 로봇이 빈 캔을 확보하면 (+), 배터리가 완전히 방전되면 큰 수치의 (-) 가 됨.</li>
              <li>캔을 찾는 가장 좋은 방법은 능동적 탐색이지만 배터리가 소모됨 (배터리가 고갈될 가능성이 있음)</li>
              <li>기다리는 것은 배터리가 소모되지 않음</li>
              <li>배터리가 고갈되면 로봇은 종료되고 구조될 때까지 기다려야 함 (낮은 보상 생성)</li>
              <li>배터리가 많으면 배터리 고갈 위험 없이 빈 캔 탐색을 완료할 수 있음
                <ul>
                  <li>배터리가 많을 때의 탐색 기간에서 배터리가 많이 남아 있을 확률 $\alpha$</li>
                  <li>배터리가 많을 때의 탐색 기간에서 배터리가 적어질 확률 $1 - \alpha$</li>
                </ul>
              </li>
              <li>반대로 배터리가 적으면
                <ul>
                  <li>배터리가 적을 때의 탐색 기간에서 배터리가 적어질 확률 $\beta$</li>
                  <li>배터리가 적을 때의 탐색 기간에서 배터리가 고갈될 확률 $1 - \beta$</li>
                </ul>
              </li>
              <li>후자의 경우 로봇은 구조되어야만 하며, 이 경우 배터리는 다시 완충된다.</li>
              <li>로봇에 의해 수집된 캔의 수량 만큼 보상이 주어지며, 로봇이 구출되었을 때 보상값 -3 이 주어진다.</li>
              <li>$r_{search}, r_{wait}$ with $r_{search} &gt; r_{wait}$ : 로봇이 검색 혹은 대기 중에 수집할 것으로 추정되는 캔 수 (즉, 예상되는 보상)</li>
              <li>충전을 위해 충전 장소로 돌아갈 때나, 배터리가 고갈되었을 때에는 캔을 회수할 수 없음</li>
              <li>이 시스템은 유한한 MDP 이며 전확 확률과 예상 보상을 아래와 같이 표기할 수 있다.</li>
            </ul>

            <p><img src="/assets/images/posts/3_1_7_MDP_transition_probabilities.png" alt="3_1_7_MDP_transition_probabilities" /></p>

            <ul>
              <li>위 표는 현 상태 $s$, 액션 $a \in A(s)$, 다음 상태 $s’$ 의 각각의 가능한 조합으로 구성된 표이다.</li>
              <li>몇몇 전환은 일어날 가능성이 0 이므로, 기대되는 보상 또한 없다.</li>
            </ul>

            <p><img src="/assets/images/posts/3_1_8_MDP_transition_graph.png" alt="3_1_8_MDP_transition_graph" /></p>

            <ul>
              <li>위 그래프는 transition graph 로, 유한 MDP 의 역학을 요약하는 또다른 방법이다.</li>
              <li>state nodes 와 action nodes 가 있음.</li>
              <li>state nodes : 큰 빈 원, 원 안에 라벨링 된 이름</li>
              <li>action nodes : 작은 검은색 원과 state nodes 를 연결 시키는 화살표</li>
              <li>상태 $s$ 에서 작업 $a$ 를 수행하면 상태노드 $s$ 에서 작업노드 $(s,a)$ 로 이동함.</li>
              <li>그 다음 환경은 $(s,a)$ 를 떠나는 화살표 중 하나를 통해 다음 상태의 노드 $s’$ 로 전환시킴</li>
              <li>각 화살표는 $(s, s’, a)$ 에 해당함.</li>
              <li>여기서 $s’$ 는 다음 상태이고, 전환확률 $p(s’|s,a)$ 와 해당 전환에 대한 예상보상은 $r(s,a,s’)$ 임.</li>
              <li>작업 노드를 떠나는 화살표의 전환 확률의 합계는 항상 1임.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Goals and Rewards</p>

    <ul>
      <li>강화학습의 목적, 목표는 특별한 신호로 정형화 되는데 이를 보상 (Reward) 이라 하고 환경이 에이전트에 전달하는 값이다.</li>
      <li>각 스텝마다 보상은 단순한 숫자 값이다. $R_t \in R$</li>
      <li>비공식적으로, 에이전트의 목표는 총 보상의 양을 최대화 하는 것이다.
        <ul>
          <li>이 최대화의 의미는 당장의 보상을 최대화 하는 것이 아닌 장기적 누적 보상을 최대화 하는 것이다.</li>
        </ul>
      </li>
      <li>보상 측면에서 위 목표를 공식화하는 것이 처음에는 제한적으로 보일 수 있지만, 실은 유연하고 광범위하게 적용될 수 있다.
        <ul>
          <li>예 1. 로봇이 걷는 법을 배움 : 보상은 로봇이 전진할 때마다 주어짐</li>
          <li>예 2. 로봇이 미로를 통과하는 법을 배움 : 매 스텝마다 -1 의 보상을 주어 에이전트가 최대한 빨리 미로를 탈출하는 것을 장려한다.</li>
          <li>예 3. 재활용 캔 수집을 하는 법을 배움 : 캔을 모을 때마다 +1 의 보상을 주며, 뭔가에 부딪히거나 누군가 소리를 지를 경우 - 보상을 준다.</li>
          <li>예 4. 체스 두는 법을 배움 : 이길 때 +1점, 질 때 -1점, 비길 때 0점의 보상을 부여</li>
        </ul>
      </li>
      <li>즉, 에이전트가 우리의 목표를 달성하게 하기 위해 최대화 할 수 있는 보상을 제공해줘야 한다.
        <ul>
          <li>따라서 보상이 정말 우리가 원하는 결과를 나타내는 것인지가 매우 중요하다.</li>
        </ul>
      </li>
      <li>보상은 우리가 원하는 것을 달성하기 위한 사전지식을 전달하는 곳이 아니다.
        <ul>
          <li>이것에 적합한 위치는 초기 정책 또는 초기 가치 함수, 혹은 이들에 영향을 줄 수 있는 값이다.</li>
          <li>예를 들어 체스 게임일 경우 상대의 말을 잡거나 센터를 장악하는 것에 보상이 주어저서는 안 된다.
            <ul>
              <li>에이전트는 승패의 여부와 관계없이 이 서브 목표를 달성하는 것을 학습할 것이다.</li>
            </ul>
          </li>
          <li>즉, 보상은 에이전트에게 달성해야 하는 것에 대한 소통을 하는 방법이고, 어떻게 달성할지에 대한 것을 이야기 하는 것이 아니다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Returns and Episodes</p>

    <p><img src="/assets/images/posts/3_3_1_MDP_sum_of_rewards.png" alt="3_3_1_MDP_sum_of_rewards" /></p>
    <ul>
      <li>정확히 시퀀스의 어떤 면을 최대화 하길 원하는지에 대하여, 가장 단순히 보상의 합계를 표현한 형태</li>
      <li>final time step $T$ 라는 개념이 있을 경우
        <ul>
          <li>환경과 에이전트 간 상호작용이 자연스럽게 끊길 때, 우리는 이것을 에피소드 (episode) 라 표현</li>
          <li>예를 들어 게임 한 판, 하나의 미로를 통과하는 것 혹은 어떠한 종류의 반복적인 상호작용 등</li>
        </ul>
      </li>
      <li>에피소드가 끝난 상태를 terminal state 라 한다.
        <ul>
          <li>이후 표준 시작 상태로 재설정하거나, 시작 상태의 표준 분포에서 샘플을 재설정한다.</li>
          <li>에피소드가 다른 방식으로 끝나더라도 (예를들어 게임에서의 승리,패배) 다음 에피소드는 이전 에피소드와 독립적으로 시행된다.</li>
          <li>즉 에피소드는 모두 terminal state 로 끝나나, 보상과 결과는 다를 수 있다.</li>
        </ul>
      </li>
      <li>위와 같은 에피소드 형태의 일을 episodic tasks 라 한다.
        <ul>
          <li>$S$ : 모든 non-terminal state</li>
          <li>$S^+$ : 모든 non-terminal state + terminal state</li>
          <li>$T$ : The time of termination</li>
        </ul>
      </li>
      <li>반면 많은 경우 환경과 에이전트의 상호작용은 자연스럽게 별개의 에피소드로 끊기지 않는다.
        <ul>
          <li>이를 continuing tasks 라 한다.</li>
          <li>final time step $T=\infty$ 이기 때문에 위 3.7의 수식으로 표현할 수 없다.</li>
          <li>(우리가 최대화를 원하는 리턴 값이 쉽게 무한대의 값이 될 수 있다.)</li>
          <li>즉, discounting 이라는 추가적 개념이 필요하다.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/3_3_2_MDP_discounted_return.png" alt="3_3_2_MDP_discounted_return" /></p>
    <ul>
      <li>에이전트는 미래의 할인된 보상값의 합 (sum of the discounted rewards) 이 최대값이 되도록 액션 ($A_t$)을 선택한다.</li>
      <li>$0 \le \gamma \le 1$ 인 $\gamma$ 파라미터를 discount rate 라 한다.</li>
      <li>discount rate 는 미래 보상의 현재 가치를 결정한다.
        <ul>
          <li>미래 $k$ time steps 에서 받을 보상은 현재의 가치로 따졌을 때 $\gamma^{k-1}$ 배의 가치만 있다.</li>
          <li>$\gamma &lt; 1$ : 극한 합 (3.8) 의 값이 유한한 값으로 수렴한다. ($R_k$ 가 제한된 값일 경우)</li>
          <li>$\gamma = 0$ : 에이전트는 오직 $A_t$ 를 선택해 $R_{t+1}$ 값을 최대화하는 방법만을 학습한다. (근시안적)</li>
          <li>$\gamma \to 1$ : 1에 가까워질 수록 미래의 보상값을 더 강하게 계산. (원시안적)</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/3_3_3_MDP_discounted_return.png" alt="3_3_3_MDP_discounted_return" /></p>
    <ul>
      <li>연속적인 time step (successive time step) 에서의 보상은 상호간 연관관계에 있고, 이는 강화학습에서 중요한 개념이다.</li>
      <li>위 공식은 모든 time steps $t &lt; T$ 에서 적용되며, 심지어 $t + 1$ 에서 termination 이 일어나는 경우에도 $G_T = 0$ 으로 정의함으로써 적용가능하다.</li>
      <li>3.8 의 인자 수가 무한대이지만, 보상이 0 이 아니고 상수 일 경우 여전히 유한한 숫자가 된다.
        <ul>
          <li>예를 들어 보상이 상수 $+1$ 이고 , $\gamma &lt; 1$ 일 경우 보상 값은 아래의 값이 된다.
  <img src="/assets/images/posts/3_3_4_MDP_discounted_return.png" alt="3_3_4_MDP_discounted_return" /></li>
        </ul>
      </li>
      <li>
        <p>예제 3.4 : Pole-Balancing</p>

        <p><img src="/assets/images/posts/3_4_1_cart_pole.png" alt="3_4_1_cart_pole" /></p>

        <ul>
          <li>목표 : 카트에 힘을 적용하여 트랙 위를 카트가 이동할 수 있되, 폴이 서 있고 넘어지지 않게 유지하는 것</li>
          <li>실패 : 폴이 주어진 각도를 넘어서 넘어지는것 혹은 카트가 트랙에서 탈선하는 것</li>
          <li>폴은 매 실패 이후 리셋되어 수직으로 서게 된다 &gt; 이 일은 episodic 으로 간주될 수 있음
            <ul>
              <li>자연스러운 에피소드가 반복됨</li>
              <li>보상은 실패하기 전 모든 타임 스텝에 +1 을 줄 수 있다.</li>
            </ul>
          </li>
          <li>위의 설정은, 폴이 영원히 성공적으로 벨런스를 잡게 되면 보상은 무한수가 된다는 뜻이다.</li>
          <li>또는 위 문제를 continuing task 로 간주할 수도 있다. (using discounting)
            <ul>
              <li>위의 경우 실패할 경우 보상은 -1 이 되고, 그 외에는 0 이 된다.</li>
              <li>즉 보상 값은 $- \gamma^K$ 가 되며, $K$ 는 실패 이전의 타임스텝이 된다.</li>
            </ul>
          </li>
          <li>양쪽의 경우 모두 폴의 균형을 최대한 유지할 수록 보상이 최대화 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="introduction-to-markov-decision-processes">Introduction to Markov Decision Processes</h2>

<ul>
  <li>학습목표
    <ul>
      <li>Markov Decision Process (MDP) 에 대해 이해하기</li>
      <li>MDP 프로세스가 정의되는 방법 이해</li>
      <li>Markov Decision Process 의 그래픽 표현 이해</li>
      <li>MDP 프레임워크로 다양한 프로세스를 작성하는 방법</li>
    </ul>
  </li>
  <li>Markov Decision Processes
    <ul>
      <li>k-Armed Bandit Problem : 같은 상황에서 같은 Action 이 항상 최적화된 선택이 되는 문제유형 (현실과 다름)
        <ul>
          <li>예 : 토끼의 왼쪽에 브로콜리 (보상 : +3) 가 있고 오른쪽에 당근 (보상 : +10) 이 있을 경우</li>
          <li>토끼는 오른쪽의 당근을 먹는 것을 선택한다.</li>
          <li>갑자기 두 음식의 위치가 바뀔경우 k-Armed Bandit Problem 으로 정의할 수 없는 문제가 된다.</li>
        </ul>
      </li>
      <li>현실 : 다른 상황이 다른 반응을 야기하며, 현재 선택한 Action 이 미래의 보상의 양을 결정한다.
        <ul>
          <li>예 : 토끼의 왼쪽에 브로콜리 (보상 : +3) 가 있고 오른쪽에 당근 (보상 : +10) 이 있을 경우</li>
          <li>단, 오른쪽 방향에 호랑이가 있음</li>
          <li>장기적 이익 관점에서 토끼는 브로콜리를 선택해야 한다. (즉각적 보상이 적다 하더라도)</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/The_dynamics_of_an_MDP.png" alt="The_dynamics_of_an_MDP" /></p>
    <ul>
      <li>The dynamics of an MDP
        <ul>
          <li>출처 : Coursera / Fundamentals of Reinforcement Learning / W2. Markov Decision Process / 동영상 : Markov Decision Processes</li>
          <li>The dynamics of an MDP : 확률 분포에 의해 정의됨.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Examples of MDPs
    <ul>
      <li>Recycling Robot 에 대한 예시
        <ul>
          <li>State : 배터리 잔량 High, 배터리 잔량 Low</li>
          <li>Action :
            <ul>
              <li>배터리 잔량 High : Search, Wait, Recharge</li>
              <li>배터리 잔량 Low : Search, Wait</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/Dynamics_of_the_Recycling_Robot.png" alt="Dynamics_of_the_Recycling_Robot" /></p>
    <ul>
      <li>Dynamics_of_the_Recycling_Robot
        <ul>
          <li>출처 : Coursera / Fundamentals of Reinforcement Learning / W2. Markov Decision Process / 동영상 : Examples of MDPs</li>
        </ul>
      </li>
      <li>MDP 는 다양한 문제에 적용이 가능하다.</li>
    </ul>
  </li>
</ul>

<h2 id="goal-of-reinforcement-learning">Goal of Reinforcement Learning</h2>

<ul>
  <li>학습목표
    <ul>
      <li>에이전트의 목표와 보상이 어떻게 관련되는지 설명</li>
      <li>에피소드에 대한 이해 및 에피소드로 표현될 수 있는 작업 식별</li>
    </ul>
  </li>
  <li>The Goal of Reinforcement Learning
    <ul>
      <li>Bandits Problems : 즉각적 보상의 최대화</li>
      <li>MDP : 타임스텝 전체의 보상 합의 최대화
        <ul>
          <li>토끼가 오른쪽의 당근을 먹는 것은 즉각적 보상의 최대화를 야기하나 이후 바뀐 상태에 의해 호랑이에 잡아먹힘</li>
          <li>로봇에게 걷는 방법을 가르쳐 줄 때, 보상 값이 앞으로 나아간 수치라 하여 이를 즉각적으로 최대화 할 경우 넘어져 걷질 못하게 됨</li>
        </ul>
      </li>
      <li>Episodic  Tasks : 마지막 스텝을 밟은 뒤 다시 초기상태로 돌아감 (에이전트-환경 간 상호작용의 종료)
        <ul>
          <li>체스게임</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Michael Littman: The Reward Hypothesis
    <ul>
      <li>Reinforcement Learning 에서 보상을 정의하는 것
        <ul>
          <li>물고기를 주면 하루를 먹고 : Programming (Good Old-Fashioned AI) &gt; 새로운 환경에서 새 프로그래밍을 해야 함</li>
          <li>물고기를 잡는 법을 알려주면 평생을 먹고 : Supervised Learning &gt; 학습 데이터를 제공해야 함</li>
          <li>물고기의 맛을 알려주면 어떻게 물고기를 잡을지 알아낸다. : Reinforcement learning &gt; 최적화문제</li>
          <li>Ashraf : All goals can be described by the maximization of expected cumulative rewards</li>
          <li>Silver : All goasl can be described by the maximization of expected cumulative rewards</li>
          <li>Sutton : What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).</li>
        </ul>
      </li>
      <li>강화학습의 정의
        <ul>
          <li>Intelligent behavior arises from the actions of an individual seeking to maximize its received reward signals in a complex and changing world.
            <ul>
              <li>identify where reward signals come from,</li>
              <li>develop algorithms that search the space of behaviors to maximize reward signals.</li>
            </ul>
          </li>
          <li>보상은 복잡한 실 세계 속에서 단순해야 한다.</li>
          <li>에이전트가 최적화해야 할 보상이 무엇인지 알아내야 한다.</li>
          <li>위 보상을 최대화 할 알고리즘을 디자인해야 한다.</li>
        </ul>
      </li>
      <li>보상 정의의 어려움
        <ul>
          <li>공통 통화 정의의 어려움
            <ul>
              <li>온도조절장치 제어의 경우 : 난방/에어컨 등의 에너지 비용 vs 거주자의 불편도?</li>
              <li>위의 정도를 달러로 표기? (자연스럽지가 않음)</li>
            </ul>
          </li>
          <li>하나의 예는 목적을 달성했을 때 +1, 그렇지 않은 경우 0 (goal reward Representation)</li>
          <li>반대로 목적이 달성하지 않았을때  매 스텝마다 -1 (action penalty Representation)</li>
        </ul>
      </li>
      <li>보상의 정의 방식에 따른 차이 발생
        <ul>
          <li>Goal-reward  Representation : 긴급함이 없음</li>
          <li>Action-Penalty Representation : 진행이 막혀 더이상 목적을 달성할 수 없는 확률이 있을 경우 오작동</li>
          <li>두 경우 모두 흐름이 길 경우 큰 문제가 발생함.</li>
          <li>어떠한 중간 보상은 에이전트가 올바른 방향으로 학습하는데 큰 도움이 될 수도 있음</li>
          <li>보상의 출처가 사람일 경우
            <ul>
              <li>사람은 보상 기능과 다르게 에이전트가 학습하는 방식에 따라 보상을 변경하는 경향이 있음 (Non-stationary Rewards)
                <ul>
                  <li>고전적인 강화학습 알고리즘은 위와 같은 경우 잘 적용되지 않음</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>보상의 지정
        <ul>
          <li>프로그래밍 방식
            <ul>
              <li>학습 에이전트에 대한 보상을 정의하는 가장 일반적인 방법</li>
              <li>예 : 상태를 가져오고 보상을 출력하는 프로그램을 작성</li>
              <li>프로그래밍 언어의 예 - temporal logic : 사람과 기계 언어의 중간 지점</li>
            </ul>
          </li>
          <li>예를 들어 사람이 주는 보상을 복사하는 방법을 배우는 것 : Mimic reward</li>
          <li>이 접근 방식의 흥미로운 버전 : 역 강화학습 : Inverse Reinforcement Learning
            <ul>
              <li>역 강화학습에서 지도자는 원하는 행동의 예를 보여주고 학습자는 지도자가 이 행동을 최적으로 만들기 위해 최대화한 보상이 무엇인지 파악</li>
              <li>강화학습이 보상에서 행동으로 진행되는 반면, 역강화학습은 행동에서 보상으로 진행됨.</li>
              <li>일단 보상이 식별되면 이러한 보상은 다른 설정에서 최대화 될 수 있으므로 환경 간에 강력한 일반화가 가능함.</li>
            </ul>
          </li>
          <li>최적화 프로세스를 통해 간접적으로 파생되는 보상
            <ul>
              <li>점수를 생성할 수 있는 높은 수준의 행동이 있는 경우</li>
              <li>해당 행동을 장려하는 보상을 고려할 수 있음</li>
            </ul>
          </li>
          <li>Meta reinforcement learning
            <ul>
              <li>동 목표를 수행하는 단일 에이전트가 아닌 다수 에이전트가 있을 경우</li>
              <li>행동의 결과로 보상을 받는 것 뿐만 아닌 이 행동에 대한 인센티브로 보상을 사용하여 평가할 수 있게 함</li>
              <li>강화학습 에이전트는 더 좋은 보상 기능과 이를 최대화하기 위한 더 좋은 알고리즘이 있는 경우 생존함</li>
              <li>개별 수준에서 더 나은 학습 방법을 만드는 진화 수준에서의 학습방법</li>
              <li>에이전트가 자손에게 보상 함수를 계승함</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>보상 이론의 도전과제
        <ul>
          <li>보상을 극대화 하는 것 이외에 다른 일을 하는것 처럼 보이는 행동의 예
            <ul>
              <li>위험 회피 행동 : 최선이 아닐 수 있지만 최악의 결과가 발생할 가능성을 최소화 하는 행동을 선택</li>
            </ul>
          </li>
          <li>원하는 행동이 항상 최선의 것이 아니라 여러 가지를 균형있게 수행하는 것이라면?
            <ul>
              <li>예 : 음악 추천 시스템 (최근 재생된 곡의 경우 그 곡에 대한 보상이 줄어야 함)</li>
            </ul>
          </li>
          <li>올바른 결정이 단순히 하나의 목표를 추구하는 것이 아니라면?
            <ul>
              <li>더 나은 목표를 만드는 것</li>
            </ul>
          </li>
          <li>그럼에도 보상을 극대화 하는 것이 지능형 에이전트에게 동기를 부여하는 훌륭한 근사치일 수 있다는 가능성을 염두</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="continuing-tasks">Continuing Tasks</h2>

<ul>
  <li>학습목표
    <ul>
      <li>연속적인 작업에 대해 할인을 이용한 보상의 수식화</li>
      <li>연속적인 time step 에서 보상이 상호간 어떻게 관련되어 있는지 확인</li>
    </ul>
  </li>
  <li>Continuing Tasks
    <ul>
      <li>
        <p>Episodic tasks 와 달리 현실에서는 에이전트와 환경이 계속 상호작용 하는 경우 (Continuing) 가 많다.</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: left">Episodic Tasks</th>
              <th style="text-align: left">Continuing Tasks</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: left">상호작용이 자연스럽게 끝남</td>
              <td style="text-align: left">상호작용이 계속 유지됨</td>
            </tr>
            <tr>
              <td style="text-align: left">각각의 에피소드는 Terminal State 로 끝남</td>
              <td style="text-align: left">Terminal state 가 없음</td>
            </tr>
            <tr>
              <td style="text-align: left">에피소드는 상호독립적임</td>
              <td style="text-align: left"> </td>
            </tr>
            <tr>
              <td style="text-align: left">$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + … + R_T$</td>
              <td style="text-align: left">$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + …$ (무한)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>예 ) 건물 안 온도조절기 : 환경과의 상호작용에 끝이 없음.
        <ul>
          <li>상태 : 시간, 건물 내 사람 수</li>
          <li>액션 : 켜기, 끄기</li>
          <li>보상 : 누군가가 온도를 수동조절 할 경우 -1, 그렇지 않은 경우 0</li>
        </ul>
      </li>
      <li>State 가 무한함에 따라 보상의 합이 무한으로 발산하는 것을 방지하기 위해 Discounting 을 한다.
        <ul>
          <li>Discounting 계수 $\gamma$ , $0 \leq \gamma &lt; 1$</li>
          <li>$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + … + \gamma^{k-1} R_{t+k} + … =  \sum_{k=0}^\infty\gamma^k R_{t+k+1}$</li>
          <li>이것은 현재의 1 달러가 1년 뒤에 받을 1달러 보다 가치가 있다고 생각하면 이치에 맞는다.</li>
          <li>$G_t$ 가 무한하지 않음 (Finite) 을 증명하는 수식
            <ul>
              <li>$G_t = \sum_{k=0}^\infty\gamma^k R_{t+k+1} \leq \sum_{k=0}^\infty\gamma^k R_{max} = R_{max} \sum_{k=0}^\infty\gamma^k = R_{max} \times {1 \over 1-\gamma}$</li>
            </ul>
          </li>
          <li>$\gamma$ 값에 따른 에이전트의 성향
            <ul>
              <li>$\gamma = 0$ : 즉각적인 보상만을 추구하는 에이전트 (Short-sighted)</li>
              <li>$\gamma \to 1$ : 미래의 보상을 중시함 (Far-sighted)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Return 값의 재귀적 표현법
        <ul>
          <li>$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + … = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + … )$</li>
          <li>$G_t = R_{t+1} + \gamma G_{t+1}$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Markov Decision Processes" /><summary type="html"><![CDATA[Course Introduction]]></summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 01. Week 1</title><link href="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 01. Week 1" /><published>2023-01-26T18:00:00+09:00</published><updated>2023-01-26T18:00:00+09:00</updated><id>http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1</id><content type="html" xml:base="http://localhost:4000/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/"><![CDATA[<h2 id="course-introduction">Course Introduction</h2>

<ul>
  <li>Supervised Learning (지도학습)
    <ul>
      <li>학습자가 답이 기입되어 있는 라벨링된 예시에 접근함</li>
      <li>정답이 무엇이었는지 말해주는 선생이 존재</li>
      <li><strong>학습데이터 : 정답 (혹은 정답인 동작) 을 가르쳐주는 정보를 학습한다.</strong></li>
      <li><strong>피드백 : 취해야 할 올바른 조치를 나타낸다. (이번 행동이 얼마나 좋았는지는 알려주지 않는다.)</strong>
        <ul>
          <li>피드백은 행동에 대해 완전 독립적.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Unsupervised Learning (자율학습)
    <ul>
      <li>데이터 기저에 있는 구조를 추출 (데이터 표현법)
        <ul>
          <li>이 데이터 표현법은 지도학습이나 강화학습에 도움을 줄 수 있음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Reinforcement Learning (강화학습)
    <ul>
      <li>학습자에게 최근의 행동에 대한 보상을 제공함</li>
      <li>좋은행동이 어떤것일지 식별해주는 환경이 존재하나 정확히 어떻게 해야하는지는 알려주지 않음</li>
      <li>지도학습이나 자율학습을 통해 일반화 (Input 이 달라져도 출력성능에 영향을 주지 않음) 를 개선할 수 있음</li>
      <li>바뀌는 환경속에서 상호작용하며 학습하는 것에 주안점을 둠
        <ul>
          <li>학습자가 단순히 반복되는 환경에서 계산을 통해 좋은 행동이 무엇인지 학습하는 것이 아님.</li>
          <li>학습자가 시행착오를 통해 변화하는 환경에서 목표를 바꾸어 가며 더 잘하는 것을 추구</li>
        </ul>
      </li>
      <li><strong>학습데이터 : 행동을 수행한 것에대한 평가를 학습한다.</strong></li>
      <li><strong>피드백 : 이번 행동이 얼마나 좋았는지를 피드백한다. (그것이 최선/최악인지는 알 수 없다)</strong>
        <ul>
          <li>피드백은 행동에 완전 종속됨.</li>
          <li>때문에 최선의 행동을 찾아 끊임없이 탐험해야 함.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="관련-자료-rlbook2018-pages-25-36">관련 자료 (RLbook2018 Pages 25-36)</h2>

<ul>
  <li>용어설명
    <ul>
      <li>Stationary (정상성) Probability : 여러 시간 구간마다 통계적 성질이 동일한 것
        <ul>
          <li>예) 주사위 던지기 : 시간에 무관하게 동일한 확률값</li>
        </ul>
      </li>
      <li>Non-Stationary (비정상성) Probability : 시간에 따라 통계적 성질이 변하는 것
        <ul>
          <li>예) 변덕스러운 날씨, 기후 등</li>
        </ul>
      </li>
      <li>Nonassociative setting : 다른 Action(행동), 다른 Environment(환경) 을 가정하지 않는 세팅
        <ul>
          <li>full reinforcement learning problem 의 복잡성을 배제 (다양한 환경, 비정상성, 행동의 보상이 즉각적이지 않음 - 현실세계)</li>
          <li>이미 환경이 어떠한 피드백을 줄지 알고 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Multi-armed Bandits
    <ul>
      <li>간단한 세팅 환경 (K-armed bandit problem 의 단순화 버전)
        <ul>
          <li>한가지 이상 상황에서의 동작을 학습하는 것을 배제 (Nonassociative setting)</li>
          <li>목적1 : 평가 피드백 (Evaluative feedback) 이 정답을 알려주는 피드백 (Instructive Feedback) 과 어떻게 다른지 확인</li>
          <li>목적2 : 둘이 결합될 수 있는지 확인</li>
        </ul>
      </li>
      <li>목표
        <ul>
          <li>기초적 학습법 (Learning methods) 안내</li>
          <li>이 Bandit problem 이 associative (연관성) 성질을 가지게 되었을 때 어떻게 변할지 확인</li>
        </ul>
      </li>
      <li>A k-armed Bandit Problem
        <ul>
          <li>문제에 대한 설명
            <ul>
              <li>반복적으로 k 개의 다른 옵션/행동을 선택</li>
              <li>각 선택 마다 보상 (선택한 행동에 따른 정상성 확률 분산을 가진 보상) 을 받는 문제</li>
              <li>목적 : 특정 기간동안 최대한의 보상을 받는 것</li>
            </ul>
          </li>
          <li>문제에 대한 예시 1
            <ul>
              <li>슬롯머신 혹은 “one-armed bandit” 문제와 동일하나 단지 레버가 k 가임.</li>
              <li>각 행동은 여러 대의 슬롯머신 중 한 대의 레버를 당기는 것과 동일</li>
              <li>가장 이익을 극대화 할 수 있는 레버에 집중하여 보상을 많이 받는 것이 목표</li>
            </ul>
          </li>
          <li>문제에 대한 예시 2
            <ul>
              <li>의사가 심각한 질병의 환자에게 실험적 치료들 중 하나를 선택</li>
              <li>보상 : 환자의 생존/ 치유율</li>
            </ul>
          </li>
          <li>
            <p>수식설명</p>

            <p><img src="/assets/images/posts/2_1_1_value_of_action.png" alt="2_1_1_value_of_action" /></p>

            <ul>
              <li>위 수식은 문제를 풀었을 때 ($q_*$) 의 value of the action 에 대한 수식이다.</li>
              <li>value of the action : 각각의 k 행동들이 가지는 기대/평균 보상값</li>
              <li>$A_t$ : time-step $t$ 시점에 선택한 action</li>
              <li>$R_t$ : $A_t$ 에 상응하는 보상</li>
              <li>$q_*(a)$ : $a$ 가 선택되었을때 기대되는 보상값</li>
              <li>$\doteq$ : is defined as</li>
              <li>If you knew the “value of the each action” : 해당 문제를 풀었다고 볼 수 있음.</li>
              <li>$Q_t(a)$ : $q_*(a)$ 와 유사한 값 (중간값)</li>
              <li>$q_*$ 값은 에이전트가 알고 있는 값이 아님</li>
            </ul>
          </li>
          <li>greedy actions
            <ul>
              <li>action value 를 계속 추정하다 보면 어느 시점에서나 적어도 하나의 가장 큰 예측값을 가지는 action 이 존재</li>
              <li>이것을 greedy actions 라고 함.</li>
              <li>이 greedy action 을 선택하는 것 : 현재 알고 있는 values of the action 값을 exploiting 한다.</li>
              <li>이 greedy action 을 선택하지 않는 것 : 현재 exploring 중이라고 한다.</li>
              <li>Exploitation (이기적 이용)
                <ul>
                  <li>현 step 에서 예측되는 보상값을 최대화 하는 옳은 방법</li>
                </ul>
              </li>
              <li>Exploration (탐색)
                <ul>
                  <li>장기 관점으로 보았을 때 이쪽의 보상 총합이 더 클 수 있음</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Balancing exploration and exploitation
            <ul>
              <li>k-armed bandit 과 이와 유사한 문제의 특정 수학적 공식에 대해 탐색과 이용의 밸런스를 맞출 수 있는 정교한 방법이 존재
                <ul>
                  <li>하지만 이런 방법들은 정상성에대한 강한 가정을 전재하고, full reinforcement learning 환경이나 어플리케이션을 침해하거나  증명할 수 없는 사전지식을 이용한다.</li>
                  <li>즉 활용 불가. (Full reinforcement learning 환경에서 이 밸런스 문제는 도전적인 과제임)</li>
                </ul>
              </li>
              <li>k-armed bandit problem 에서는 균형에 대해서만 고려하고, 단순 Exploitation 하는 것보다 균형을 맞춘 방식이 더 잘 작동한다는 것을 증명할 것임.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Action-value Methods
        <ul>
          <li>행동 선택을 위해 행동(Action) 에 대한 가치를 측정하는 방법을 통상 Action-value method 라 한다.</li>
          <li>
            <p>수식설명 1</p>

            <p><img src="/assets/images/posts/2_2_1_averaging_the_rewards.png" alt="2_2_1_averaging_the_rewards" /></p>

            <ul>
              <li>위 수식은 action-value method 중 보상평균값으로 추정하는 방식에 대한 수식이다.</li>
              <li>Action-value 는 통상적으로 해당 Action 이 선택되었을 때 보상 값의 평균치를 말한다.</li>
              <li>1(predictate) 는 행위를 하였을 때는 1, 아닌 경우 0 (가상의 1/0, 횟수의 개념)</li>
              <li>분모가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.</li>
              <li>이 방식을 sample-average method 라 부르며, 이는 Action-value 를 구하기 위한 많은 방법 중 하나이다.</li>
            </ul>
          </li>
          <li>
            <p>수식설명 2</p>

            <p><img src="/assets/images/posts/2_2_2_greedy_action_selection_method.png" alt="2_2_2_greedy_action_selection_method" /></p>

            <ul>
              <li>위 수식은 greedy action selection method 에 대한 수식이다.</li>
              <li>action selection 에서 가장 단순한 방법은 가장 큰 예측값에 해당하는 action을 선택하는 것이다.</li>
              <li>현 시점 이전까지 가장 탐욕적인 action 으로 정의된 행동을 하는 것.</li>
              <li>$\underset{a}{\arg\max}$ : 후술되는 값이 최대값이 되는 action $a$ 를 의미</li>
              <li>Greedy action selection 은 현 지식을 이용해 당장의 보상을 최대화 하는 방식</li>
              <li>정말 더 나은 방식을 찾기 위해 열등한 방식을 샘플링하는데 시간을 할애하지 않음.</li>
            </ul>
          </li>
          <li>$\varepsilon$-greedy methods
            <ul>
              <li>위와 다른 대안으로 대부분 탐욕스러운 행동을 하되</li>
              <li>아주 작은 확률($\varepsilon$)로 action-value 와 관계 없이 균등한 확률로 $a$ 를 선택하는 방법이 있음.</li>
              <li>이 방법을 통한 샘플링 횟수가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.</li>
              <li>이것은 점근적인 보장일 뿐, 실질적인 효과에 대한 것은 아니다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The 10-armed Testbed
    <ul>
      <li>
        <p>조건 설명</p>

        <p><img src="/assets/images/posts/2_3_1_10_armed_testbed.png" alt="2_3_1_10_armed_testbed" /></p>

        <ul>
          <li>10-armed testbed 의 bandit problem 문제 예시</li>
          <li>각 10 개 action 의 true value $q_*(a)$ 값 은 평균 0, 분산 1 인 정규분포 (표준정규분포) 를 따른다.</li>
          <li>위 조건으로 2000번의 서로 다른 bandit problem 을 수행, 평균 값을 취함.</li>
        </ul>
      </li>
      <li>
        <p>결과</p>

        <p><img src="/assets/images/posts/2_3_2_result_10_armed_testbed.png" alt="2_3_2_result_10_armed_testbed" /></p>

        <ul>
          <li>보상의 분산이 클수록 더 많은 탐험이 필요하며 $\varepsilon$-greedy methods 가 greedy methods 보다 더 잘 작동한다.</li>
          <li>만약 보상의 분산이 0이라면 각각의 action 후에 true value 값을 바로 알 수 있게 된다.</li>
          <li>위의 경우 greedy methods 가 더 잘 작동하게 된다. (탐험할 필요가 없음.)</li>
          <li>그러나 이런 결정론적인 상황에서도 몇가지 가정이 불확실하다면 탐험하는 쪽이 유리하다.
            <ul>
              <li>비정상성 환경 (시간에 따라 true action-value 가 변함.)</li>
              <li>즉 Reinforcement learning 은 탐색과 이용에 균형이 필요함.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Incremental Implementation
    <ul>
      <li>action-value methods 를 획득한 보상의 평균 값으로 추정한다.</li>
      <li>
        <p>이 경우 어떻게 전산화 하여 계산할지?</p>

        <p><img src="/assets/images/posts/2_4_1_estimated_value_of_single_action.png" alt="2_4_1_estimated_value_of_single_action" /></p>

        <ul>
          <li>특정 단일 Action 에 대한 Action-value 를 예측, action 은 $n-1$ 번 선택됨.</li>
          <li>위 명확한 계산법은 모든 이전 기록을 가지고 있어야 하고, 예측값이 필요할 때마다 계산해야함.
            <ul>
              <li>메모리가 많이 필요하고 연산량이 상당해진다.</li>
              <li>위의 방법이 아니라 이전 평균값에서 이번 보상값을 업데이트 하는 방식을 취하는 것이 효율적이다.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/2_4_2_update_action_value.png" alt="2_4_2_update_action_value" /></p>

        <ul>
          <li>위의 구현법을 이용하면 메모리는 $Q_n$ 과 $n$ 값만을 저장하고 있고, 작은 계산을 통해 매번 새로운 예측값을 구할 수 있음.</li>
        </ul>

        <p><img src="/assets/images/posts/2_4_3_update_action_value.png" alt="2_4_3_update_action_value" /></p>

        <ul>
          <li>수식의 뜻은 위와 같음</li>
          <li>Target - OldEstimate = error
            <ul>
              <li>위 에러 값은 예측값이 Target 에 가까워질 수록 작아짐</li>
              <li>Target 은 예측이 움직이기 원하는 방향을 가리킴</li>
            </ul>
          </li>
          <li>StepSize : 각 타임스텝마다 변하는 step-size parameter 이며, 이 책에서는 $\alpha$ 혹은 $\alpha_t(a)$ 로 나타낸다.</li>
        </ul>

        <p><img src="/assets/images/posts/2_4_4_simple_bandit_algorithm.png" alt="2_4_4_simple_bandit_algorithm" /></p>

        <ul>
          <li>완성된 simple bandit algorithm</li>
          <li>breaking ties randomly.. 동점 기록일 경우 랜덤하게 선택한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Tracking a Nonstationary Problem
    <ul>
      <li>보상의 확률이 변하는 reinforcement learning 문제에서는 오랜 과거 보상보다 최근 보상에 비중을 더 두는 것이 설득력있다.</li>
      <li>
        <p>이러한 방법 중 하나로 상수 파라미터 (a constant step-size parameter) 를 사용하는 것이 유명하다.</p>

        <p><img src="/assets/images/posts/2_5_1_constant_step_size_parameter.png" alt="2_5_1_constant_step_size_parameter" /></p>

        <ul>
          <li>위의 수식을 weighted average 라고도 하는데 각 가중치의 합이 1이 되기 때문이다.
            <ul>
              <li>$(1-\alpha)^n + \sum_{i=1}^n\alpha(1-\alpha)^{n-i} = 1$</li>
            </ul>
          </li>
          <li>$1-\alpha$ 값이 1보다 작기 때문에 승수가 커질수록 (이전 step 의 값일수록) 가중치 값이 decay 됨</li>
          <li>때로는 위 수식을 exponential recency-weighted average 라고도 함 (지수적 최근성 가중치 평균)</li>
        </ul>
      </li>
      <li>때로는 step 별로 변동하는 step-size parameter 를 사용하는 것이 편할 때가 있음.
        <ul>
          <li>예를 들어 $\frac{1}{n}$ step-size parameter (sample-average method) 는 충분히 큰 step 을 진행할 경우 true action value 로 수렴하는 것을 보장한다.</li>
        </ul>
      </li>
      <li>
        <p>확률적 근사 이론은 확률 1로 수렴을 보장하는 데 필요한 조건을 제공한다.</p>

        <p><img src="/assets/images/posts/2_5_2_conditions_required_to_convergence_with_prob1.png" alt="2_5_2_conditions_required_to_convergence_with_prob1" /></p>

        <ul>
          <li>첫번째 조건은 초기 조건이나 무작위 변동을 극복할 수 있을 정도로 step-size 가 큰 것을 보장</li>
          <li>두번째 조건은 step-size 가 수렴을 확신할 정도로 충분히 작은 것을 보장</li>
          <li>$\frac{1}{n}$ 은 이 두 조건을 모두 만족하나, 상수 step 파라미터는 두번째 조건을 충족하지 않아 가장 최근의 보상값에 의해 완전히 수렴하지 못하게 됨.
            <ul>
              <li>이것은 비정상성 환경에서 필요한 내용이다.</li>
            </ul>
          </li>
          <li>두 조건이 만족하더라도 매우 느리게 수렴하거나, 만족스러운 수렴율을 얻기 위해 파라미터를 튜닝해야 할 수도 있음.</li>
          <li>위 이론은 이론적인 내용에는 자주 사용되나, 실제 적용 환경에서는 잘 사용되지 않음.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimistic Initial Values
    <ul>
      <li>위에 언급한 모든 방법들은 initial action-value estimates ($Q_1(a)$) 에 어느정도 영향을 받는다.</li>
      <li>통계적 표현으로 이러한 방식들은 biased by their initial estimates (초기추정치에 의해 편향된다) 라고 한다.</li>
      <li>예를 들어 sample-average methods 는 이 편향이 모든 action을 한 번 이상씩 수행했을 때 사라진다면
        <ul>
          <li>액션 a=1 to k 에 대해 초기값 $Q(a)$, $N(a)$ 를 초기값으로 쓰고, 한 번이라도 a 가 시도되면 수식에 따른 값으로 변경</li>
          <li>n 으로 나눌 때 0 으로 나눌 수 없으므로..</li>
        </ul>
      </li>
      <li>상수 a 를 사용하는 경우 이 편향은 영구적이다. (시간의 흐름에 따라 점차 감소하지만)
        <ul>
          <li>별도의 초기값 할당이 아니라 처음부터 수식을 사용하되, 수식 내 0번째 스텝의 값이 초기값임.</li>
        </ul>
      </li>
      <li>장점 : 예상할 수 있는 보상 수준에 대한 사전지식을 제공하는 쉬운 방법</li>
      <li>단점 : 모든 매개변수를 0 으로 설정하는 경우 사용자가 반드시 파라미터를 선택해야 한다.</li>
      <li>간단한 탐색 장려의 방법 : 초기 값을 0 대신 +5 로 설정 (10-armed testbed 상황으로 가정)
        <ul>
          <li>초기값 +5 의 값은 매우 낙관적인 수치</li>
          <li>특정 action 을 선택하고 받는 보상 값이 예측치보다 작음 (disappointed with the rewards)</li>
          <li>학습자는 다른 action 을 선택하게 되고 이 상황을 몇번 반복됨. (greedy action 일지라도…)</li>
        </ul>

        <p><img src="/assets/images/posts/2_6_1_optimistic_greedy.png" alt="2_6_1_optimistic_greedy" /></p>

        <ul>
          <li>10-armed bandit testbed 에서 greedy method 를 초기값 $Q_1(a) = +5$ 로 세팅한 결과</li>
          <li>비교군은 $\varepsilon$-greedy method 에 초기값 $Q_1(a) = 0$</li>
          <li>이 trick 은 stationary problem 에서 꽤 효과적이나, 탐색을 장려하는 일반적인 방법은 아님.</li>
          <li>이러한 비판은 sample-average methods 에서도 통용된다.
            <ul>
              <li>초기 시점을 특수한 이벤트로 여긴다.</li>
              <li>모든 보상을 똑같은 가중치로 평균을 구한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Upper-Confidence-Bound Action Selection
    <ul>
      <li>action-value 추정값의 불확실성 때문에 탐험은 반드시 필요하다.</li>
      <li>greedy actions 는 현재 시점에는 가장 최적의 선택이나 다른 action 이 실제로는 더 좋은 것일 수 있다.</li>
      <li>$\varepsilon$-greedy action selection 은 강제적으로 non-greedy action 을 선택하지만 선호도 없이 무차별적인 선택을 하여 greedy 한 선택을 하게 될 수도 있다.
        <ul>
          <li>강제적인 선택을 할 때는 non-greedy 한 선택을 하는 것이 좋음</li>
          <li>추정치가 최대치에 얼마나 가까운지와 추정치의 불확실성을 모두 고려</li>
          <li>실제로 최적일 가능성에 따라 탐욕스럽지 않은 action 을 선택하는 것이 좋음.</li>
        </ul>

        <p><img src="/assets/images/posts/2_7_1_UCB.png" alt="2_7_1_UCB" /></p>

        <ul>
          <li>UCB (Upper-Confidence-Bound) Action Selection 은 그러한 효율적인 방식 중 하나이다.</li>
          <li>$N_t(a)$ 는 t step 이 진행되었을때 a action 이 선택된 횟수로, 많이 선택될 수록 우항의 피연산자 값이 작아진다.</li>
          <li>$\ln t$ 는 step 이 커짐에 따라 값이 무한대까지 증가 (수렴하지 않음) 하나 그 증가폭이 서서히 줄어든다</li>
          <li>$c$ 는 탐험의 정도 (강도) 를 나타내는 수치이다.</li>
          <li>즉 action이 많이 선택될수록 $Q_t(a)$ 의 값은 정확해지고, 우항의 피연산자 값은 작아진다.</li>
          <li>action 이 한번도 선택되지 않을 경우 해당 a 를 maximizing action 으로 여긴다. (해당 a 에 대한 무조건적인 탐험)</li>
        </ul>

        <p><img src="/assets/images/posts/2_7_2_UCB.png" alt="2_7_2_UCB" /></p>

        <ul>
          <li>이러한 UCB 방식은 10-armed testbed 에서 $\varepsilon$-greedy 보다 나은 성과를 보여주기도 한다.</li>
          <li>단 몇가지 단점으로 인해 실용적이지 않은 방식이다.
            <ul>
              <li>bandits 문제에서 reinforcement learning 문제로 확장하기 어렵다.</li>
              <li>nonstationary 한 문제들에는 더 어려운 방식의 action-value method 가 필요하다.</li>
              <li>훨신 더 거대한 환경 (state space) 에서의 적용 (특히 뒤에 배울 function approximation 방식) 이 어렵다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lesson-1-the-k-armed-bandit-problem">Lesson 1: The K-Armed Bandit Problem</h2>

<ul>
  <li>학습목표
    <ul>
      <li>Define reward</li>
      <li>Understand the temporal nature of the bandit problem</li>
      <li>Define k-armed bandit</li>
      <li>Define action-values</li>
    </ul>
  </li>
  <li>Sequential Decision Making with Evaluative Feedback
    <ul>
      <li>불확실성 아래에서의 의사결정
        <ul>
          <li>의사가 3가지 처방약으로 환자에게 실험적 처방을 할 때…</li>
          <li>몇번의 환자 반응을 보고 가장 잘 듣는 약을 고집할 경우
            <ul>
              <li>더이상 다른 약의 데이터를 모을 수 없음</li>
              <li>나머지 두 약이 실제로 나음에도 몇몇 결과가 나쁘게 나왔는지 알 수 없음</li>
            </ul>
          </li>
          <li>다른 약으로 계속 실험할 경우
            <ul>
              <li>나쁜 결과를 계속 초래할 수 있음</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>강화학습에서의 기초적 컨셉 용어
        <ul>
          <li>에이전트 (Agent) : action을 선택하는 존재 - 의사</li>
          <li>액션 (Action) : 선택지 - 3가지 약 중 하나를 선택하는 것</li>
          <li>보상 (Rewards) : 액션에 대한 결과 - 환자의 상태</li>
          <li>값 (Value Function - Action-Value (Function)) : 기대되는 보상 값 - 환자의 혈압 값
            <ul>
              <li>에이전트가 액션을 선택했을 때 그 값 (Action-Value = $q_*$)이 최대화 할 경우 목적을 달성함</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$q_*(a)$ 값 구하기
        <ul>
          <li>각 약의 결과값이 서로 다른 확률분포를 가졌을 경우 $q_*$ 는 각 분포의 평균이 될 수 있다.</li>
        </ul>
      </li>
      <li>Bandits Problem 을 고려하는 이유
        <ul>
          <li>문제와 알고리즘 디자인 선택에 있어 가장 간단한 세팅</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lesson-2-what-to-learn-estimating-action-values">Lesson 2: What to Learn? Estimating Action Values</h2>

<ul>
  <li>학습목표
    <ul>
      <li>Define action-value estimation methods</li>
      <li>Define exploration and exploitation</li>
      <li>Select actions greedily using an action-value function</li>
      <li>Define online learning</li>
      <li>Understand a simple online sample-average action-value estimation method</li>
      <li>Define the general online update equation</li>
      <li>Understand why we might use a constant step-size in the case of non-stationarity</li>
    </ul>
  </li>
  <li>Learning Action Values
    <ul>
      <li>Estimate action values using the sample-average method
        <ul>
          <li>의사가 처방 후 환자가 나아질 경우 1, 그렇지 않은 경우 0 으로 표기하고 3개의 약을 처방해 평균을 구함</li>
          <li>Step 이 많이 진행될 수록 평균 데이터는 더 정확해짐</li>
        </ul>
      </li>
      <li>Describe greedy action selection
        <ul>
          <li>의시가 해당 시점에 가장 기대치가 큰 약을 처방할 경우 이 행동을 Greedy action 이라고 함</li>
          <li>greedy action 을 선택하는 것을 현 지식을 활용한 이용 (exploitation) 이라고 함</li>
          <li>당장의 기대되는 보상을 포기하고 다른 선택을 하는 것을 non-greedy action 이라고 하고 이를 탐험 (exploration) 이라고 함</li>
          <li>탐험을 통해 기대되는 보상을 희생하고 non-greedy action 의 보상에 대한 더 많은 정보를 얻게 됨</li>
        </ul>
      </li>
      <li>Introduce the exploration-exploitation dilemma
        <ul>
          <li>에이전트는 동시에 탐험과 이용을 할 수 없음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Estimating Action Values Incrementally
    <ul>
      <li>action value 의 점진적 표현법 (sample-average method 를 이용)
        <ul>
          <li>Incremental update rule</li>
          <li>NewEstimate &lt;- OldEstimate + StepSize(Target - OldEstimate)</li>
        </ul>
      </li>
      <li>점진적 업데이트 룰(Incremental update rule)이 더 널리 쓰이는 이유
        <ul>
          <li>모든 이전 값들을 기억할 필요가 없다.</li>
        </ul>
      </li>
      <li>보편적 업데이트 룰을 non-stationary bandit problem 에 적용하는 방법
        <ul>
          <li>Non-stationary Bandit Problem : 의사의 3가지 약중 특정 하나의 약이 겨울이 되면 효율이 높아진다.</li>
          <li>보상의 분포가 시간에 따라 변하게 되는 경우를 Non-stationary 하다 라고 함.</li>
          <li>StepSize 파라미터가 상수 (예:0.1) 일 경우 이전 Step 일수록 영향도가 작아지고, 최신 Step의 보상값을 더 반영함.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lesson-3-exploration-vs-exploitation-tradeoff">Lesson 3: Exploration vs. Exploitation Tradeoff</h2>

<ul>
  <li>학습목표
    <ul>
      <li>Define epsilon-greedy</li>
      <li>Compare the short-term benefits of exploitation and the long-term benefits of exploration</li>
      <li>Understand optimistic initial values</li>
      <li>Describe the benefits of optimistic initial values for early exploration</li>
      <li>Explain the criticisms of optimistic initial values</li>
      <li>Describe the upper confidence bound action selection method</li>
      <li>Define optimism in the face of uncertainty</li>
    </ul>
  </li>
  <li>What is the trade-off?
    <ul>
      <li>exploration-exploitation 의 등가교환
        <ul>
          <li>Exploration (탐색) : 장기적 이익을 위해 지식을 늘림</li>
          <li>Exploitation (이용) : 단기적 이익을 위해 지식을 이용</li>
          <li>탐색만 하면 단기적 보상이 작아지고, 이용만 하면 타 선택지의 true value 를 모르기 때문에 장기적으로 손해일 수 있음</li>
          <li>한번의 선택에서 탐색 과 이용 둘 중 하나만 가능</li>
        </ul>
      </li>
      <li>Epsilon-Greedy 방식 (탐색과 이용의 균형을 맞추는 쉬운 방법)
        <ul>
          <li>Epsilon-Greedy 는 대부분 Exploitation (Greedy method) 하고, 적은 확률로 Exploration (Random choice) 한다.</li>
        </ul>
      </li>
      <li>각 방식을 비교할 때 한 번의 진행으로는 노이즈가 많아 확인이 어려움.
        <ul>
          <li>1000 개의 에이전트로 보상 데이터를 모아 평균값으로 비교</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimistic Initial Values
    <ul>
      <li>Optimistic initial values 가 초기 탐색을 장려하는 이유
        <ul>
          <li>초기 예상 보상치를 높게 잡고 시작</li>
          <li>첫 선택의 보상이 주어지더라도 평균값으로 인해 값이 떨어짐</li>
          <li>에이전트는 첫 선택에 실망을 하고 선택되지 않은 다른 높은 초기치의 옵션 중 하나를 선택</li>
          <li>초기 탐험을 통해 모든 Action 을 골고루 선택하게 됨.</li>
        </ul>
      </li>
      <li>Optimistic initial values 의 한계
        <ul>
          <li>초기 단계에서만 탐색을 진행한다. 이는 Non-stationary Problems 에서 문제가 됨.</li>
          <li>Maximal Reward 를 시작하기 전 알 방법이 없기에 Optimistic Initial Value 를 어느정도로 설정해야 할지 모른다.</li>
          <li>그럼에도 Optimistic initial values 는 휴리스틱 (충분한 정보 없이 빠르게 사용할 수 있는 직관적인) 한 방법으로 자주 활용됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Upper-Confidence Bound(UCB) Action Selection
    <ul>
      <li>UCB action-selection 방식이 예측의 불확실성을 이용해 탐색을 유도하는 방법</li>
      <li>UCB의 C는 confidence 를 뜻하며, 예측한 $Q(a)$ 값의 오차범주 범위 $(c)$ 내에 실제 값이 들어올 것이라 확신하는 정도의 수치이다.
        <ul>
          <li>범위가 작을수록 결과에 더욱 확신한다는 뜻임</li>
        </ul>
      </li>
      <li>불확실성 (범위) 에 마주했을때 가장 높은 값 (Upper-Confidence Bound) 을 선택</li>
      <li>수식에 대한 풀이
        <ul>
          <li>c : 사용자 정의 파라미터 (얼마나 탐험을 할 건지 컨트롤)</li>
          <li>좌측 피연산자 : Exploit (이용)</li>
          <li>우측 피연산자 : Explore (탐험)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Contextual Bandits for Real World Reinforcement Learning
    <ul>
      <li>Contextual Bandits 는 Reinforcement Learning 이 현실에 배포되는 방식임</li>
      <li>Reinforcement 는 현실과 어떻게 다른가?
        <ul>
          <li>Reinforcement Learning 은 시뮬레이터로 동작한다.
            <ul>
              <li>시뮬레이터는 관측치를 제공</li>
              <li>Learning 알고리즘은 어떤 Action을 선택할 지 정책을 가진다.</li>
              <li>시뮬레이터는 실행하고 보상을 제공한다.</li>
            </ul>
          </li>
          <li>현실이 제공하는 관측값은 시뮬레이터와 다르다.
            <ul>
              <li>같은 정책이라 해도 관측치가 다르기 때문에 시뮬레이터에서의 Action 과 다른 Action 을 수행하게 된다.</li>
              <li>시뮬레이터와 현실은 보상도 다르다.</li>
            </ul>
          </li>
          <li>즉 현실과 시뮬레이터에는 갭이 존재한다.</li>
        </ul>
      </li>
      <li>Real World based Reinforcement Learning : 현실 기반 강화학습을 하려면 우선순위를 변경해야 함
        <ul>
          <li>Temporal Credit Assignment &lt; Generalization
            <ul>
              <li>Temporal Credit Assignment Problem (CAP) : 시간적 기여도 할당문제. 일련의 행동이 모두 끝난 뒤 보상을 얻을 수 있는 환경에서 수행한 행동들 중 어떠한 행동이 기여도가 있고 어떠한 행동이 벌점을 줄 것인지 결정하는 문제.</li>
              <li>Generalization : 다양한 관찰값을 통한 일반화</li>
            </ul>
          </li>
          <li>Control environment &lt; Environment controls
            <ul>
              <li>시뮬레이션 환경에서는 자유자재로 한 스텝을 더 진행하거나 할 수 있다면 (컨트롤 가능)</li>
              <li>현실에서는 환경이 지배함. (환경에 의해 컨트롤당함)</li>
            </ul>
          </li>
          <li>Computational efficiency &lt; Statistical efficiency
            <ul>
              <li>시뮬레이션 환경에서 계산효율이 중요하다면 (학습할 샘플이 많으므로)</li>
              <li>현실에서는 통계적 효율이 중요함 (현실이 주는 샘플만 가질 수 있다.)</li>
            </ul>
          </li>
          <li>State &lt; Features
            <ul>
              <li>시뮬레이션에서는 상태를 생각 (상태값은 결정을 내리는 데 기반이 되는 요소)</li>
              <li>현실에서는 매우 복잡한 관측값을 가지는 경우 (필요한 것보다 정보가 많음) 가 많아 어느 것이 핵심 요인인지가 중요</li>
            </ul>
          </li>
          <li>Learning &lt; Evaluation
            <ul>
              <li>현실에서는 “Off Policy Evaluation” 이 중요하다.
                <ul>
                  <li>Off Policy : 정책 업데이트에 어떤 데이터를 써도 상관이 없는 경우</li>
                  <li>즉, 최신 업데이트 정책에서 수집된 데이터가 아니어도 사용가능</li>
                  <li>학습 알고리즘이 학습 뿐만 아니라 정책 평가에 쓰일 수 있는 부산물 데이터 또한 제공</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Last policy &lt; Every Policy
            <ul>
              <li>시뮬레이션에서는 가장 최신의 정책이 중요</li>
              <li>현실에서는 모든 포인트의 데이터가 세상과의 어느 정도 상호작용이 포함되어 있음</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="chapter-summary-rlbook2018-pages-42-43">Chapter Summary (RLbook2018 Pages 42-43)</h2>

<h2 id="weekly-assessment">Weekly Assessment</h2>]]></content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html"><![CDATA[Course Introduction]]></summary></entry></feed>