<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://bluesplatter.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bluesplatter.com/" rel="alternate" type="text/html" /><updated>2023-03-16T14:46:18+09:00</updated><id>https://bluesplatter.com/feed.xml</id><title type="html">Bluesplatter</title><subtitle>전문적이지 않은 정보들, 감상, 즉흥적인 내용들</subtitle><author><name>HY03</name><email>hyunik03@gmail.com</email></author><entry><title type="html">Fundamentals of Reinforcement Learning - 02. Week 2. Markov Decision Processes</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 02. Week 2. Markov Decision Processes" /><published>2023-02-07T18:00:00+09:00</published><updated>2023-02-07T18:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2/">&lt;h2 id=&quot;course-introduction&quot;&gt;Course Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;문제를 접하게 되었을 때, 가장 중요한 단계가 문제를 Markov Decision Process(MDP) 로 전환하는 것이다.&lt;/li&gt;
  &lt;li&gt;솔루션의 질은 이 전환과 밀접한 관계가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;관련-자료-rlbook2018-pages-47-56&quot;&gt;관련 자료 (RLbook2018 Pages 47-56)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Finite Markov Decision Processes (유한한 마르코프 결정 프로세스)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;bandits 문제와 같이 피드백에 대한 평가를 포함&lt;/li&gt;
      &lt;li&gt;또한 연관성 측면 (다른 상황에서 다른 액션을 선택하는 것) 의 성격도 가지고 있음
        &lt;ul&gt;
          &lt;li&gt;bandits 문제는 동일 상황에서 다른 액션을 선택하는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 는 고전적인 정형화된 sequential decision making (순차 결정) 이다.
        &lt;ul&gt;
          &lt;li&gt;액션이 즉각적인 보상에만 영향을 주는 것이 아닌,&lt;/li&gt;
          &lt;li&gt;후속 상황(situations), 에이전트의 상태(states), 미래의 보상 (future rewards) 에 영향을 줌.&lt;/li&gt;
          &lt;li&gt;그러므로 MDP 는 지연된 보상, 그리고 지연된 보상과 즉각적인 보상 간의 tradeoff (거래합의) 가 필요하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 수식 관련
        &lt;ul&gt;
          &lt;li&gt;bandit problems
            &lt;ul&gt;
              &lt;li&gt;estimate the value $q_*(a)$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;MDP
            &lt;ul&gt;
              &lt;li&gt;estimate the value $q_*(s,a)$ : a : action, s : state&lt;/li&gt;
              &lt;li&gt;or estimate the value $\upsilon_*(s)$ : 상태 기반 추정값&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 는 강화학습 문제에 있어 수학적으로 이상적인 형태
        &lt;ul&gt;
          &lt;li&gt;수학적 구조 상 핵심 요소들 : 보상, 가치함수, 벨먼 방정식 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습은 적용 가능성의 폭과 수학적 취급 용이성 사이에 있어 tradeoff (거래합의) 가 필요하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Agent-Environment Interface&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_1_agent_environment_in_MDP.png&quot; alt=&quot;3_1_1_agent_environment_in_MDP&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;MDP 는 목표 달성을 위해 상호작용을 통한 학습 문제에 있어 직접적인 틀이다.&lt;/li&gt;
      &lt;li&gt;학습자 (learner) 와 결정자 (decision maker) 는 에이전트 (agent) 라 한다.&lt;/li&gt;
      &lt;li&gt;에이전트 (agent) 밖의 모든 구성으로 상호작용을 하는 것을 환경 (environment) 이라 한다.&lt;/li&gt;
      &lt;li&gt;상호작용은 연속적으로 발생한다.
        &lt;ul&gt;
          &lt;li&gt;에이전트는 액션을 선택하고 환경은 액션에 반응한다.
            &lt;ul&gt;
              &lt;li&gt;환경은 새로운 상황을 에이전트에 제공한다.&lt;/li&gt;
              &lt;li&gt;환경은 에이전트에 보상을 제공한다.&lt;/li&gt;
              &lt;li&gt;보상은 특정한 수치값으로 에이전트가 액션을 선택하는 것을 통해 최대화하길 원하는 값이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;특별히, 에이전트는 환경과 이산적 타임스텝 (0, 1, 2, 3) 의 순서로 상호작용한다고 가정한다.
            &lt;ul&gt;
              &lt;li&gt;많은 아이디어들이 시간 연속적인 것이 될 수 있어도, 단순성을 유지하기 위해 케이스를 제한한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;각 타임스텝 t 에 따라…
            &lt;ul&gt;
              &lt;li&gt;agent 는 환경의 상태 (environment’s state) 값을 받음 : $S_t \in$ S&lt;/li&gt;
              &lt;li&gt;이에 따라 agent 는 액션을 선택함 : $A_t \in$ A$(s)$&lt;/li&gt;
              &lt;li&gt;액션에 따른 결과로 에이전트는 수치적 보상을 얻음 : $R_{t+1} \in R \subset$ R&lt;/li&gt;
              &lt;li&gt;이후 다른 환경의 상태 값을 제공받음 : $S_{t+1}$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위에 따라 MDP 와 에이전트는 아래와 같은 시퀀스, 혹은 궤적 (trajectory) 를 남기게 됨
            &lt;ul&gt;
              &lt;li&gt;$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, …$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_2_MDP_function_p_1.png&quot; alt=&quot;3_1_2_MDP_function_p_1&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;유한 MDP (finite MDP) 에서 모든 상태, 액션, 보상의 집합은 유한한 수의 요소 (element) 이다.&lt;/li&gt;
      &lt;li&gt;$R_t$ 와 $S_t$ 는 이전 상태와 액션값에만 영향을 받는 이산확률분포 (discrete probability distributions) 로 정의됨
        &lt;ul&gt;
          &lt;li&gt;이산확률분포 : 확률변수가 가질 수 있는 값이 명확하고 셀 수 있는 경우의 분포 (주사위의 눈 $1, …, 6$)&lt;/li&gt;
          &lt;li&gt;연속확률분포 : 확률변수가 가질 수 있는 값이 연속적인 실수여서 셀 수 없는 경우의 분포 ($y = f(x)$)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;우선 제공된 state 와 action 에 따른 time t 시점의 확률 값이 존재 (상단 수식)&lt;/li&gt;
      &lt;li&gt;모든 $s’, s \in S, r \in R, a \in A(s)$ 에 대해 function $p$ 는 dynamics of the MDP 로 정의된다.&lt;/li&gt;
      &lt;li&gt;일반적인 함수 표현, The dynamics function $p : S \times R \times S \times A \to [0,1]$&lt;/li&gt;
      &lt;li&gt;| : 조건부 확률에 대한 표기법이지만, 여기서는 $p$ 가 $s, a$ 의 각 선택에 대한 확률분포를 지정한다는 것을 뜻함.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_3_MDP_function_p_2.png&quot; alt=&quot;3_1_3_MDP_function_p_2&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;MDP (Markov decision process) 에서 $p$ 로 주어진 확률은 환경의 역학을 완벽히 특성화함.
        &lt;ul&gt;
          &lt;li&gt;즉, 가능한 $S_t$, $R_t$ 값에 대한 가능성은 이전에 즉시 제공된 $S_{t-1}$, $A_{t-1}$ 값에만 의존한다.&lt;/li&gt;
          &lt;li&gt;그 이전의 상태값과 액션은 영향을 주지 않는다.&lt;/li&gt;
          &lt;li&gt;이것은 결정 과정에 대한 제한이 아닌 상태에 대한 제한을 의미한다.&lt;/li&gt;
          &lt;li&gt;상태에 미래의 차이를 발생 시킬 수 있는 이전 에이전트-환경 간 상호작용에 대한 측면이 모두 정보로 포함되어 있어야 한다.&lt;/li&gt;
          &lt;li&gt;만약 그렇다면, 이 상태를 마르코프 속성 (Markov property) 이라 한다.
            &lt;ul&gt;
              &lt;li&gt;이후 과정에서 Markov 속성에 의존하지 않는 근사 방법 (approximation methods) 을 고려할 것&lt;/li&gt;
              &lt;li&gt;17장에서는 Markov 가 아닌 관찰값으로부터 어떻게 Markov 상태를 구성하고 학습할지를 고려할 것&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;4개의 인수를 가진 dynamic function $p$ 에서 state-transition probabilities (상태 전이 확률) 와 같은 알고싶은 모든 것을 계산할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_4_MDP_function_p_3.png&quot; alt=&quot;3_1_4_MDP_function_p_3&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;$p : S \times S \times A \to [0,1]$&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;또한 예상되는 보상값을 2개 인자 (state,action) 의 r 함수로 계산할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_5_MDP_function_p_4.png&quot; alt=&quot;3_1_5_MDP_function_p_4&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;$r : S \times \ A \to R$&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;예상되는 보상값을 3개 인자 (state, action, next state) 의 r 함수로 계산할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_6_MDP_function_p_5.png&quot; alt=&quot;3_1_6_MDP_function_p_5&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;$r : S \times \ A \times S \to R$&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;이 책에서 우리는 주로 4개 인자의 $p$ 함수 (3.2) 를 사용하지만 때때로 다른 함수가 편리할 경우도 있음.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;MDP 프레임워크는 추상적이고 유연하여 다양한 문제에 적용할 수 있음
            &lt;ul&gt;
              &lt;li&gt;스텝 (Step) 은 고정된 시간 간격일 필요가 없음. 의사 결정과 행동의 임의적 연속 단계일 수 있음&lt;/li&gt;
              &lt;li&gt;동작 (Action) 은 로봇 팔의 모터 전압과 같이 낮은 수준의 제어부터 대학원에 갈 것인지 여부를 정하는 높은 수준의 결정일 수 있음.&lt;/li&gt;
              &lt;li&gt;상태 (State) 는 직접적인 센서의 데이터처럼 낮은 수준의 감각일 수 있고, 방에 있는 물체에 대한 상직적 설명과 같이 높은 수준의 추상일 수 있음.&lt;/li&gt;
              &lt;li&gt;또한 상태 (State) 는 과거 감각의 기억이거나 정신적, 주관적인 것일 수 있음.
                &lt;ul&gt;
                  &lt;li&gt;예를 들어 에이전트는 물체가 어디있는지 확신하지 못할 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;액션 (Action) 은 정신적일수도, 계산적일 수도 있음
                &lt;ul&gt;
                  &lt;li&gt;일부 동작은 에이전트가 어디에 집중할지 를 변경하는 것일 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;즉 액션은 우리가 결정을 내리는 방법을 배우고자 하는 모든 결정, 상태는 결정을 내리는 데 유용한 모든 것이 될 수 있음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;MDP 에서 에이전트와 환경의 경계는 일반적인 물리적 경계와 동일하지 않음.
            &lt;ul&gt;
              &lt;li&gt;대게 경계는 중간지점보다 에이전트에 더 가깝게 그려짐
                &lt;ul&gt;
                  &lt;li&gt;예를 들어 로봇의 기계적 감지 하드웨어는 에이전트의 일부가 아닌 환경의 일부로 간주되어야 함.&lt;/li&gt;
                  &lt;li&gt;생물체일 경우 근육, 골격 및 감각 기관은 환경의 일부로 간주되어야 함.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;보상 또한 인공 학습 시스템 본체 내부에서 계산되지만, 에이전트 외부에 있는 것으로 간주됨.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉, 에이전트가 임의로 변경할 수 없는 모든 것은 환경의 일부로 간주한다는 것임.
            &lt;ul&gt;
              &lt;li&gt;에이전트는 환경의 일부를 알고 있음
                &lt;ul&gt;
                  &lt;li&gt;에이전트는 자신의 행동과 상태에 대해 보상이 어떻게 함수로 계산되는지에 대해 알고 있음.&lt;/li&gt;
                  &lt;li&gt;그러나 보상 계산은 에이전트 밖의 영역임.&lt;/li&gt;
                  &lt;li&gt;우리가 루빅큐브가 어떻게 작동하는지 알지만, 여전히 해결할 수 없는 문제가 될 수 있는 것처럼&lt;/li&gt;
                  &lt;li&gt;환경이 어떻게 작동하는 지에 모든 것을 알고있으면서도 여전히 어려운 강화 학습 작업에 직면할 수 있음.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;즉, 에이전트와 환경의 경계는 에이전트의 지식이 아닌 제어의 한계를 나타냄.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;에이전트와 환경의 경계는 서로 다른 목적을 위해 다르게 위치할 수 있음
            &lt;ul&gt;
              &lt;li&gt;복잡한 로봇을 예로 들면 각각 고유한 경계를 가진 여러 에이전트가 한번에 작동할 수 있음
                &lt;ul&gt;
                  &lt;li&gt;높은 수준의 결정을 구현하는 에이전트가 낮은 수준의 에이전트가 직면한 상태의 일부를 형성하는 높은 수준의 결정을 내릴 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;에이전트와 환경 간의 경계는 상태, 액션, 보상을 결정한 뒤 결정을 위한 관심사를 식별한 뒤에 정해짐.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉, MDP 프레임워크는 상호작용으로부터 목표 지향적 학습을 하는 문제를 추상화한 것이다.
            &lt;ul&gt;
              &lt;li&gt;감각, 기억, 제어 장치의 세부사항이 무엇이든&lt;/li&gt;
              &lt;li&gt;달성하려는 목표가 무엇이든&lt;/li&gt;
              &lt;li&gt;목표 지향적 행동을 학습하는 문제는 에이전트와 그 환경 사이를 오가는 3가지 신호로 축소될 수 있다고 제안하는 것.
                &lt;ul&gt;
                  &lt;li&gt;Actions : 에이전트의 선택을 나타내는 신호&lt;/li&gt;
                  &lt;li&gt;States : 선택이 이루어지는 기준(상태) 를 나타내는 신호&lt;/li&gt;
                  &lt;li&gt;Rewards : 에이전트의 목표를 정의하는 신호&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;이 프레임워크는 모든 의사결정 학습문제를 유용하게 나타내기에 충분하지 않을 수 있지만, 널리 이용 가능하고 적용 가능한 것으로 입증됨.
            &lt;ul&gt;
              &lt;li&gt;물론 states 와 actions 는 작업마다 크게 다르며, 이것을 표현하는 방식이 성능에 큰 영향을 줌&lt;/li&gt;
              &lt;li&gt;다른 종류의 학습과 마찬가지로 강화학습에서 이러한 표현 선택은 과학보다 예술에 가까움.&lt;/li&gt;
              &lt;li&gt;이 책에서 우리는 상태와 행동을 나타내는 좋은 방법에 관한 몇 가지 조언과 예를 제공하지만,&lt;/li&gt;
              &lt;li&gt;우리의 주요 초점은 일반적인 원칙 (표현법이 선택되면 그것이 어떻게 동작하는지) 을 배우는 것에 있음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예제 3.1 : Bioreactor (바이오 리액터 - 세포배양기)
            &lt;ul&gt;
              &lt;li&gt;강화 학습이 세포배양기의 순간 온도와 교반 속도 (뒤섞는 교반기의 회전 속도) 를 결정하기 위해 적용되어 있다고 가정&lt;/li&gt;
              &lt;li&gt;Actions : 목표 온도와 목표 교반속도를 달성하기 위해 발열체와 모터를 직접 조종하는 하위 제어시스템에 전달되는 값&lt;/li&gt;
              &lt;li&gt;States : 여과되고 지연된 열전대, 기타 센서 값과 대상 화학물질을 나타내는 상징적 입력값 등&lt;/li&gt;
              &lt;li&gt;Rewards : 유용한 화합물이 생성되었는지에 대한 순간적인 측정값&lt;/li&gt;
              &lt;li&gt;여기서 상태 값은 list 혹은 vector 값 (센서 측정값, 상징적 입력값)&lt;/li&gt;
              &lt;li&gt;액션 값은 vector 값 (목표 온도와 교반속도로 이루어진)&lt;/li&gt;
              &lt;li&gt;보상 값은 항상 단일 숫자값&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예제 3.2 : Pick-and-Place Robot
            &lt;ul&gt;
              &lt;li&gt;반복적인 물체 이동 작업에서 로봇 팔의 동작을 제어하기 위해 강화학습을 사용한다고 가정&lt;/li&gt;
              &lt;li&gt;빠르고 부드러운 동작을 학습하려면 학습 에이전트가 모터를 직접 제어하고, 기계 연결장치의 현재 위치와 속도에 대한 짧은 지연시간의 정보를 가져와야 함&lt;/li&gt;
              &lt;li&gt;Actions : 각 관절의 모터에 적용되는 전압&lt;/li&gt;
              &lt;li&gt;States : 관절의 각도와 속도의 최신 값&lt;/li&gt;
              &lt;li&gt;Rewards : 잘 운반된 개체에 대해 + 1 값&lt;/li&gt;
              &lt;li&gt;부드러운 움직임을 장려하기 위해 각 단계의 순간순간의 갑작스러운 움직임에 작은 처벌 (보상의 음수값) 을 부여&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예제 3.3 : Recycling Robot (재활용 로봇)
            &lt;ul&gt;
              &lt;li&gt;모바일 로못은 사무실 환경에서 빈 음료수 캔을 모으는 일을 함&lt;/li&gt;
              &lt;li&gt;로봇은 캔을 감지하는 센서, 캔을 집어 일체형 통에 넣을 수 있는 팔과 그리퍼를 가지고 있음&lt;/li&gt;
              &lt;li&gt;로봇은 충전식 배터리로 작동&lt;/li&gt;
              &lt;li&gt;로봇의 제어 시스템에는 센서 정보를 해석하는 파츠, 탐색기능 파츠, 팔과 그리퍼를 컨트롤 하는 파츠 가 있음.&lt;/li&gt;
              &lt;li&gt;높은 수준의 의사결정 (빈 캔을 어떻게 찾을지) 은 현재 배터리 충전 레벨에 따라 강화학습 에이전트가 내림.&lt;/li&gt;
              &lt;li&gt;간단한 예제를 만들기 위해 충전정도는 2가지 레벨만 구분한다고 가정, 작은 상태 집합 $S = \lbrace high,low \rbrace$  으로 구성&lt;/li&gt;
              &lt;li&gt;각 상태에서 에이전트는 다음 중 하나의 결정을 할 수 있음.
                &lt;ul&gt;
                  &lt;li&gt;(1) 일정 시간 동안 적극적으로 캔을 찾음&lt;/li&gt;
                  &lt;li&gt;(2) 움직이지 않고 누군가가 캔을 가져올 때까지 기다림&lt;/li&gt;
                  &lt;li&gt;(3) 배터리 충전을 위해 충전 장소로 돌아감&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;배터리가 많이 남았을 때 (high), 충전은 언제나 멍청한 선택이므로 해당 상태의 action set 으로 포함시키지 않는다.
                &lt;ul&gt;
                  &lt;li&gt;action set 은 $A(high) = \lbrace search,wait \rbrace$ 와 $A(low) = \lbrace search, wait, recharge \rbrace$ 이다.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;대부분의 경우 보상은 0 이지만, 로봇이 빈 캔을 확보하면 (+), 배터리가 완전히 방전되면 큰 수치의 (-) 가 됨.&lt;/li&gt;
              &lt;li&gt;캔을 찾는 가장 좋은 방법은 능동적 탐색이지만 배터리가 소모됨 (배터리가 고갈될 가능성이 있음)&lt;/li&gt;
              &lt;li&gt;기다리는 것은 배터리가 소모되지 않음&lt;/li&gt;
              &lt;li&gt;배터리가 고갈되면 로못은 종료되고 구조될 때까지 기다려야 함 (낮은 보상 생성)&lt;/li&gt;
              &lt;li&gt;배터리가 많으면 배터리 고갈 위험 없이 빈 캔 탐색을 완료할 수 있음
                &lt;ul&gt;
                  &lt;li&gt;배터리가 많을 때의 탐색 기간에서 배터리가 많이 남아 있을 확률 $\alpha$&lt;/li&gt;
                  &lt;li&gt;배터리가 많을 때의 탐색 기간에서 배터리가 적어질 확률 $1 - \alpha$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;반대로 배터리가 적으면
                &lt;ul&gt;
                  &lt;li&gt;배터리가 적을 때의 탐색 기간에서 배터리가 적어질 확률 $\beta$&lt;/li&gt;
                  &lt;li&gt;배터리가 적을 때의 탐색 기간에서 배터리가 고갈될 확률 $1 - \beta$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;후자의 경우 로봇은 구조되어야만 하며, 이 경우 배터리는 다시 완충된다.&lt;/li&gt;
              &lt;li&gt;로봇에 의해 수집된 캔의 수량 만큼 보상이 주어지며, 로봇이 구출되었을 때 보상값 -3 이 주어진다.&lt;/li&gt;
              &lt;li&gt;$r_{search}, r_{wait}$ with $r_{search} &amp;gt; r_{wait}$ : 로봇이 검색 혹은 대기 중에 수집할 것으로 추정되는 캔 수 (즉, 예상되는 보상)&lt;/li&gt;
              &lt;li&gt;충전을 위해 충전 장소로 돌아갈 때나, 배터리가 고갈되었을 때에는 캔을 회수할 수 없음&lt;/li&gt;
              &lt;li&gt;이 시스템은 유한한 MDP 이며 전확 확률과 예상 보상을 아래와 같이 표기할 수 있다.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_7_MDP_transition_probabilities.png&quot; alt=&quot;3_1_7_MDP_transition_probabilities&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 표는 현 상태 $s$, 액션 $a \in A(s)$, 다음 상태 $s’$ 의 각각의 가능한 조합으로 구성된 표이다.&lt;/li&gt;
              &lt;li&gt;몇몇 전환은 일어날 가능성이 0 이므로, 기대되는 보상 또한 없다.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_1_8_MDP_transition_graph.png&quot; alt=&quot;3_1_8_MDP_transition_graph&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 그래프는 transition graph 로, 유한 MDP 의 역학을 요약하는 또다른 방법이다.&lt;/li&gt;
              &lt;li&gt;state nodes 와 action nodes 가 있음.&lt;/li&gt;
              &lt;li&gt;state nodes : 큰 빈 원, 원 안에 라벨링 된 이름&lt;/li&gt;
              &lt;li&gt;action nodes : 작은 검은색 원과 state nodes 를 연결 시키는 화살표&lt;/li&gt;
              &lt;li&gt;상태 $s$ 에서 작업 $a$ 를 수행하면 상태노드 $s$ 에서 작업노드 $(s,a)$ 로 이동함.&lt;/li&gt;
              &lt;li&gt;그 다음 환경은 $(s,a)$ 를 떠나는 화살표 중 하나를 통해 다음 상태의 노드 $s’$ 로 전환시킴&lt;/li&gt;
              &lt;li&gt;각 화살표는 $(s, s’, a)$ 에 해당함.&lt;/li&gt;
              &lt;li&gt;여기서 $s’$ 는 다음 상태이고, 전환확률 $p(s’|s,a)$ 와 해당 전환에 대한 예상보상은 $r(s,a,s’)$ 임.&lt;/li&gt;
              &lt;li&gt;작업 노드를 떠나는 화살표의 전환 확률의 합계는 항상 1임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goals and Rewards&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;강화학습의 목적, 목표는 특별한 신호로 정형화 되는데 이를 보상 (Reward) 이라 하고 환경이 에이전트에 전달하는 값이다.&lt;/li&gt;
      &lt;li&gt;각 스텝마다 보상은 단순한 숫자 값이다. $R_t \in R$&lt;/li&gt;
      &lt;li&gt;비공식적으로, 에이전트의 목표는 총 보상의 양을 최대화 하는 것이다.
        &lt;ul&gt;
          &lt;li&gt;이 최대화의 의미는 당장의 보상을 최대화 하는 것이 아닌 장기적 누적 보상을 최대화 하는 것이다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상 측면에서 위 목표를 공식화하는 것이 처음에는 제한적으로 보일 수 있지만, 실은 유연하고 광범위하게 적용될 수 있다.
        &lt;ul&gt;
          &lt;li&gt;예 1. 로봇이 걷는 법을 배움 : 보상은 로봇이 전진할 때마다 주어짐&lt;/li&gt;
          &lt;li&gt;예 2. 로봇이 미로를 통과하는 법을 배움 : 매 스텝마다 -1 의 보상을 주어 에이전트가 최대한 빨리 미로를 탈출하는 것을 장려한다.&lt;/li&gt;
          &lt;li&gt;예 3. 재활용 캔 수집을 하는 법을 배움 : 캔을 모을 때마다 +1 의 보상을 주며, 뭔가에 부딪히거나 누군가 소리를 지를 경우 - 보상을 준다.&lt;/li&gt;
          &lt;li&gt;예 4. 체스 두는 법을 배움 : 이길 때 +1점, 질 때 -1점, 비길 때 0점의 보상을 부여&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;즉, 에이전트가 우리의 목표를 달성하게 하기 위해 최대화 할 수 있는 보상을 제공해줘야 한다.
        &lt;ul&gt;
          &lt;li&gt;따라서 보상이 정말 우리가 원하는 결과를 나타내는 것인지가 매우 중요하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상은 우리가 원하는 것을 달성하기 위한 사전지식을 전달하는 곳이 아니다.
        &lt;ul&gt;
          &lt;li&gt;이것에 적합한 위치는 초기 정책 또는 초기 가치 함수, 혹은 이들에 영향을 줄 수 있는 값이다.&lt;/li&gt;
          &lt;li&gt;예를 들어 체스 게임일 경우 상대의 말을 잡거나 센터를 장악하는 것에 보상이 주어저서는 안 된다.
            &lt;ul&gt;
              &lt;li&gt;에이전트는 승패의 여부와 관계없이 이 서브 목표를 달성하는 것을 학습할 것이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉, 보상은 에이전트에게 달성해야 하는 것에 대한 소통을 하는 방법이고, 어떻게 달성할지에 대한 것을 이야기 하는 것이 아니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Returns and Episodes&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_3_1_MDP_sum_of_rewards.png&quot; alt=&quot;3_3_1_MDP_sum_of_rewards&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;정확히 시퀀스의 어떤 면을 최대화 하길 원하는지에 대하여, 가장 단순히 보상의 합계를 표현한 형태&lt;/li&gt;
      &lt;li&gt;final time step $T$ 라는 개념이 있을 경우
        &lt;ul&gt;
          &lt;li&gt;환경과 에이전트 간 상호작용이 자연스럽게 끊길 때, 우리는 이것을 에피소드 ($episode$) 라 표현&lt;/li&gt;
          &lt;li&gt;예를 들어 게임 한 판, 하나의 미로를 통과하는 것 혹은 어떠한 종류의 반복적인 상호작용 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;에피소드가 끝난 상태를 terminal state 라 한다.
        &lt;ul&gt;
          &lt;li&gt;이후 표준 시작 상태로 재설정하거나, 시작 상태의 표준 분포에서 샘플을 재설정한다.&lt;/li&gt;
          &lt;li&gt;에피소드가 다른 방식으로 끝나더라도 (예를들어 게임에서의 승리,패배) 다음 에피소드는 이전 에피소드와 독립적으로 시행된다.&lt;/li&gt;
          &lt;li&gt;즉 에피소드는 모두 terminal state 로 끝나나, 보상과 결과는 다를 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;위와 같은 에피소드 형태의 일을 episodic tasks 라 한다.
        &lt;ul&gt;
          &lt;li&gt;$S$ : 모든 non-terminal state&lt;/li&gt;
          &lt;li&gt;$S^+$ : 모든 non-terminal state + terminal state&lt;/li&gt;
          &lt;li&gt;$T$ : The time of termination&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;반면 많은 경우 환경과 에이전트의 상호작용은 자연스럽게 별개의 에피소드로 끊기지 않는다.
        &lt;ul&gt;
          &lt;li&gt;이를 continuing tasks 라 한다.&lt;/li&gt;
          &lt;li&gt;final time step $T=\infty$ 이기 때문에 위 3.7의 수식으로 표현할 수 없다.&lt;/li&gt;
          &lt;li&gt;(우리가 최대화를 원하는 리턴 값이 쉽게 무한대의 값이 될 수 있다.)&lt;/li&gt;
          &lt;li&gt;즉, discounting 이라는 추가적 개념이 필요하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_3_2_MDP_discounted_return.png&quot; alt=&quot;3_3_2_MDP_discounted_return&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;에이전트는 미래의 할인된 보상값의 합 (sum of the discounted rewards) 이 최대값이 되도록 액션 ($A_t$)을 선택한다.&lt;/li&gt;
      &lt;li&gt;$0 \le \gamma \le 1$ 인 $\gamma$ 파라미터를 discount rate 라 한다.&lt;/li&gt;
      &lt;li&gt;discount rate 는 미래 보상의 현재 가치를 결정한다.
        &lt;ul&gt;
          &lt;li&gt;미래 $k$ time steps 에서 받을 보상은 현재의 가치로 따졌을 때 $\gamma^{k-1}$ 배의 가치만 있다.&lt;/li&gt;
          &lt;li&gt;$\gamma &amp;lt; 1$ : 극한 합 (3.8) 의 값이 유한한 값으로 수렴한다. ($R_k$ 가 제한된 값일 경우)&lt;/li&gt;
          &lt;li&gt;$\gamma = 0$ : 에이전트는 오직 $A_t$ 를 선택해 $R_{t+1}$ 값을 최대화하는 방법만을 학습한다. (근시안적)&lt;/li&gt;
          &lt;li&gt;$\gamma \to 1$ : 1에 가까워질 수록 미래의 보상값을 더 강하게 계산. (원시안적)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_3_3_MDP_discounted_return.png&quot; alt=&quot;3_3_3_MDP_discounted_return&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;연속적인 time step (successive time step) 에서의 보상은 상호간 연관관계에 있고, 이는 강화학습에서 중요한 개념이다.&lt;/li&gt;
      &lt;li&gt;위 공식은 모든 time steps $t &amp;lt; T$ 에서 적용되며, 심지어 $t + 1$ 에서 termination 이 일어나는 경우에도 $G_T = 0$ 으로 정의함으로써 적용가능하다.&lt;/li&gt;
      &lt;li&gt;3.8 의 인자 수가 무한대이지만, 보상이 0 이 아니고 상수 일 경우 여전히 유한한 숫자가 된다.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 보상이 상수 $+1$ 이고 , $\gamma &amp;lt; 1$ 일 경우 보상 값은 아래의 값이 된다.
  &lt;img src=&quot;/assets/images/posts/3_3_4_MDP_discounted_return.png&quot; alt=&quot;3_3_4_MDP_discounted_return&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;예제 3.4 : Pole-Balancing&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/3_4_1_cart_pole.png&quot; alt=&quot;3_4_1_cart_pole&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;목표 : 카트에 힘을 적용하여 트랙 위를 카트가 이동할 수 있되, 폴이 서 있고 넘어지지 않게 유지하는 것&lt;/li&gt;
          &lt;li&gt;실패 : 폴이 주어진 각도를 넘어서 넘어지는것 혹은 카트가 트랙에서 탈선하는 것&lt;/li&gt;
          &lt;li&gt;폴은 매 실패 이후 리셋되어 수직으로 서게 된다 &amp;gt; 이 일은 episodic 으로 간주될 수 있음
            &lt;ul&gt;
              &lt;li&gt;자연스러운 에피소드가 반복됨&lt;/li&gt;
              &lt;li&gt;보상은 실패하기 전 모든 타임 스텝에 +1 을 줄 수 있다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;위의 설정은, 폴이 영원히 성공적으로 벨런스를 잡게 되면 보상은 무한수가 된다는 뜻이다.&lt;/li&gt;
          &lt;li&gt;또는 위 문제를 continuing task 로 간주할 수도 있다. (using discounting)
            &lt;ul&gt;
              &lt;li&gt;위의 경우 실패할 경우 보상은 -1 이 되고, 그 외에는 0 이 된다.&lt;/li&gt;
              &lt;li&gt;즉 보상 값은 $- \gamma^K$ 가 되며, $K$ 는 실패 이전의 타임스텝이 된다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;양쪽의 경우 모두 폴의 균형을 최대한 유지할 수록 보상이 최대화 된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-markov-decision-processes&quot;&gt;Introduction to Markov Decision Processes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Markov Decision Process (MDP) 에 대해 이해하기&lt;/li&gt;
      &lt;li&gt;MDP 프로세스가 정의되는 방법 이해&lt;/li&gt;
      &lt;li&gt;Markov Decision Process 의 그래픽 표현 이해&lt;/li&gt;
      &lt;li&gt;MDP 프레임워크로 다양한 프로세스를 작성하는 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Markov Decision Processes
    &lt;ul&gt;
      &lt;li&gt;k-Armed Bandit Problem : 같은 상황에서 같은 Action 이 항상 최적화된 선택이 되는 문제유형 (현실과 다름)
        &lt;ul&gt;
          &lt;li&gt;예 : 토끼의 왼쪽에 브로콜리 (보상 : +3) 가 있고 오른쪽에 당근 (보상 : +10) 이 있을 경우&lt;/li&gt;
          &lt;li&gt;토끼는 오른쪽의 당근을 먹는 것을 선택한다.&lt;/li&gt;
          &lt;li&gt;갑자기 두 음식의 위치가 바뀔경우 k-Armed Bandit Problem 으로 정의할 수 없는 문제가 된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;현실 : 다른 상황이 다른 반응을 야기하며, 현재 선택한 Action 이 미래의 보상의 양을 결정한다.
        &lt;ul&gt;
          &lt;li&gt;예 : 토끼의 왼쪽에 브로콜리 (보상 : +3) 가 있고 오른쪽에 당근 (보상 : +10) 이 있을 경우&lt;/li&gt;
          &lt;li&gt;단, 오른쪽 방향에 호랑이가 있음&lt;/li&gt;
          &lt;li&gt;장기적 이익 관점에서 토끼는 브로콜리를 선택해야 한다. (즉각적 보상이 적다 하더라도)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/The_dynamics_of_an_MDP.png&quot; alt=&quot;The_dynamics_of_an_MDP&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;The dynamics of an MDP
        &lt;ul&gt;
          &lt;li&gt;출처 : Coursera / Fundamentals of Reinforcement Learning / W2. Markov Decision Process / 동영상 : Markov Decision Processes&lt;/li&gt;
          &lt;li&gt;The dynamics of an MDP : 확률 분포에 의해 정의됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Examples of MDPs
    &lt;ul&gt;
      &lt;li&gt;Recycling Robot 에 대한 예시
        &lt;ul&gt;
          &lt;li&gt;State : 배터리 잔량 High, 배터리 잔량 Low&lt;/li&gt;
          &lt;li&gt;Action :
            &lt;ul&gt;
              &lt;li&gt;배터리 잔량 High : Search, Wait, Recharge&lt;/li&gt;
              &lt;li&gt;배터리 잔량 Low : Search, Wait&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/posts/Dynamics_of_the_Recycling_Robot.png&quot; alt=&quot;Dynamics_of_the_Recycling_Robot&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Dynamics_of_the_Recycling_Robot
        &lt;ul&gt;
          &lt;li&gt;출처 : Coursera / Fundamentals of Reinforcement Learning / W2. Markov Decision Process / 동영상 : Examples of MDPs&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MDP 는 다양한 문제에 적용이 가능하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;goal-of-reinforcement-learning&quot;&gt;Goal of Reinforcement Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;에이전트의 목표와 보상이 어떻게 관련되는지 설명&lt;/li&gt;
      &lt;li&gt;에피소드에 대한 이해 및 에피소드로 표현될 수 있는 작업 식별&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The Goal of Reinforcement Learning
    &lt;ul&gt;
      &lt;li&gt;Bandits Problems : 즉각적 보상의 최대화&lt;/li&gt;
      &lt;li&gt;MDP : 타임스텝 전체의 보상 합의 최대화
        &lt;ul&gt;
          &lt;li&gt;토끼가 오른쪽의 당근을 먹는 것은 즉각적 보상의 최대화를 야기하나 이후 바뀐 상태에 의해 호랑이에 잡아먹힘&lt;/li&gt;
          &lt;li&gt;로봇에게 걷는 방법을 가르쳐 줄 때, 보상 값이 앞으로 나아간 수치라 하여 이를 즉각적으로 최대화 할 경우 넘어져 걷질 못하게 됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Episodic  Tasks : 마지막 스텝을 밟은 뒤 다시 초기상태로 돌아감 (에이전트-환경 간 상호작용의 종료)
        &lt;ul&gt;
          &lt;li&gt;체스게임&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Michael Littman: The Reward Hypothesis
    &lt;ul&gt;
      &lt;li&gt;Reinforcement Learning 에서 보상을 정의하는 것
        &lt;ul&gt;
          &lt;li&gt;물고기를 주면 하루를 먹고 : Programming (Good Old-Fashioned AI) &amp;gt; 새로운 환경에서 새 프로그래밍을 해야 함&lt;/li&gt;
          &lt;li&gt;물고기를 잡는 법을 알려주면 평생을 먹고 : Supervised Learning &amp;gt; 학습 데이터를 제공해야 함&lt;/li&gt;
          &lt;li&gt;물고기의 맛을 알려주면 어떻게 물고기를 잡을지 알아낸다. : Reinforcement learning &amp;gt; 최적화문제&lt;/li&gt;
          &lt;li&gt;Ashraf : All goals can be described by the maximization of expected cumulative rewards&lt;/li&gt;
          &lt;li&gt;Silver : All goasl can be described by the maximization of expected cumulative rewards&lt;/li&gt;
          &lt;li&gt;Sutton : What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습의 정의
        &lt;ul&gt;
          &lt;li&gt;Intelligent behavior arises from the actions of an individual seeking to maximize its received reward signals in a complex and changing world.
            &lt;ul&gt;
              &lt;li&gt;identify where reward signals come from,&lt;/li&gt;
              &lt;li&gt;develop algorithms that search the space of behaviors to maximize reward signals.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;보상은 복잡한 실 세계 속에서 단순해야 한다.&lt;/li&gt;
          &lt;li&gt;에이전트가 최적화해야 할 보상이 무엇인지 알아내야 한다.&lt;/li&gt;
          &lt;li&gt;위 보상을 최대화 할 알고리즘을 디자인해야 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상 정의의 어려움
        &lt;ul&gt;
          &lt;li&gt;공통 통화 정의의 어려움
            &lt;ul&gt;
              &lt;li&gt;온도조절장치 제어의 경우 : 난방/에어컨 등의 에너지 비용 vs 거주자의 불편도?&lt;/li&gt;
              &lt;li&gt;위의 정도를 달러로 표기? (자연스럽지가 않음)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;하나의 예는 목적을 달성했을 때 +1, 그렇지 않은 경우 0 (goal reward Representation)&lt;/li&gt;
          &lt;li&gt;반대로 목적이 달성하지 않았을때  매 스텝마다 -1 (action penalty Representation)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상의 정의 방식에 따른 차이 발생
        &lt;ul&gt;
          &lt;li&gt;Goal-reward  Representation : 긴급함이 없음&lt;/li&gt;
          &lt;li&gt;Action-Penalty Representation : 진행이 막혀 더이상 목적을 달성할 수 없는 확률이 있을 경우 오작동&lt;/li&gt;
          &lt;li&gt;두 경우 모두 흐름이 길 경우 큰 문제가 발생함.&lt;/li&gt;
          &lt;li&gt;어떠한 중간 보상은 에이전트가 올바른 방향으로 학습하는데 큰 도움이 될 수도 있음&lt;/li&gt;
          &lt;li&gt;보상의 출처가 사람일 경우
            &lt;ul&gt;
              &lt;li&gt;사람은 보상 기능과 다르게 에이전트가 학습하는 방식에 따라 보상을 변경하는 경향이 있음 (Non-stationary Rewards)
                &lt;ul&gt;
                  &lt;li&gt;고전적인 강화학습 알고리즘은 위와 같은 경우 잘 적용되지 않음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상의 지정
        &lt;ul&gt;
          &lt;li&gt;프로그래밍 방식
            &lt;ul&gt;
              &lt;li&gt;학습 에이전트에 대한 보상을 정의하는 가장 일반적인 방법&lt;/li&gt;
              &lt;li&gt;예 : 상태를 가져오고 보상을 출력하는 프로그램을 작성&lt;/li&gt;
              &lt;li&gt;프로그래밍 언어의 예 - temporal logic : 사람과 기계 언어의 중간 지점&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;예를 들어 사람이 주는 보상을 복사하는 방법을 배우는 것 : Mimic reward&lt;/li&gt;
          &lt;li&gt;이 접근 방식의 흥미로운 버전 : 역 강화학습 : Inverse Reinforcement Learning
            &lt;ul&gt;
              &lt;li&gt;역 강화학습에서 지도자는 원하는 행동의 예를 보여주고 학습자는 지도자가 이 행동을 최적으로 만들기 위해 최대화한 보상이 무엇인지 파악&lt;/li&gt;
              &lt;li&gt;강화학습이 보상에서 행동으로 진행되는 반면, 역강화학습은 행동에서 보상으로 진행됨.&lt;/li&gt;
              &lt;li&gt;일단 보상이 식별되면 이러한 보상은 다른 설정에서 최대화 될 수 있으므로 환경 간에 강력한 일반화가 가능함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;최적화 프로세스를 통해 간접적으로 파생되는 보상
            &lt;ul&gt;
              &lt;li&gt;점수를 생성할 수 있는 높은 수준의 행동이 있는 경우&lt;/li&gt;
              &lt;li&gt;해당 행동을 장려하는 보상을 고려할 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Meta reinforcement learning
            &lt;ul&gt;
              &lt;li&gt;동 목표를 수행하는 단일 에이전트가 아닌 다수 에이전트가 있을 경우&lt;/li&gt;
              &lt;li&gt;행동의 결과로 보상을 받는 것 뿐만 아닌 이 행동에 대한 인센티브로 보상을 사용하여 평가할 수 있게 함&lt;/li&gt;
              &lt;li&gt;강화학습 에이전트는 더 좋은 보상 기능과 이를 최대화하기 위한 더 좋은 알고리즘이 있는 경우 생존함&lt;/li&gt;
              &lt;li&gt;개별 수준에서 더 나은 학습 방법을 만드는 진화 수준에서의 학습방법&lt;/li&gt;
              &lt;li&gt;에이전트가 자손에게 보상 함수를 계승함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보상 이론의 도전과제
        &lt;ul&gt;
          &lt;li&gt;보상을 극대화 하는 것 이외에 다른 일을 하는것 처럼 보이는 행동의 예
            &lt;ul&gt;
              &lt;li&gt;위험 회피 행동 : 최선이 아닐 수 있지만 최악의 결과가 발생할 가능성을 최소화 하는 행동을 선택&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;원하는 행동이 항상 최선의 것이 아니라 여러 가지를 균형있게 수행하는 것이라면?
            &lt;ul&gt;
              &lt;li&gt;예 : 음악 추천 시스템 (최근 재생된 곡의 경우 그 곡에 대한 보상이 줄어야 함)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;올바른 결정이 단순히 하나의 목표를 추구하는 것이 아니라면?
            &lt;ul&gt;
              &lt;li&gt;더 나은 목표를 만드는 것&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;그럼에도 보상을 극대화 하는 것이 지능형 에이전트에게 동기를 부여하는 훌륭한 근사치일 수 있다는 가능성을 염두&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;continuing-tasks&quot;&gt;Continuing Tasks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;연속적인 작업에 대해 할인을 이용한 보상의 수식화&lt;/li&gt;
      &lt;li&gt;연속적인 time step 에서 보상이 상호간 어떻게 관련되어 있는지 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Continuing Tasks
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Episodic tasks 와 달리 현실에서는 에이전트와 환경이 계속 상호작용 하는 경우 (Continuing) 가 많다.&lt;/p&gt;

        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th style=&quot;text-align: left&quot;&gt;Episodic Tasks&lt;/th&gt;
              &lt;th style=&quot;text-align: left&quot;&gt;Continuing Tasks&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;상호작용이 자연스럽게 끝남&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;상호작용이 계속 유지됨&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;각각의 에피소드는 Terminal State 로 끝남&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;Terminal state 가 없음&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;에피소드는 상호독립적임&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + … + R_T$&lt;/td&gt;
              &lt;td style=&quot;text-align: left&quot;&gt;$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + …$ (무한)&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;예 ) 건물 안 온도조절기 : 환경과의 상호작용에 끝이 없음.
        &lt;ul&gt;
          &lt;li&gt;상태 : 시간, 건물 내 사람 수&lt;/li&gt;
          &lt;li&gt;액션 : 켜기, 끄기&lt;/li&gt;
          &lt;li&gt;보상 : 누군가가 온도를 수동조절 할 경우 -1, 그렇지 않은 경우 0&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;State 가 무한함에 따라 보상의 합이 무한으로 발산하는 것을 방지하기 위해 Discounting 을 한다.
        &lt;ul&gt;
          &lt;li&gt;Discounting 계수 $\gamma$ , $0 \leq \gamma &amp;lt; 1$&lt;/li&gt;
          &lt;li&gt;$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + … + \gamma^{k-1} R_{t+k} + … =  \sum_{k=0}^\infty\gamma^k R_{t+k+1}$&lt;/li&gt;
          &lt;li&gt;이것은 현재의 1 달러가 1년 뒤에 받을 1달러 보다 가치가 있다고 생각하면 이치에 맞는다.&lt;/li&gt;
          &lt;li&gt;$G_t$ 가 무한하지 않음 (Finite) 을 증명하는 수식
            &lt;ul&gt;
              &lt;li&gt;$G_t = \sum_{k=0}^\infty\gamma^k R_{t+k+1} \leq \sum_{k=0}^\infty\gamma^k R_{max} = R_{max} \sum_{k=0}^\infty\gamma^k = R_{max} \times {1 \over 1-\gamma}$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$\gamma$ 값에 따른 에이전트의 성향
            &lt;ul&gt;
              &lt;li&gt;$\gamma = 0$ : 즉각적인 보상만을 추구하는 에이전트 (Short-sighted)&lt;/li&gt;
              &lt;li&gt;$\gamma \to 1$ : 미래의 보상을 중시함 (Far-sighted)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Return 값의 재귀적 표현법
        &lt;ul&gt;
          &lt;li&gt;$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + … = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + … )$&lt;/li&gt;
          &lt;li&gt;$G_t = R_{t+1} + \gamma G_{t+1}$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><category term="Markov Decision Processes" /><summary type="html">Course Introduction</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 01. Week 1</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 01. Week 1" /><published>2023-01-26T18:00:00+09:00</published><updated>2023-01-26T18:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/">&lt;h2 id=&quot;course-introduction&quot;&gt;Course Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised Learning (지도학습)
    &lt;ul&gt;
      &lt;li&gt;학습자가 답이 기입되어 있는 라벨링된 예시에 접근함&lt;/li&gt;
      &lt;li&gt;정답이 무엇이었는지 말해주는 선생이 존재&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;학습데이터 : 정답 (혹은 정답인 동작) 을 가르쳐주는 정보를 학습한다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;피드백 : 취해야 할 올바른 조치를 나타낸다. (이번 행동이 얼마나 좋았는지는 알려주지 않는다.)&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;피드백은 행동에 대해 완전 독립적.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning (자율학습)
    &lt;ul&gt;
      &lt;li&gt;데이터 기저에 있는 구조를 추출 (데이터 표현법)
        &lt;ul&gt;
          &lt;li&gt;이 데이터 표현법은 지도학습이나 강화학습에 도움을 줄 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reinforcement Learning (강화학습)
    &lt;ul&gt;
      &lt;li&gt;학습자에게 최근의 행동에 대한 보상을 제공함&lt;/li&gt;
      &lt;li&gt;좋은행동이 어떤것일지 식별해주는 환경이 존재하나 정확히 어떻게 해야하는지는 알려주지 않음&lt;/li&gt;
      &lt;li&gt;지도학습이나 자율학습을 통해 일반화 (Input 이 달라져도 출력성능에 영향을 주지 않음) 를 개선할 수 있음&lt;/li&gt;
      &lt;li&gt;바뀌는 환경속에서 상호작용하며 학습하는 것에 주안점을 둠
        &lt;ul&gt;
          &lt;li&gt;학습자가 단순히 반복되는 환경에서 계산을 통해 좋은 행동이 무엇인지 학습하는 것이 아님.&lt;/li&gt;
          &lt;li&gt;학습자가 시행착오를 통해 변화하는 환경에서 목표를 바꾸어 가며 더 잘하는 것을 추구&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;학습데이터 : 행동을 수행한 것에대한 평가를 학습한다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;피드백 : 이번 행동이 얼마나 좋았는지를 피드백한다. (그것이 최선/최악인지는 알 수 없다)&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;피드백은 행동에 완전 종속됨.&lt;/li&gt;
          &lt;li&gt;때문에 최선의 행동을 찾아 끊임없이 탐험해야 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;관련-자료-rlbook2018-pages-25-36&quot;&gt;관련 자료 (RLbook2018 Pages 25-36)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;용어설명
    &lt;ul&gt;
      &lt;li&gt;Stationary (정상성) Probability : 여러 시간 구간마다 통계적 성질이 동일한 것
        &lt;ul&gt;
          &lt;li&gt;예) 주사위 던지기 : 시간에 무관하게 동일한 확률값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Non-Stationary (비정상성) Probability : 시간에 따라 통계적 성질이 변하는 것
        &lt;ul&gt;
          &lt;li&gt;예) 변덕스러운 날씨, 기후 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nonassociative setting : 다른 Action(행동), 다른 Environment(환경) 을 가정하지 않는 세팅
        &lt;ul&gt;
          &lt;li&gt;full reinforcement learning problem 의 복잡성을 배제 (다양한 환경, 비정상성, 행동의 보상이 즉각적이지 않음 - 현실세계)&lt;/li&gt;
          &lt;li&gt;이미 환경이 어떠한 피드백을 줄지 알고 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multi-armed Bandits
    &lt;ul&gt;
      &lt;li&gt;간단한 세팅 환경 (K-armed bandit problem 의 단순화 버전)
        &lt;ul&gt;
          &lt;li&gt;한가지 이상 상황에서의 동작을 학습하는 것을 배제 (Nonassociative setting)&lt;/li&gt;
          &lt;li&gt;목적1 : 평가 피드백 (Evaluative feedback) 이 정답을 알려주는 피드백 (Instructive Feedback) 과 어떻게 다른지 확인&lt;/li&gt;
          &lt;li&gt;목적2 : 둘이 결합될 수 있는지 확인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;목표
        &lt;ul&gt;
          &lt;li&gt;기초적 학습법 (Learning methods) 안내&lt;/li&gt;
          &lt;li&gt;이 Bandit problem 이 associative (연관성) 성질을 가지게 되었을 때 어떻게 변할지 확인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;A k-armed Bandit Problem
        &lt;ul&gt;
          &lt;li&gt;문제에 대한 설명
            &lt;ul&gt;
              &lt;li&gt;반복적으로 k 개의 다른 옵션/행동을 선택&lt;/li&gt;
              &lt;li&gt;각 선택 마다 보상 (선택한 행동에 따른 정상성 확률 분산을 가진 보상) 을 받는 문제&lt;/li&gt;
              &lt;li&gt;목적 : 특정 기간동안 최대한의 보상을 받는 것&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;문제에 대한 예시 1
            &lt;ul&gt;
              &lt;li&gt;슬롯머신 혹은 “one-armed bandit” 문제와 동일하나 단지 레버가 k 가임.&lt;/li&gt;
              &lt;li&gt;각 행동은 여러 대의 슬롯머신 중 한 대의 레버를 당기는 것과 동일&lt;/li&gt;
              &lt;li&gt;가장 이익을 극대화 할 수 있는 레버에 집중하여 보상을 많이 받는 것이 목표&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;문제에 대한 예시 2
            &lt;ul&gt;
              &lt;li&gt;의사가 심각한 질병의 환자에게 실험적 치료들 중 하나를 선택&lt;/li&gt;
              &lt;li&gt;보상 : 환자의 생존/ 치유율&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_1_1_value_of_action.png&quot; alt=&quot;2_1_1_value_of_action&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 문제를 풀었을 때 ($q_*$) 의 value of the action 에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;value of the action : 각각의 k 행동들이 가지는 기대/평균 보상값&lt;/li&gt;
              &lt;li&gt;$A_t$ : time-step $t$ 시점에 선택한 action&lt;/li&gt;
              &lt;li&gt;$R_t$ : $A_t$ 에 상응하는 보상&lt;/li&gt;
              &lt;li&gt;$q_*(a)$ : $a$ 가 선택되었을때 기대되는 보상값&lt;/li&gt;
              &lt;li&gt;$\doteq$ : is defined as&lt;/li&gt;
              &lt;li&gt;If you knew the “value of the each action” : 해당 문제를 풀었다고 볼 수 있음.&lt;/li&gt;
              &lt;li&gt;$Q_t(a)$ : $q_*(a)$ 와 유사한 값 (중간값)&lt;/li&gt;
              &lt;li&gt;$q_*$ 값은 에이전트가 알고 있는 값이 아님&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;greedy actions
            &lt;ul&gt;
              &lt;li&gt;action value 를 계속 추정하다 보면 어느 시점에서나 적어도 하나의 가장 큰 예측값을 가지는 action 이 존재&lt;/li&gt;
              &lt;li&gt;이것을 greedy actions 라고 함.&lt;/li&gt;
              &lt;li&gt;이 greedy action 을 선택하는 것 : 현재 알고 있는 values of the action 값을 exploiting 한다.&lt;/li&gt;
              &lt;li&gt;이 greedy action 을 선택하지 않는 것 : 현재 exploring 중이라고 한다.&lt;/li&gt;
              &lt;li&gt;Exploitation (이기적 이용)
                &lt;ul&gt;
                  &lt;li&gt;현 step 에서 예측되는 보상값을 최대화 하는 옳은 방법&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Exploration (탐색)
                &lt;ul&gt;
                  &lt;li&gt;장기 관점으로 보았을 때 이쪽의 보상 총합이 더 클 수 있음&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Balancing exploration and exploitation
            &lt;ul&gt;
              &lt;li&gt;k-armed bandit 과 이와 유사한 문제의 특정 수학적 공식에 대해 탐색과 이용의 밸런스를 맞출 수 있는 정교한 방법이 존재
                &lt;ul&gt;
                  &lt;li&gt;하지만 이런 방법들은 정상성에대한 강한 가정을 전재하고, full reinforcement learning 환경이나 어플리케이션을 침해하거나  증명할 수 없는 사전지식을 이용한다.&lt;/li&gt;
                  &lt;li&gt;즉 활용 불가. (Full reinforcement learning 환경에서 이 밸런스 문제는 도전적인 과제임)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;k-armed bandit problem 에서는 균형에 대해서만 고려하고, 단순 Exploitation 하는 것보다 균형을 맞춘 방식이 더 잘 작동한다는 것을 증명할 것임.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Action-value Methods
        &lt;ul&gt;
          &lt;li&gt;행동 선택을 위해 행동(Action) 에 대한 가치를 측정하는 방법을 통상 Action-value method 라 한다.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명 1&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_2_1_averaging_the_rewards.png&quot; alt=&quot;2_2_1_averaging_the_rewards&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 action-value method 중 보상평균값으로 추정하는 방식에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;Action-value 는 통상적으로 해당 Action 이 선택되었을 때 보상 값의 평균치를 말한다.&lt;/li&gt;
              &lt;li&gt;1(predictate) 는 행위를 하였을 때는 1, 아닌 경우 0 (가상의 1/0, 횟수의 개념)&lt;/li&gt;
              &lt;li&gt;분모가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.&lt;/li&gt;
              &lt;li&gt;이 방식을 sample-average method 라 부르며, 이는 Action-value 를 구하기 위한 많은 방법 중 하나이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;수식설명 2&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_2_2_greedy_action_selection_method.png&quot; alt=&quot;2_2_2_greedy_action_selection_method&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;위 수식은 greedy action selection method 에 대한 수식이다.&lt;/li&gt;
              &lt;li&gt;action selection 에서 가장 단순한 방법은 가장 큰 예측값에 해당하는 action을 선택하는 것이다.&lt;/li&gt;
              &lt;li&gt;현 시점 이전까지 가장 탐욕적인 action 으로 정의된 행동을 하는 것.&lt;/li&gt;
              &lt;li&gt;$\underset{a}{\arg\max}$ : 후술되는 값이 최대값이 되는 action $a$ 를 의미&lt;/li&gt;
              &lt;li&gt;Greedy action selection 은 현 지식을 이용해 당장의 보상을 최대화 하는 방식&lt;/li&gt;
              &lt;li&gt;정말 더 나은 방식을 찾기 위해 열등한 방식을 샘플링하는데 시간을 할애하지 않음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$\epsilon$-greedy methods
            &lt;ul&gt;
              &lt;li&gt;위와 다른 대안으로 대부분 탐욕스러운 행동을 하되&lt;/li&gt;
              &lt;li&gt;아주 작은 확률($\epsilon$)로 action-value 와 관계 없이 균등한 확률로 $a$ 를 선택하는 방법이 있음.&lt;/li&gt;
              &lt;li&gt;이 방법을 통한 샘플링 횟수가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.&lt;/li&gt;
              &lt;li&gt;이것은 점근적인 보장일 뿐, 실질적인 효과에 대한 것은 아니다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The 10-armed Testbed
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;조건 설명&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_3_1_10_armed_testbed.png&quot; alt=&quot;2_3_1_10_armed_testbed&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;10-armed testbed 의 bandit problem 문제 예시&lt;/li&gt;
          &lt;li&gt;각 10 개 action 의 true value $q_*(a)$ 값 은 평균 0, 분산 1 인 정규분포 (표준정규분포) 를 따른다.&lt;/li&gt;
          &lt;li&gt;위 조건으로 2000번의 서로 다른 bandit problem 을 수행, 평균 값을 취함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;결과&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_3_2_result_10_armed_testbed.png&quot; alt=&quot;2_3_2_result_10_armed_testbed&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;보상의 분산이 클수록 더 많은 탐험이 필요하며 $\epsilon$-greedy methods 가 greedy methods 보다 더 잘 작동한다.&lt;/li&gt;
          &lt;li&gt;만약 보상의 분산이 0이라면 각각의 action 후에 true value 값을 바로 알 수 있게 된다.&lt;/li&gt;
          &lt;li&gt;위의 경우 greedy methods 가 더 잘 작동하게 된다. (탐험할 필요가 없음.)&lt;/li&gt;
          &lt;li&gt;그러나 이런 결정론적인 상황에서도 몇가지 가정이 불확실하다면 탐험하는 쪽이 유리하다.
            &lt;ul&gt;
              &lt;li&gt;비정상성 환경 (시간에 따라 true action-value 가 변함.)&lt;/li&gt;
              &lt;li&gt;즉 Reinforcement learning 은 탐색과 이용에 균형이 필요함.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Incremental Implementation
    &lt;ul&gt;
      &lt;li&gt;action-value methods 를 획득한 보상의 평균 값으로 추정한다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이 경우 어떻게 전산화 하여 계산할지?&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_1_estimated_value_of_single_action.png&quot; alt=&quot;2_4_1_estimated_value_of_single_action&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;특정 단일 Action 에 대한 Action-value 를 예측, action 은 $n-1$ 번 선택됨.&lt;/li&gt;
          &lt;li&gt;위 명확한 계산법은 모든 이전 기록을 가지고 있어야 하고, 예측값이 필요할 때마다 계산해야함.
            &lt;ul&gt;
              &lt;li&gt;메모리가 많이 필요하고 연산량이 상당해진다.&lt;/li&gt;
              &lt;li&gt;위의 방법이 아니라 이전 평균값에서 이번 보상값을 업데이트 하는 방식을 취하는 것이 효율적이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_2_update_action_value.png&quot; alt=&quot;2_4_2_update_action_value&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 구현법을 이용하면 메모리는 $Q_n$ 과 $n$ 값만을 저장하고 있고, 작은 계산을 통해 매번 새로운 예측값을 구할 수 있음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_3_update_action_value.png&quot; alt=&quot;2_4_3_update_action_value&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;수식의 뜻은 위와 같음&lt;/li&gt;
          &lt;li&gt;Target - OldEstimate = error
            &lt;ul&gt;
              &lt;li&gt;위 에러 값은 예측값이 Target 에 가까워질 수록 작아짐&lt;/li&gt;
              &lt;li&gt;Target 은 예측이 움직이기 원하는 방향을 가리킴&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;StepSize : 각 타임스텝마다 변하는 step-size parameter 이며, 이 책에서는 $\alpha$ 혹은 $\alpha_t(a)$ 로 나타낸다.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_4_4_simple_bandit_algorithm.png&quot; alt=&quot;2_4_4_simple_bandit_algorithm&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;완성된 simple bandit algorithm&lt;/li&gt;
          &lt;li&gt;breaking ties randomly.. 동점 기록일 경우 랜덤하게 선택한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tracking a Nonstationary Problem
    &lt;ul&gt;
      &lt;li&gt;보상의 확률이 변하는 reinforcement learning 문제에서는 오랜 과거 보상보다 최근 보상에 비중을 더 두는 것이 설득력있다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이러한 방법 중 하나로 상수 파라미터 (a constant step-size parameter) 를 사용하는 것이 유명하다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_5_1_constant_step_size_parameter.png&quot; alt=&quot;2_5_1_constant_step_size_parameter&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;위의 수식을 weighted average 라고도 하는데 각 가중치의 합이 1이 되기 때문이다.
            &lt;ul&gt;
              &lt;li&gt;$(1-\alpha)^n + \sum_{i=1}^n\alpha(1-\alpha)^{n-i} = 1$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$1-\alpha$ 값이 1보다 작기 때문에 승수가 커질수록 (이전 step 의 값일수록) 가중치 값이 decay 됨&lt;/li&gt;
          &lt;li&gt;때로는 위 수식을 exponential recency-weighted average 라고도 함 (지수적 최근성 가중치 평균)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;때로는 step 별로 변동하는 step-size parameter 를 사용하는 것이 편할 때가 있음.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 $\frac{1}{n}$ step-size parameter (sample-average method) 는 충분히 큰 step 을 진행할 경우 true action value 로 수렴하는 것을 보장한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;확률적 근사 이론은 확률 1로 수렴을 보장하는 데 필요한 조건을 제공한다.&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_5_2_conditions_required_to_convergence_with_prob1.png&quot; alt=&quot;2_5_2_conditions_required_to_convergence_with_prob1&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;첫번째 조건은 초기 조건이나 무작위 변동을 극복할 수 있을 정도로 step-size 가 큰 것을 보장&lt;/li&gt;
          &lt;li&gt;두번째 조건은 step-size 가 수렴을 확신할 정도로 충분히 작은 것을 보장&lt;/li&gt;
          &lt;li&gt;$\frac{1}{n}$ 은 이 두 조건을 모두 만족하나, 상수 step 파라미터는 두번째 조건을 충족하지 않아 가장 최근의 보상값에 의해 완전히 수렴하지 못하게 됨.
            &lt;ul&gt;
              &lt;li&gt;이것은 비정상성 환경에서 필요한 내용이다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;두 조건이 만족하더라도 매우 느리게 수렴하거나, 만족스러운 수렴율을 얻기 위해 파라미터를 튜닝해야 할 수도 있음.&lt;/li&gt;
          &lt;li&gt;위 이론은 이론적인 내용에는 자주 사용되나, 실제 적용 환경에서는 잘 사용되지 않음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic Initial Values
    &lt;ul&gt;
      &lt;li&gt;위에 언급한 모든 방법들은 initial action-value estimates ($Q_1(a)$) 에 어느정도 영향을 받는다.&lt;/li&gt;
      &lt;li&gt;통계적 표현으로 이러한 방식들은 biased by their initial estimates (초기추정치에 의해 편향된다) 라고 한다.&lt;/li&gt;
      &lt;li&gt;예를 들어 sample-average methods 는 이 편향이 모든 action을 한 번 이상씩 수행했을 때 사라진다면
        &lt;ul&gt;
          &lt;li&gt;액션 a=1 to k 에 대해 초기값 $Q(a)$, $N(a)$ 를 초기값으로 쓰고, 한 번이라도 a 가 시도되면 수식에 따른 값으로 변경&lt;/li&gt;
          &lt;li&gt;n 으로 나눌 때 0 으로 나눌 수 없으므로..&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;상수 a 를 사용하는 경우 이 편향은 영구적이다. (시간의 흐름에 따라 점차 감소하지만)
        &lt;ul&gt;
          &lt;li&gt;별도의 초기값 할당이 아니라 처음부터 수식을 사용하되, 수식 내 0번째 스텝의 값이 초기값임.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;장점 : 예상할 수 있는 보상 수준에 대한 사전지식을 제공하는 쉬운 방법&lt;/li&gt;
      &lt;li&gt;단점 : 모든 매개변수를 0 으로 설정하는 경우 사용자가 반드시 파라미터를 선택해야 한다.&lt;/li&gt;
      &lt;li&gt;간단한 탐색 장려의 방법 : 초기 값을 0 대신 +5 로 설정 (10-armed testbed 상황으로 가정)
        &lt;ul&gt;
          &lt;li&gt;초기값 +5 의 값은 매우 낙관적인 수치&lt;/li&gt;
          &lt;li&gt;특정 action 을 선택하고 받는 보상 값이 예측치보다 작음 (disappointed with the rewards)&lt;/li&gt;
          &lt;li&gt;학습자는 다른 action 을 선택하게 되고 이 상황을 몇번 반복됨. (greedy action 일지라도…)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_6_1_optimistic_greedy.png&quot; alt=&quot;2_6_1_optimistic_greedy&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;10-armed bandit testbed 에서 greedy method 를 초기값 $Q_1(a) = +5$ 로 세팅한 결과&lt;/li&gt;
          &lt;li&gt;비교군은 $\epsilon$-greedy method 에 초기값 $Q_1(a) = 0$&lt;/li&gt;
          &lt;li&gt;이 trick 은 stationary problem 에서 꽤 효과적이나, 탐색을 장려하는 일반적인 방법은 아님.&lt;/li&gt;
          &lt;li&gt;이러한 비판은 sample-average methods 에서도 통용된다.
            &lt;ul&gt;
              &lt;li&gt;초기 시점을 특수한 이벤트로 여긴다.&lt;/li&gt;
              &lt;li&gt;모든 보상을 똑같은 가중치로 평균을 구한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upper-Confidence-Bound Action Selection
    &lt;ul&gt;
      &lt;li&gt;action-value 추정값의 불확실성 때문에 탐험은 반드시 필요하다.&lt;/li&gt;
      &lt;li&gt;greedy actions 는 현재 시점에는 가장 최적의 선택이나 다른 action 이 실제로는 더 좋은 것일 수 있다.&lt;/li&gt;
      &lt;li&gt;$\epsilon$-greedy action selection 은 강제적으로 non-greedy action 을 선택하지만 선호도 없이 무차별적인 선택을 하여 greedy 한 선택을 하게 될 수도 있다.
        &lt;ul&gt;
          &lt;li&gt;강제적인 선택을 할 때는 non-greedy 한 선택을 하는 것이 좋음&lt;/li&gt;
          &lt;li&gt;추정치가 최대치에 얼마나 가까운지와 추정치의 불확실성을 모두 고려&lt;/li&gt;
          &lt;li&gt;실제로 최적일 가능성에 따라 탐욕스럽지 않은 action 을 선택하는 것이 좋음.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_7_1_UCB.png&quot; alt=&quot;2_7_1_UCB&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;UCB (Upper-Confidence-Bound) Action Selection 은 그러한 효율적인 방식 중 하나이다.&lt;/li&gt;
          &lt;li&gt;$N_t(a)$ 는 t step 이 진행되었을때 a action 이 선택된 횟수로, 많이 선택될 수록 우항의 피연산자 값이 작아진다.&lt;/li&gt;
          &lt;li&gt;$\ln t$ 는 step 이 커짐에 따라 값이 무한대까지 증가 (수렴하지 않음) 하나 그 증가폭이 서서히 줄어든다&lt;/li&gt;
          &lt;li&gt;$c$ 는 탐험의 정도 (강도) 를 나타내는 수치이다.&lt;/li&gt;
          &lt;li&gt;즉 action이 많이 선택될수록 $Q_t(a)$ 의 값은 정확해지고, 우항의 피연산자 값은 작아진다.&lt;/li&gt;
          &lt;li&gt;action 이 한번도 선택되지 않을 경우 해당 a 를 maximizing action 으로 여긴다. (해당 a 에 대한 무조건적인 탐험)&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/images/posts/2_7_2_UCB.png&quot; alt=&quot;2_7_2_UCB&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;이러한 UCB 방식은 10-armed testbed 에서 $\epsilon$-greedy 보다 나은 성과를 보여주기도 한다.&lt;/li&gt;
          &lt;li&gt;단 몇가지 단점으로 인해 실용적이지 않은 방식이다.
            &lt;ul&gt;
              &lt;li&gt;bandits 문제에서 reinforcement learning 문제로 확장하기 어렵다.&lt;/li&gt;
              &lt;li&gt;nonstationary 한 문제들에는 더 어려운 방식의 action-value method 가 필요하다.&lt;/li&gt;
              &lt;li&gt;훨신 더 거대한 환경 (state space) 에서의 적용 (특히 뒤에 배울 function approximation 방식) 이 어렵다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-1-the-k-armed-bandit-problem&quot;&gt;Lesson 1: The K-Armed Bandit Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define reward&lt;/li&gt;
      &lt;li&gt;Understand the temporal nature of the bandit problem&lt;/li&gt;
      &lt;li&gt;Define k-armed bandit&lt;/li&gt;
      &lt;li&gt;Define action-values&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sequential Decision Making with Evaluative Feedback
    &lt;ul&gt;
      &lt;li&gt;불확실성 아래에서의 의사결정
        &lt;ul&gt;
          &lt;li&gt;의사가 3가지 처방약으로 환자에게 실험적 처방을 할 때…&lt;/li&gt;
          &lt;li&gt;몇번의 환자 반응을 보고 가장 잘 듣는 약을 고집할 경우
            &lt;ul&gt;
              &lt;li&gt;더이상 다른 약의 데이터를 모을 수 없음&lt;/li&gt;
              &lt;li&gt;나머지 두 약이 실제로 나음에도 몇몇 결과가 나쁘게 나왔는지 알 수 없음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;다른 약으로 계속 실험할 경우
            &lt;ul&gt;
              &lt;li&gt;나쁜 결과를 계속 초래할 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;강화학습에서의 기초적 컨셉 용어
        &lt;ul&gt;
          &lt;li&gt;에이전트 (Agent) : action을 선택하는 존재 - 의사&lt;/li&gt;
          &lt;li&gt;액션 (Action) : 선택지 - 3가지 약 중 하나를 선택하는 것&lt;/li&gt;
          &lt;li&gt;보상 (Rewards) : 액션에 대한 결과 - 환자의 상태&lt;/li&gt;
          &lt;li&gt;값 (Value Function - Action-Value (Function)) : 기대되는 보상 값 - 환자의 혈압 값
            &lt;ul&gt;
              &lt;li&gt;에이전트가 액션을 선택했을 때 그 값 (Action-Value = $q_*$)이 최대화 할 경우 목적을 달성함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$q_*(a)$ 값 구하기
        &lt;ul&gt;
          &lt;li&gt;각 약의 결과값이 서로 다른 확률분포를 가졌을 경우 $q_*$ 는 각 분포의 평균이 될 수 있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bandits Problem 을 고려하는 이유
        &lt;ul&gt;
          &lt;li&gt;문제와 알고리즘 디자인 선택에 있어 가장 간단한 세팅&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-2-what-to-learn-estimating-action-values&quot;&gt;Lesson 2: What to Learn? Estimating Action Values&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define action-value estimation methods&lt;/li&gt;
      &lt;li&gt;Define exploration and exploitation&lt;/li&gt;
      &lt;li&gt;Select actions greedily using an action-value function&lt;/li&gt;
      &lt;li&gt;Define online learning&lt;/li&gt;
      &lt;li&gt;Understand a simple online sample-average action-value estimation method&lt;/li&gt;
      &lt;li&gt;Define the general online update equation&lt;/li&gt;
      &lt;li&gt;Understand why we might use a constant step-size in the case of non-stationarity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning Action Values
    &lt;ul&gt;
      &lt;li&gt;Estimate action values using the sample-average method
        &lt;ul&gt;
          &lt;li&gt;의사가 처방 후 환자가 나아질 경우 1, 그렇지 않은 경우 0 으로 표기하고 3개의 약을 처방해 평균을 구함&lt;/li&gt;
          &lt;li&gt;Step 이 많이 진행될 수록 평균 데이터는 더 정확해짐&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Describe greedy action selection
        &lt;ul&gt;
          &lt;li&gt;의시가 해당 시점에 가장 기대치가 큰 약을 처방할 경우 이 행동을 Greedy action 이라고 함&lt;/li&gt;
          &lt;li&gt;greedy action 을 선택하는 것을 현 지식을 활용한 이용 (exploitation) 이라고 함&lt;/li&gt;
          &lt;li&gt;당장의 기대되는 보상을 포기하고 다른 선택을 하는 것을 non-greedy action 이라고 하고 이를 탐험 (exploration) 이라고 함&lt;/li&gt;
          &lt;li&gt;탐험을 통해 기대되는 보상을 희생하고 non-greedy action 의 보상에 대한 더 많은 정보를 얻게 됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Introduce the exploration-exploitation dilemma
        &lt;ul&gt;
          &lt;li&gt;에이전트는 동시에 탐험과 이용을 할 수 없음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Estimating Action Values Incrementally
    &lt;ul&gt;
      &lt;li&gt;action value 의 점진적 표현법 (sample-average method 를 이용)
        &lt;ul&gt;
          &lt;li&gt;Incremental update rule&lt;/li&gt;
          &lt;li&gt;NewEstimate &amp;lt;- OldEstimate + StepSize(Target - OldEstimate)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;점진적 업데이트 룰(Incremental update rule)이 더 널리 쓰이는 이유
        &lt;ul&gt;
          &lt;li&gt;모든 이전 값들을 기억할 필요가 없다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;보편적 업데이트 룰을 non-stationary bandit problem 에 적용하는 방법
        &lt;ul&gt;
          &lt;li&gt;Non-stationary Bandit Problem : 의사의 3가지 약중 특정 하나의 약이 겨울이 되면 효율이 높아진다.&lt;/li&gt;
          &lt;li&gt;보상의 분포가 시간에 따라 변하게 되는 경우를 Non-stationary 하다 라고 함.&lt;/li&gt;
          &lt;li&gt;StepSize 파라미터가 상수 (예:0.1) 일 경우 이전 Step 일수록 영향도가 작아지고, 최신 Step의 보상값을 더 반영함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-3-exploration-vs-exploitation-tradeoff&quot;&gt;Lesson 3: Exploration vs. Exploitation Tradeoff&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습목표
    &lt;ul&gt;
      &lt;li&gt;Define epsilon-greedy&lt;/li&gt;
      &lt;li&gt;Compare the short-term benefits of exploitation and the long-term benefits of exploration&lt;/li&gt;
      &lt;li&gt;Understand optimistic initial values&lt;/li&gt;
      &lt;li&gt;Describe the benefits of optimistic initial values for early exploration&lt;/li&gt;
      &lt;li&gt;Explain the criticisms of optimistic initial values&lt;/li&gt;
      &lt;li&gt;Describe the upper confidence bound action selection method&lt;/li&gt;
      &lt;li&gt;Define optimism in the face of uncertainty&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What is the trade-off?
    &lt;ul&gt;
      &lt;li&gt;exploration-exploitation 의 등가교환
        &lt;ul&gt;
          &lt;li&gt;Exploration (탐색) : 장기적 이익을 위해 지식을 늘림&lt;/li&gt;
          &lt;li&gt;Exploitation (이용) : 단기적 이익을 위해 지식을 이용&lt;/li&gt;
          &lt;li&gt;탐색만 하면 단기적 보상이 작아지고, 이용만 하면 타 선택지의 true value 를 모르기 때문에 장기적으로 손해일 수 있음&lt;/li&gt;
          &lt;li&gt;한번의 선택에서 탐색 과 이용 둘 중 하나만 가능&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Epsilon-Greedy 방식 (탐색과 이용의 균형을 맞추는 쉬운 방법)
        &lt;ul&gt;
          &lt;li&gt;Epsilon-Greedy 는 대부분 Exploitation (Greedy method) 하고, 적은 확률로 Exploration (Random choice) 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;각 방식을 비교할 때 한 번의 진행으로는 노이즈가 많아 확인이 어려움.
        &lt;ul&gt;
          &lt;li&gt;1000 개의 에이전트로 보상 데이터를 모아 평균값으로 비교&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic Initial Values
    &lt;ul&gt;
      &lt;li&gt;Optimistic initial values 가 초기 탐색을 장려하는 이유
        &lt;ul&gt;
          &lt;li&gt;초기 예상 보상치를 높게 잡고 시작&lt;/li&gt;
          &lt;li&gt;첫 선택의 보상이 주어지더라도 평균값으로 인해 값이 떨어짐&lt;/li&gt;
          &lt;li&gt;에이전트는 첫 선택에 실망을 하고 선택되지 않은 다른 높은 초기치의 옵션 중 하나를 선택&lt;/li&gt;
          &lt;li&gt;초기 탐험을 통해 모든 Action 을 골고루 선택하게 됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Optimistic initial values 의 한계
        &lt;ul&gt;
          &lt;li&gt;초기 단계에서만 탐색을 진행한다. 이는 Non-stationary Problems 에서 문제가 됨.&lt;/li&gt;
          &lt;li&gt;Maximal Reward 를 시작하기 전 알 방법이 없기에 Optimistic Initial Value 를 어느정도로 설정해야 할지 모른다.&lt;/li&gt;
          &lt;li&gt;그럼에도 Optimistic initial values 는 휴리스틱 (충분한 정보 없이 빠르게 사용할 수 있는 직관적인) 한 방법으로 자주 활용됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upper-Confidence Bound(UCB) Action Selection
    &lt;ul&gt;
      &lt;li&gt;UCB action-selection 방식이 예측의 불확실성을 이용해 탐색을 유도하는 방법&lt;/li&gt;
      &lt;li&gt;UCB의 C는 confidence 를 뜻하며, 예측한 $Q(a)$ 값의 오차범주 범위 $(c)$ 내에 실제 값이 들어올 것이라 확신하는 정도의 수치이다.
        &lt;ul&gt;
          &lt;li&gt;범위가 작을수록 결과에 더욱 확신한다는 뜻임&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;불확실성 (범위) 에 마주했을때 가장 높은 값 (Upper-Confidence Bound) 을 선택&lt;/li&gt;
      &lt;li&gt;수식에 대한 풀이
        &lt;ul&gt;
          &lt;li&gt;c : 사용자 정의 파라미터 (얼마나 탐험을 할 건지 컨트롤)&lt;/li&gt;
          &lt;li&gt;좌측 피연산자 : Exploit (이용)&lt;/li&gt;
          &lt;li&gt;우측 피연산자 : Explore (탐험)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Contextual Bandits for Real World Reinforcement Learning
    &lt;ul&gt;
      &lt;li&gt;Contextual Bandits 는 Reinforcement Learning 이 현실에 배포되는 방식임&lt;/li&gt;
      &lt;li&gt;Reinforcement 는 현실과 어떻게 다른가?
        &lt;ul&gt;
          &lt;li&gt;Reinforcement Learning 은 시뮬레이터로 동작한다.
            &lt;ul&gt;
              &lt;li&gt;시뮬레이터는 관측치를 제공&lt;/li&gt;
              &lt;li&gt;Learning 알고리즘은 어떤 Action을 선택할 지 정책을 가진다.&lt;/li&gt;
              &lt;li&gt;시뮬레이터는 실행하고 보상을 제공한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;현실이 제공하는 관측값은 시뮬레이터와 다르다.
            &lt;ul&gt;
              &lt;li&gt;같은 정책이라 해도 관측치가 다르기 때문에 시뮬레이터에서의 Action 과 다른 Action 을 수행하게 된다.&lt;/li&gt;
              &lt;li&gt;시뮬레이터와 현실은 보상도 다르다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;즉 현실과 시뮬레이터에는 갭이 존재한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Real World based Reinforcement Learning : 현실 기반 강화학습을 하려면 우선순위를 변경해야 함
        &lt;ul&gt;
          &lt;li&gt;Temporal Credit Assignment &amp;lt; Generalization
            &lt;ul&gt;
              &lt;li&gt;Temporal Credit Assignment Problem (CAP) : 시간적 기여도 할당문제. 일련의 행동이 모두 끝난 뒤 보상을 얻을 수 있는 환경에서 수행한 행동들 중 어떠한 행동이 기여도가 있고 어떠한 행동이 벌점을 줄 것인지 결정하는 문제.&lt;/li&gt;
              &lt;li&gt;Generalization : 다양한 관찰값을 통한 일반화&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Control environment &amp;lt; Environment controls
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션 환경에서는 자유자재로 한 스텝을 더 진행하거나 할 수 있다면 (컨트롤 가능)&lt;/li&gt;
              &lt;li&gt;현실에서는 환경이 지배함. (환경에 의해 컨트롤당함)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Computational efficiency &amp;lt; Statistical efficiency
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션 환경에서 계산효율이 중요하다면 (학습할 샘플이 많으므로)&lt;/li&gt;
              &lt;li&gt;현실에서는 통계적 효율이 중요함 (현실이 주는 샘플만 가질 수 있다.)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;State &amp;lt; Features
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션에서는 상태를 생각 (상태값은 결정을 내리는 데 기반이 되는 요소)&lt;/li&gt;
              &lt;li&gt;현실에서는 매우 복잡한 관측값을 가지는 경우 (필요한 것보다 정보가 많음) 가 많아 어느 것이 핵심 요인인지가 중요&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Learning &amp;lt; Evaluation
            &lt;ul&gt;
              &lt;li&gt;현실에서는 “Off Policy Evaluation” 이 중요하다.
                &lt;ul&gt;
                  &lt;li&gt;Off Policy : 정책 업데이트에 어떤 데이터를 써도 상관이 없는 경우&lt;/li&gt;
                  &lt;li&gt;즉, 최신 업데이트 정책에서 수집된 데이터가 아니어도 사용가능&lt;/li&gt;
                  &lt;li&gt;학습 알고리즘이 학습 뿐만 아니라 정책 평가에 쓰일 수 있는 부산물 데이터 또한 제공&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Last policy &amp;lt; Every Policy
            &lt;ul&gt;
              &lt;li&gt;시뮬레이션에서는 가장 최신의 정책이 중요&lt;/li&gt;
              &lt;li&gt;현실에서는 모든 포인트의 데이터가 세상과의 어느 정도 상호작용이 포함되어 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-summary-rlbook2018-pages-42-43&quot;&gt;Chapter Summary (RLbook2018 Pages 42-43)&lt;/h2&gt;

&lt;h2 id=&quot;weekly-assessment&quot;&gt;Weekly Assessment&lt;/h2&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">Course Introduction</summary></entry><entry><title type="html">Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020</title><link href="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/" rel="alternate" type="text/html" title="Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020" /><published>2023-01-26T11:00:00+09:00</published><updated>2023-01-26T11:00:00+09:00</updated><id>https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020</id><content type="html" xml:base="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/">&lt;h2 id=&quot;1-구매정보&quot;&gt;1. 구매정보&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;빈티지 : 2020&lt;/li&gt;
  &lt;li&gt;가격 : 40,000 원 (할인 및 페이백 후 추정가)&lt;/li&gt;
  &lt;li&gt;구매처 : 편의점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-이미지&quot;&gt;2. 이미지&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/Textbook_Napa_Cabernet_Sauvignon.jpg&quot; alt=&quot;Textbook_Napa_Cabernet_Sauvignon.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;about:blank&quot; target=&quot;_blank&quot;&gt;링크없음&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-감상&quot;&gt;3. 감상&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;1일차
    &lt;ul&gt;
      &lt;li&gt;레드 와인&lt;/li&gt;
      &lt;li&gt;향이 강하지 않음&lt;/li&gt;
      &lt;li&gt;맛이 굉장히 가벼운 느낌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-총점&quot;&gt;3. 총점&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;3/5
    &lt;ul&gt;
      &lt;li&gt;많은 기대를 한 와인이었으나, 기대에 못미침&lt;/li&gt;
      &lt;li&gt;가볍고 경쾌한 느낌이어서 많이 마실 수 있었음&lt;/li&gt;
      &lt;li&gt;하지만 이번 와인을 계기로 난 와인보다 양주나 정종이 더 맞다는 결론을 내림.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Wine" /><category term="와인" /><summary type="html">1. 구매정보 빈티지 : 2020 가격 : 40,000 원 (할인 및 페이백 후 추정가) 구매처 : 편의점</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 분석 학습을 위한 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;강화학습의 기반 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 04. Real-world time series data</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 04. Real-world time series data" /><published>2022-11-07T14:00:00+09:00</published><updated>2022-11-07T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/">&lt;h1 id=&quot;real-world-time-series-data&quot;&gt;Real-world time series data&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;실사용 데이터 (태양의 흑점활동) 이용하기&lt;/li&gt;
  &lt;li&gt;Conv1D, LSTM, DNN 을 결합한 모델을 활용할 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions&quot;&gt;Convolutions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, // 32개의 필터를 학습할 1D Conv
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9)

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bi-directional-lstms&quot;&gt;Bi-directional LSTMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), // LSTM 의 입력값을 재구성하는 Lambda 레이어를 없앰
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500) // epoch 늘리기
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;windowed_dataset 헬퍼 함수&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):

          ds = tf.data.Dataset.from_tensor_slices(series)
          ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
          ds = ds.flat_map(lambda w: w.batch(window_size + 1))
          ds = ds.shuffle(shuffle_buffer)
          ds = ds.map(lambda w: (w[:-1], w[-1]))

          return ds.batch(batch_size).prefetch(1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bi-directional LSTM&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), 
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), // 양방향
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;결과수치는 긍정적이나 검증세트에 예측을 플로팅 해보면 과적합이 보여 일부 파라미터에 변화를 줄 필요가 있음&lt;/li&gt;
      &lt;li&gt;MAE 로 손실을 플로팅하면 문제점을 확인할 수 있음
        &lt;ul&gt;
          &lt;li&gt;스파이크 현상은 배치 크기가 작아서 무작위 노이즈가 많기 때문임&lt;/li&gt;
          &lt;li&gt;배치사이즈를 줄이거나 늘이는 것에 따라 학습과 예측이 달라짐&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-batch-sizing&quot;&gt;More on batch sizing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;신경망을 더 빠르게 학습하도록 하는 최적화 알고리즘
    &lt;ul&gt;
      &lt;li&gt;머신러닝 : 잘 작동되는 모델을 찾기 위해 많은 훈련을 거쳐야 하는 반복적인 과정
        &lt;ul&gt;
          &lt;li&gt;모델을 빠르게 훈련시키는 것이 매우 중요함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;딥러닝은 빅데이터에서 가장 잘 작동됨 -&amp;gt; 훈련이 어려움 (큰 데이터 세트에서 훈련하는 것은 매우 느린과정)&lt;/li&gt;
      &lt;li&gt;좋은 최적화 알고리즘을 찾는 것은 효율성을 향상시켜준다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;미니배치 경사 하강법
    &lt;ul&gt;
      &lt;li&gt;벡터화 : m개의 샘플에 대한 계산을 효율적으로 만들어줌. 명시적인 반복문 없이도 훈련 세트를 진행할 수 있도록 함.&lt;/li&gt;
      &lt;li&gt;훈련 샘플을 받아서 큰 벡터에 저장함
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^m]&lt;/code&gt; : shape : (n_x,m)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = [y^1, y^2, ..., y^m]&lt;/code&gt; : shape : (1,m)&lt;/li&gt;
          &lt;li&gt;하지만 m 의 수치가 크면 여전히 학습은 느리다.&lt;/li&gt;
          &lt;li&gt;예를들어 m 의 수치가 500만 이라면?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;전체 훈련 세트에 대한 경사 하강법을 구현하면 경사 하강법의 작은 한 단계를 밟기 전에 모든 훈련 세트를 처리해야 함
        &lt;ul&gt;
          &lt;li&gt;즉, 경사하강법의 다음 단계를 밟기 전에 500만 개의 전체 훈련 샘플을 처리해야 함&lt;/li&gt;
          &lt;li&gt;500만 개의 전체 훈련 샘플을 모두 훈련하기 전에 경사 하강법이 진행되도록 하면 더 빠른 알고리즘을 얻을 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;훈련 세트를 더 작은 훈련세트 (미니배치) 로 나눔
        &lt;ul&gt;
          &lt;li&gt;mini-batch 가 1000개의 샘플을 갖는다고 가정&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^1000 | x^1001, ..., x^2000 | ... | ... x^m]&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = X^{1},                   X^{2}, ... ,               X^{5000}&lt;/code&gt; : shape : (n_x, 1000)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = Y^{1},                   Y^{2}, ... ,               Y^{5000}&lt;/code&gt; : shape : (1, 1000)&lt;/li&gt;
          &lt;li&gt;Mini-batch t : X^{t}, Y^{t}&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;표기법 정의
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x^(i)&lt;/code&gt; : i 번째 훈련 샘플&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z^[l]&lt;/code&gt; : l 번째 신경망의 z 값&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X^{t}&lt;/code&gt; : t 번째 미니배치 X&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Batch Gradient Descent : 일반적인 경사하강법, 모든 훈련 세트를 동시에 훈련시킴, 훈련 샘플의 모든 배치를 진행시킨다는 관점&lt;/li&gt;
      &lt;li&gt;미니배치 : 전체 훈련 세트 X,Y 를 한번에 진행시키지 않고, 하나의 미니배치 X^{t}, Y^{t} 를 동시에 진행시키는 알고리즘
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for t=1 ,..., 5000 : 총 미니배치의 수 (5000개)
      // 1 step of gradient descent using X^{t}, Y^{t}
      // (as if m=1000)
      // 모든 1000개의 샘플에 대해 명시적인 반복문을 갖는 것보다 벡터화를 사용해 모든 1000개의 샘플을 동시에 진행함
      Forward prop on X^{t}
          Z^[1] = W^[1] * X^{t} + b^[1] : Vectorized Implementation (1000 examples)
          A^[1] = g^[1] * (Z^[1]) : Vectorized Implementation (1000 examples)
          ...
          A^[l] = g^[l] * (Z^[l]) : Vectorized Implementation (1000 examples)
      Compute cost J^{t} = 1/1000 * Sum (( i = 1 to l) Loss(expect(y^(i)), y^(i)))
          + 정규화 항
      Backprop to compute Gradients cost J^(t) using X^{t}, Y^{t}
          W^[l] = W^[l] - adW^[l], b^[l] = b^[l] - adb^[l]
      ...
      1 epoch : pass through training set (5000 개의 경사하강단계)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm-notebook&quot;&gt;Convolutions with LSTM notebook&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm&quot;&gt;Convolutions with LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;real-data---sunspots&quot;&gt;Real data - sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;소스 내려받기 : 케글에서 내려받거나, 이번학습을 위한 데이터 제공 저장소를 사용 (후자)&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv -O /tmp/sunspots.csv&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CSV 읽기&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      import csv
      time_step = []
      sunspots = []

      with open(&apos;/tmp/sunspots.csv&apos;) as csvfile:
          reader = csv.reader(csvfile, delimiter=&apos;,&apos;)
          next(reader)
          for row in reader :
              sunspots.append(float(row[2]))
              time_step.append(int(row[0]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;numpy 배열로 전환&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      series = np.array(sunspots) // numpy 에 항목을 추가할 때마다 목록을 복제하는데,
      time = np.array(time_step) // 메모리 관리 과정이 많이 진행되기 때문에 데이터 양이 많으면 느려질 수 있음
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;시계열을 훈련 및 검증 데이터 세트로 분할&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      split_time = 1000
      time_train = time[:split_time]
      x_train = series[:split_time]
      time_valid = time[split_time:]
      x_valid = series[split_time:]

      window_size = 20
      batch_size = 32
      shuffle_buffer_size = 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이전 windowed_dataset 함수와 동일 코드를 사용&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
          dataset = tf.data.Dataset.from_tensor_slices(series)
          dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
          dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
          dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
          dataset = dataset.batch(batch_size).prefetch(1)
          return dataset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-and-tune-the-model&quot;&gt;Train and tune the model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;이전 수업에서 배웠던 모델로 예측하여 플로팅하면, 결과는 괜찮아보이나 MAE 가 매우 큼&lt;/li&gt;
  &lt;li&gt;이는 이전 window_size 가 20 (여기서는 약 2년이 안되는 시간) 이나, 사실 흑점 데이터의 주기는 11년 혹은 22년으로 추정됨&lt;/li&gt;
  &lt;li&gt;window_size 를 11년에 해당하는 132로 두고 다시 훈련을 하면 차트는 더 잘 나오나 MAE는 더 커짐
    &lt;ul&gt;
      &lt;li&gt;데이터를 되돌아보면 11년 주기의 계절성을 갖지만 창 안에 계절 전체가 있어야 할 필요는 없음&lt;/li&gt;
      &lt;li&gt;플롯을 확대해보면 전형적인 시계열 형태 데이터임&lt;/li&gt;
      &lt;li&gt;나중에 오는 값이 앞선 값과 연관이 있지만 노이즈가 많음&lt;/li&gt;
      &lt;li&gt;그래서 훈련 시에 창 크기가 클 필요는 없을 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;data 의 분할을 1000을 훈련, 2500을 검증으로 설정하였는데 이는 좋지 못한 분할임
    &lt;ul&gt;
      &lt;li&gt;3500과 500으로 지정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망 설계와 파라미터의 크기 변경
    &lt;ul&gt;
      &lt;li&gt;10, 10, 1 레이어 를 30, 15, 1 로 값을 바꿔서 훈련 (입력 Shape 값이 30)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예측
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.predict(series[3205:3235][np.newaxis])&lt;/code&gt; : 7.077 개의 흑점 예상 (실 데이터 8.7 개)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;설정 변경 : MAE 13.7&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  split_time = 3000
  window_size = 60
	
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(20, input_shape=[window_size], activation=&quot;relu&quot;),
      tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
      tf.keras.layers.Dense(1)
  ])
	
  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;8.13 예측 (실제값 8.7)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots-notebooks-lab-2--lab-3&quot;&gt;Sunspots notebooks (Lab 2 &amp;amp; Lab 3)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 으로 대체&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots&quot;&gt;Sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;입력 창의 크기를 60으로 함&lt;/li&gt;
  &lt;li&gt;DNN 을 Dense 20, 10, 1 로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;combining-our-tools-for-analysis&quot;&gt;Combining our tools for analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      window_size = 60
      batch_size = 64
      train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=&quot;casual&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(30, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 400)
      ])

      lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch / 20))
      optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-8, momentum=0.9)
      model.compile(loss=tf.keras.losses.Huber(), optimizer = optimizer, metrics=[&quot;mae&quot;])
      history = model.fit(train-set, epochs=100, callbacks=[lr_schedule])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;batch_size 의 변경 (256) : loss 에 노이즈가 생길 경우 고려해볼 파라미터&lt;/li&gt;
  &lt;li&gt;하이퍼파라미터를 다양하게 실험해 봐야 함&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Real-world time series data</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 03. Recurrent Neural Networks for Time Series</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 03. Recurrent Neural Networks for Time Series" /><published>2022-10-25T14:00:00+09:00</published><updated>2022-10-25T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series/">&lt;h1 id=&quot;recurrent-neural-networks-for-time-series&quot;&gt;Recurrent Neural Networks for Time Series&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Recurrent Neural Networks 와 Long Short Term Memory Networks 는 시계열 데이터의 예측과 분류에 매우 유용함&lt;/li&gt;
  &lt;li&gt;Lambda Layer : 신경망 내 임의의 코드를 레이어로 활용할 수 있음 (전처리와 후처리)
    &lt;ul&gt;
      &lt;li&gt;명시적인 전처리 단계로 데이터를 스케일링한 다음 신경망에 넣는 게 아니라 Lambda 레이어를 사용할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conceptual-overview&quot;&gt;Conceptual overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN : 순환 레이어를 포함한 신경망
    &lt;ul&gt;
      &lt;li&gt;시퀀스 입력값을 순차적으로 처리하도록 설계&lt;/li&gt;
      &lt;li&gt;입력값의 형태 : 배치 사이즈, 타임스탬프 (윈도우사이즈), 컬럼디멘전 (다변량) = 3차원
        &lt;ul&gt;
          &lt;li&gt;지금까지 사용한 입력값 형태 : 배치 사이즈, 입력값 특징 수 (윈도우 사이즈)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN Cell
    &lt;ul&gt;
      &lt;li&gt;겉으로 보기에는 셀이 많은 것 같지만, 셀은 하나 뿐이고 이를 반복적으로 사용하여 출력값을 산출&lt;/li&gt;
      &lt;li&gt;입력값이 2개 (X 값과 상태벡터 H 값) - 상태벡터값을 이용해 이전 입력값의 잔존 데이터를 전달받음&lt;/li&gt;
      &lt;li&gt;입력차원 (예: 타임스탬프가 30개) 만큼 반복&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rnn-notebook&quot;&gt;RNN Notebook&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shape-of-the-inputs-to-the-rnn&quot;&gt;Shape of the inputs to the RNN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터의 형태, 데이터를 분할한 배치
    &lt;ul&gt;
      &lt;li&gt;예시
        &lt;ul&gt;
          &lt;li&gt;Window size 가 30 : 시간 단계가 30&lt;/li&gt;
          &lt;li&gt;4개로 일괄 처리 : 배치값 4&lt;/li&gt;
          &lt;li&gt;입력 형태는 4 * 30 * 1&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;셀의 관점
        &lt;ul&gt;
          &lt;li&gt;하나의 셀은 고정된 시간 단계에서 (Batch Size : 4 * 1) 의 입력을 받음&lt;/li&gt;
          &lt;li&gt;레이어 내 메모리셀이 3개의 뉴런으로 구성된다면&lt;/li&gt;
          &lt;li&gt;출력값 행렬은 4 * 3&lt;/li&gt;
          &lt;li&gt;출력 형태는 4(Batch Size) * 30(Window Size) * 3(Unit Size)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;단순 RNN 에서의 상태 출력값 H 는 출력값 행렬 Y 와 동일함&lt;/li&gt;
      &lt;li&gt;일부 경우에는 시퀀스를 입력하되, 출력값의 경우 배치 내 각 인스턴스에 대한 단일 벡터를 얻고 싶은 경우가 있음
        &lt;ul&gt;
          &lt;li&gt;마지막 (마지막 시퀀스 스텝-Window) 을 제외하고 모든 출력값을 무시&lt;/li&gt;
          &lt;li&gt;시퀀스 출력값을 도출하려면 레이어를 생성할 때 return_sequences 를 True 로 지정해야 함
            &lt;ul&gt;
              &lt;li&gt;하나의 RNN 레이어를 다른 레이어 위에 스태킹 할때 이 작업이 반드시 필요&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outputting-a-sequence&quot;&gt;Outputting a sequence&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;적층 예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  model = tf.keras.models.Sequential([
	tf.keras.layers.SimpleRNN(40, return_sequences=True, input_shape = [None,1]),
	tf.keras.layers.SimpleRNN(40),
	tf.keras.layers.Dense(1),
  ])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;타 RNN 레이어에 입력으로 들어가야 하는 RNN 레이어에 return_sequences 를 True 로 설정&lt;/li&gt;
      &lt;li&gt;Dense 레이어에 입력으로 들어가야 하는 RNN 레이어는 마지막 시퀀스 단계의 결과값만을 출력&lt;/li&gt;
      &lt;li&gt;input_shape (배치 사이즈) 를 설정하지 않음 : 어떤 크기든 상관이 없으니 정의할 필요가 없음&lt;/li&gt;
      &lt;li&gt;Timestamp 값을 None 으로 설정 : 시퀀스 길이와 관계 없이 입력값을 받음&lt;/li&gt;
      &lt;li&gt;마지막 차원이 1로 되어있는 이유 : 일변량 시계열을 다루기 때문&lt;/li&gt;
      &lt;li&gt;두번째 층 RNN 레이어에 return_sequences 값을 True 로 설정할 경우
        &lt;ul&gt;
          &lt;li&gt;시퀀스 값이 출력됨&lt;/li&gt;
          &lt;li&gt;Keras 는 각 시간 단계별로 동일한 Dense 레이어를 독립적으로 활용함&lt;/li&gt;
          &lt;li&gt;입력값이 시퀀스이고 출력값 또한 시퀀스일 경우 : 시퀀스 to 시퀀스 RNN&lt;/li&gt;
          &lt;li&gt;차원의 값은 RNN 레이어의 유닛 값에 따라 변동될 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lambda-layers&quot;&gt;Lambda layers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  model = tf.keras.models.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),
                        input_shape=[window_size]),
    tf.keras.layers.SimpleRNN(40, return_sequences=True),
    tf.keras.layers.SimpleRNN(40),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;첫 lambda 레이어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.expand_dims(x, axis=-1)&lt;/code&gt; : 기존 window 생성 function 을 그대로 활용하기 위해 차원을 하나 늘림 (2차원-&amp;gt;3차원)&lt;/li&gt;
      &lt;li&gt;마지막 lambda 레이어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lambda x: x * 100.0&lt;/code&gt; : RNN 의 기본 활성함수 tanh 의 출력값 -1 ~ 1 &amp;gt; 시계열 값은 10개 단위로 구성되고, 비슷한 값으로 출력값을 올리면 학습에 도움이 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adjusting-the-learning-rate-dynamically&quot;&gt;Adjusting the learning rate dynamically&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  train_set = windowed_dataset(x_train, window_size, batch_size=128,
      shuffle_buffer=shuffle_buffer_size)

  model = tf.keras.models.Sequential([
          tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1), input_shape=[None]),
          tf.keras.layers.SimpleRNN(40, return_sequences=True),
          tf.keras.layers.SimpleRNN(40),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 100.0)
      ])

  lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))

  optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)

  model.compile(loss=tf.kears.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

  history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;callback 함수를 활용, epoch 진행 별로 학습률을 약간 변경&lt;/li&gt;
      &lt;li&gt;Huber 손실함수 : 이상치에 덜 민감하게 반응하는 손실함수, 데이터에 노이즈가 많이 섞여있을 때 시도해볼만 함
        &lt;ul&gt;
          &lt;li&gt;squared error loss 보다 이상치에 덜 민감함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lstm&quot;&gt;LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN
    &lt;ul&gt;
      &lt;li&gt;X 가 셀에 투입되면 Y 결과값과 H 상태벡터가 출력되고, 이는 다음 셀에 영향을 줌&lt;/li&gt;
      &lt;li&gt;Step 이 진행되면서 초기 H 상태벡터의 영향도는 점점 작아짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM
    &lt;ul&gt;
      &lt;li&gt;전체 훈련 기간 동안 상태를 유지해주는 셀 상태를 추가함&lt;/li&gt;
      &lt;li&gt;상태 값이 셀 간에 이동을 하고 Step 사이를 이동하면서 더 잘 유지될 수 있게함 - 앞 단계에 있던 데이터가 전체 추정치에 더 큰 영향을 줌&lt;/li&gt;
      &lt;li&gt;상태는 양방향으로 움직일 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;coding-lstms&quot;&gt;Coding LSTMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.backend.clear_session()&lt;/code&gt; : 내부 변수를 초기화. 이후 버전에 영향을 주지 않고 여러 모델을 시험해 볼 수 있음&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))&lt;/code&gt; : 32개 셀의 단일 LSTM 레이어 추가. 예측에 미치는 영향을 파악할 수 있도록 양방향으로 만듦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))&lt;/code&gt; : LSTM 레이어를 한층 더 쌓음, retrun_sequences 를 True 로 설정해야만 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Recurrent Neural Networks for Time Series</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 02. Deep Neural Networks for Time Series</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 02. Deep Neural Networks for Time Series" /><published>2022-10-21T14:00:00+09:00</published><updated>2022-10-21T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series/">&lt;h1 id=&quot;deep_neural_networks_for_time_series&quot;&gt;Deep_Neural_Networks_for_Time_Series&lt;/h1&gt;

&lt;h2 id=&quot;preparing-features-and-labels&quot;&gt;Preparing features and labels&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;시계열에서의 Input 값과 Label 값
    &lt;ul&gt;
      &lt;li&gt;Features : 입력되는 값
        &lt;ul&gt;
          &lt;li&gt;예 : 이전 30개의 시점의 값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Labels : 예측할 값 (정답)
        &lt;ul&gt;
          &lt;li&gt;예 : 미래 시점의 값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)
	dataset = tf.data.Dataset.range(10)

	# Window the data but only take those with the specified size
	dataset = dataset.window(5, shift=1, drop_remainder=True)

	# Flatten the windows by putting its elements in a single batch
	dataset = dataset.flat_map(lambda window: window.batch(5))

	# Create tuples with features (first four elements of the window) and labels (last element)
	dataset = dataset.map(lambda window: (window[:-1], window[-1]))

	# Shuffle the windows
	dataset = dataset.shuffle(buffer_size=10)

	# Create batches of windows ( 한번에 여러 데이터 처리를 위함 )
	dataset = dataset.batch(2).prefetch(1)

	# Print the results
	for x,y in dataset:
	  print(&quot;x = &quot;, x.numpy())
	  print(&quot;y = &quot;, y.numpy())
	  print()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;feeding-windowed-dataset-into-neural-network&quot;&gt;Feeding windowed dataset into neural network&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
	    &quot;&quot;&quot;Generates dataset windows

	    Args:
	      series (array of float) - contains the values of the time series
	      window_size (int) - the number of time steps to include in the feature
	      batch_size (int) - the batch size
	      shuffle_buffer(int) - buffer size to use for the shuffle method

	    Returns:
	      dataset (TF Dataset) - TF Dataset containing time windows
	    &quot;&quot;&quot;
	  
	    # Generate a TF Dataset from the series values
	    dataset = tf.data.Dataset.from_tensor_slices(series)
	    
	    # Window the data but only take those with the specified size
	    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
	    
	    # Flatten the windows by putting its elements in a single batch
	    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))

	    # Create tuples with features and labels 
	    dataset = dataset.map(lambda window: (window[:-1], window[-1]))

	    # Shuffle the windows 
	    dataset = dataset.shuffle(shuffle_buffer) // shuffle buffer 의 크기만큼 이동하면서 buffer 내에서 무작위로 하나씩 선택 (선택속도증가)
	    
	    # Create batches of windows
	    dataset = dataset.batch(batch_size).prefetch(1)
	    
	    return dataset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;single-layer-neural-network&quot;&gt;Single layer neural network&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	# Generate the dataset windows
	dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

	# Build the single layer neural network
	l0 = tf.keras.layers.Dense(1, input_shape=[window_size])
	model = tf.keras.models.Sequential([l0])

	# Print the initial layer weights
	print(&quot;Layer weights: \n {} \n&quot;.format(l0.get_weights()))

	# Print the model summary
	model.summary()

	# Set the training parameters
	model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))

	# Train the model
	model.fit(dataset,epochs=100)

	# Print the layer weights
	print(&quot;Layer weights {}&quot;.format(l0.get_weights()))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;machine-learning-on-time-windows&quot;&gt;Machine learning on time windows&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-single-layer-neural-network&quot;&gt;More on single layer neural network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-neural-network-training-tuning-and-prediction&quot;&gt;Deep neural network training, tuning and prediction&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
	dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
	model = tf.keras.models.Sequential([
		tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
		tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
		tf.keras.layers.Dense(1)
	])

	model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
	model.fit(dataset, epochs=100, verbose=0)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;우리가 선택한 학습률이 아니라 최적의 학습률을 선택할 수 있다면 더 좋은 결과가 나올 것
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;callback 을 활용하는 기법&lt;/p&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;		
      dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
      model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
          tf.keras.layers.Dense(1)
      ])
		
      // 각 epoch 종료 시마다 callback 에서 호출, epoch 숫자값을 기준으로 학습률을 값으로 변경
      // 
      lr_schedule = tf.keras.callbacks.LearningRateScheduler(
          lambda epoch: 1e-8 * 10**(epoch / 20))
		
      optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)
		
      model.compile(loss=&quot;mse&quot;, optimizer=optimizer)
			
      history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])
		
      // 트레이닝을 마치고 나면 epoch 당 학습률에 대한 epoch 당 오차를 플로팅
      // x 축은 learning rate, y축은 epoch 의 손실
      lrs = 1e-8 (10 ** (np.arange(100) / 20))
      plt.semilogx(lrs, history.history[&quot;loss&quot;])
      plt.axis([1e-8, 1e-3, 0, 300])
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;위에서 구한 learning_rate (7e-6) 로 재훈련&lt;/p&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
      window_size = 30
      dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
      model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
          tf.keras.layers.Dense(1)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_rate=7e-6, momentum=0.9)
      model.compile(loss=&quot;mse&quot;, optimizer=optimizer)
      model.fit(dataset, epochs=500)

      // 훈련 도중 산출한 손실 플로팅 코드
      loss = history.history[&apos;loss&apos;]
      epochs = range(len(loss))
      plt.plot(epochs, loss, &apos;b&apos;, label=&apos;Training Loss&apos;)
      plt.show()

      // 초기손실 (왜곡값) 자르기
      loss = history.history[&apos;loss&apos;]
      epochs = range(10, len(loss))
      plot_loss = loss[10:]
      print(plot_loss)
      plt.plot(epochs, plot_loss, &apos;b&apos;, label=&apos;Training Loss&apos;)
      plt.show()

      // mean absolute error 값 확인
      tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-neural-network&quot;&gt;Deep neural network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 참조&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Deep_Neural_Networks_for_Time_Series</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 01. Sequences and Prediction</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 01. Sequences and Prediction" /><published>2022-10-20T14:00:00+09:00</published><updated>2022-10-20T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;순차 시계열 데이터 (Sequential time series data)
    &lt;ul&gt;
      &lt;li&gt;값이 시간에 따라 변하는 것&lt;/li&gt;
      &lt;li&gt;예
        &lt;ul&gt;
          &lt;li&gt;주식거래의 종가&lt;/li&gt;
          &lt;li&gt;특정 일의 기온&lt;/li&gt;
          &lt;li&gt;웹사이트의 방문자 수&lt;/li&gt;
          &lt;li&gt;스프레드시트에 기록할 수 있는 데이터&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다룰 내용
    &lt;ul&gt;
      &lt;li&gt;미래 시점의 값 예측을 위한 다양한 방법론&lt;/li&gt;
      &lt;li&gt;위의 내용의 구현법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-a-conversation-with-andrew-ng&quot;&gt;Introduction, A conversation with Andrew Ng&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터의 합성 시퀀스를 만들기&lt;/li&gt;
  &lt;li&gt;데이터 시계열에서 공통 속성을 살펴보기
    &lt;ul&gt;
      &lt;li&gt;계절성 : 날씨의 경우 6월이 1월보다 따듯하고, 11월은 10월보다 습할 수 있음&lt;/li&gt;
      &lt;li&gt;경향성 : 주식의 종가처럼 시간이 가면서 상승, 혹은 하강&lt;/li&gt;
      &lt;li&gt;노이즈 : 무작위 요소&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;구현 : 흑점 활동 모니터
    &lt;ul&gt;
      &lt;li&gt;흑점 활동
        &lt;ul&gt;
          &lt;li&gt;11년, 혹은 22년의 주기 (계절성)&lt;/li&gt;
          &lt;li&gt;노이즈&lt;/li&gt;
          &lt;li&gt;250년 전부터 측정해온 데이터 활용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;sequences-and-prediction&quot;&gt;Sequences and Prediction&lt;/h1&gt;

&lt;h2 id=&quot;time-series-examples&quot;&gt;Time series examples&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 (Time Series) 이란 무엇인가?
    &lt;ul&gt;
      &lt;li&gt;오랜 시간에 걸쳐 균등한 간격으로 순서가 지정된 시퀀스로 나타나는 값&lt;/li&gt;
      &lt;li&gt;다변량 시계열 : 각 시점에서 복수 개의 값이 표시된 경우
        &lt;ul&gt;
          &lt;li&gt;데이터에 추가값을 더하여 상관관계를 파악할 수 있음
            &lt;ul&gt;
              &lt;li&gt;시간의 흐름에 따른 기온과 이산화탄소 배출량의 상관관계&lt;/li&gt;
              &lt;li&gt;자동차의 이동경로 (동시간의 간격 (속도), 위도, 경도 등)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;machine-learning-applied-to-time-series&quot;&gt;Machine learning applied to time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;머신러닝으로 가능한 시계열 관련 작업
    &lt;ul&gt;
      &lt;li&gt;데이터를 기반으로 한 예측 작업&lt;/li&gt;
      &lt;li&gt;이미 가지고 있는 데이터보다 이전 시점의 데이터를 예측&lt;/li&gt;
      &lt;li&gt;실질적으로 존재하지 않는 데이터의 시점의 데이터를 예측&lt;/li&gt;
      &lt;li&gt;변칙 감지에 활용&lt;/li&gt;
      &lt;li&gt;패턴의 발견 (예: 음파를 인식)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;common-patterns-in-time-series&quot;&gt;Common patterns in time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;흔하게 나타나는 시계열 패턴 (눈으로 보고 인식하는데 유용)
    &lt;ul&gt;
      &lt;li&gt;추세 : 특정한 방향으로 움직이는 경우 (예: 무어의 법칙)&lt;/li&gt;
      &lt;li&gt;계절성 : 패턴이 예측 가능한 간격으로 반복됨 (예: 쇼핑사이트 방문자수 (주말에 올라감))&lt;/li&gt;
      &lt;li&gt;노이즈 : 전혀 예측이 불가능한 임의의 값들로 구성된 세트&lt;/li&gt;
      &lt;li&gt;자기상관관계 : 시간의 흐름에 따라 과거 혹은 현재의 값이 미래에 영향을 주는 것&lt;/li&gt;
      &lt;li&gt;복합적으로 나타나는 경우&lt;/li&gt;
      &lt;li&gt;비정상시계열 : 명확한 패턴을 보이다가 큰 이벤트로 인해 패턴이 깨지는 경우
        &lt;ul&gt;
          &lt;li&gt;특정 경향성을 보이는 경우에 특정 구간만 학습&lt;/li&gt;
          &lt;li&gt;하지만 현실의 데이터는 단순하지 않음 (패턴이 깨지며 경향이 나타났으나 다시 과거 패턴으로 회귀)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-time-series&quot;&gt;Introduction to time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Colab 파일&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-validation-and-test-sets&quot;&gt;Train, validation and test sets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;예측 모델의 성능 측정
    &lt;ul&gt;
      &lt;li&gt;Fixed Partitioning (고정 파티셔닝) : 시계열을 훈련기간, 검증 기간, 테스트 기간으로 분할
        &lt;ul&gt;
          &lt;li&gt;시계열이 계절성이 있는 경우 각각의 기간에 계절 전체를 포함하고 싶음&lt;/li&gt;
          &lt;li&gt;시간이 지남에 따라 검증 기간의 데이터를 훈련에 사용, 테스트 기간의 데이터로 검증을 하고, 새로운 테스트 기간으로 테스트를 함&lt;/li&gt;
          &lt;li&gt;테스트 기간은 현재 데이터에 가장 영향을 많이 줄 수 있는 데이터. 따라서 테스트 세트를 포기하는 경우가 흔함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;롤 포워드 파티셔닝
        &lt;ul&gt;
          &lt;li&gt;짧은 훈련기간을 가지고, 점점 증가시켜 (한번에 하루, 한번에 한 주) 반복수행&lt;/li&gt;
          &lt;li&gt;검증기간에는 다음 달이나 다음 주를 예측&lt;/li&gt;
          &lt;li&gt;고정 파티셔닝을 여러 번 시행하고 모델을 계속 다듬는 과정&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;metrics-for-evaluating-performance&quot;&gt;Metrics for evaluating performance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;성능을 계산할 지표
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      errors = forcasts - actual // 오차산출
      mse = np.square(errors).mean() // 평균제곱오차 :  가장 일반적인 지표 (음수제거를 하여 에러간 상쇄가 없도록 함)
      rmse = np.sqrt(mse) // 평균제곱근오차 : 원래 에러 규모와 동일한 규모를 만들기 위해 제곱근 계
      mae = np.abs(errors).mean() // 평균절대오(편)차 :  제곱 대신 절대값을 사용
      mape = np.abs(errors / x valid).mean() // 평균절대백분율오차 : 절대 오차와 절대값의 평균 비율 (값 대비 오차의 크기를 파악)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;작은 오차보다 큰 오차가 생겼을 경우 비용이 훨씬 크다면 MSE&lt;/li&gt;
      &lt;li&gt;손익이 단순 오차의 크기에 비례한다면 MAE
        &lt;ul&gt;
          &lt;li&gt;케라스에서의 구현
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keras.metrics.mean_absolute_error(x_valid, native_forecast).numpy()&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;moving-average-and-differencing&quot;&gt;Moving average and differencing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;이동 평균을 계산 : 일반적이고 간단한 예측 방법
    &lt;ul&gt;
      &lt;li&gt;노이즈를 제거하고, 원본 시계열을 대략적으로 유추하는 곡선이 도출&lt;/li&gt;
      &lt;li&gt;추세나 계절성을 예측하지는 않음 : 현재 시점에서 미래를 예측하고자 하는 기간 이후에는 단순 예측보다 결과가 저조할 수 있음&lt;/li&gt;
      &lt;li&gt;차분은 이를 피하는 방법 중 하나 : 시계열에서 추세와 계절성을 제거
        &lt;ul&gt;
          &lt;li&gt;즉, 시계열 자체를 연구하는게 아니라 T 시점의 값과 이전 기간의 값 사이의 차이를 연구&lt;/li&gt;
          &lt;li&gt;차분에 이동평균선을 예측하면 이는 차분에 대한 예측일 뿐이고, 원본 시계열에 대한 것은 아님
            &lt;ul&gt;
              &lt;li&gt;뺀 값 (이전 기간의 값) 을 다시 더해줘야 함&lt;/li&gt;
              &lt;li&gt;하지만 이전의 값을 더해줄 때 노이즈도 같이 생기게 됨. &amp;gt; 이동 평균을 이용하여 과거의 노이즈를 제거&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trailing-versus-centered-windows&quot;&gt;Trailing versus centered windows&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Trailing Window (현재 값의 이동 평균을 산출할때)
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t1000 = (t970 + t971 + ... + t999) / 30&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Centered Window (과거 값의 이동 평균을 산출할때) : 정확도가 Trailing Window 보다 높음
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t635 = (t630 + t631 + ... + t640) / 11&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forecasting&quot;&gt;Forecasting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;week-1-working-with-time-series&quot;&gt;Week 1: Working with time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Introduction</summary></entry><entry><title type="html">Jenkins Pipeline - Declarative and IaC approaches for DevOps</title><link href="https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps/" rel="alternate" type="text/html" title="Jenkins Pipeline - Declarative and IaC approaches for DevOps" /><published>2022-09-30T14:00:00+09:00</published><updated>2022-09-30T14:00:00+09:00</updated><id>https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps</id><content type="html" xml:base="https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps/">&lt;h1 id=&quot;후기&quot;&gt;후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_JenkinsPipeline-DeclarativeandIaCapproachesforDevOps_course_info.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;해당 강좌는 영상플레이 시간 2시간 (하지만 영어 강좌여서 3시간 이상) 소요 되는 강좌입니다.&lt;/li&gt;
  &lt;li&gt;장점은 부담되는 양의 자료 (책, 강의 등) 를 보기 전에 짧은 시간 훑어보기에 좋은 강의라는 점입니다.&lt;/li&gt;
  &lt;li&gt;대부분 Declarative Script 로 Jenkinsfile 을 작성하여 활용할 것으로 생각하는 바, 기초에 좋은 강의일듯 싶습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;강좌&quot;&gt;강좌&lt;/h1&gt;

&lt;h2 id=&quot;파이프라인이란-무엇인가&quot;&gt;파이프라인이란 무엇인가?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;정의와 기능
    &lt;ul&gt;
      &lt;li&gt;SCM(Source Code Management) 의 Continuous Delivery 절차를 위한 플러그인 집합&lt;/li&gt;
      &lt;li&gt;제품 개발 라이프사이클 (Submitting Code -&amp;gt; Testing -&amp;gt; Staging -&amp;gt; Deployment …) 에 연관&lt;/li&gt;
      &lt;li&gt;각 단계의 성공 / 실패 여부 제공&lt;/li&gt;
      &lt;li&gt;다양한 타 환경에서의 운영 지원&lt;/li&gt;
      &lt;li&gt;저장소 단계에서 실 환경 배포까지의 자동화&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;파이프라인 작성법
    &lt;ul&gt;
      &lt;li&gt;Pipeline script 를 Jenkins UI 에서 작성&lt;/li&gt;
      &lt;li&gt;Jenkins file 을 통한 작성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;파이프라인 언어
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;declarative&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;scripted&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;jenkinsfile 의 장점
    &lt;ul&gt;
      &lt;li&gt;IaC (Infrastructure as Code)
        &lt;ul&gt;
          &lt;li&gt;application code 와 마찬가지로 취급되어 저장소에 committed 됨&lt;/li&gt;
          &lt;li&gt;저장소의 이점을 누리며, 동시에 어떤 구조로 되어있는지 구성원들이 시각적으로 확인할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;파이프라인-스크립트의-형태와-전역변수&quot;&gt;파이프라인 스크립트의 형태와 전역변수&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;젠킨스 서버 구동
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;java -jar jenkins.war httpPort=8080&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;브라우저에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:8080&lt;/code&gt; 으로 접속&lt;/li&gt;
      &lt;li&gt;기본 계정 로그인 : admin / admin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;new items
    &lt;ul&gt;
      &lt;li&gt;네이밍&lt;/li&gt;
      &lt;li&gt;Pipeline 생성&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pipeline 섹션으로 이동&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Jenkinsfile (Declarative Pipeline)
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  pipeline {
      agent any // Execute this Pipeline or any of its stages, on any available agent.
      stages {
          // stage : conceptually distinct subset or tasks performed throughout the entire pipeline
          stage(&apos;Build&apos;) { // Defines the &quot;Build&quot; stage. (Keyword is flexible)
              // Steps represents a single task.
              // It tells Jenkins what to do at a particular point in time of a particular step in the process.
              steps {
                  //  Perform some steps related to the &quot;Build&quot; stage.
              }
          }
          stage(&apos;Test&apos;) { // Defines the &quot;Test&quot; stage.
              steps {
                  // Perform some steps related to the &apos;&quot;Test&quot; stage.
              }
          }
          stage(&apos;Deploy&apos;) { Defines the &quot;Deploy&quot; stage. 
              steps {
                  // Perform some steps related to the &quot;Deploy&quot; stage.
              }
          }
      }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Environment Variables (Global Variables)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8080/pipeline-syntax/globals#env&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;첫-파이프라인-스크립트-작성과-환경변수-삽입&quot;&gt;첫 파이프라인 스크립트 작성과 환경변수 삽입&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;pipeline 섹션에 코드 작성&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          stages {
              stage(&apos;stage 1&apos;){
                  steps {
                      echo &quot;hello world&quot;
                      echo BUILD_ID // Global Variable
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;Build Now&lt;/li&gt;
      &lt;li&gt;Console Output 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;environment 변수 삽입&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;environment 변수는 최상단에 위치할 수도, stage 안에 존재할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          environment {
              mainenv = &apos;dev&apos;
          }
          stages {
              stage(&apos;stage 1&apos;){
                  steps {
                      echo &quot;hello world&quot;
                      echo BUILD_ID
                      echo &quot;&quot;&quot;mainenv = ${mainenv}&quot;&quot;&quot; // enject template, literals, variables
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          environment {
              mainenv = &apos;dev&apos;
          }
          stages {
              stage(&apos;stage 1&apos;){
                  environment{
                      subenv = &quot;prod&quot;
                  }
                  steps {
                      echo &quot;&quot;&quot;inside stage 1: mainenv = ${mainenv}&quot;&quot;&quot; // enject template, literals, variables
                      echo &quot;&quot;&quot;inside stage 1: subenv = ${subenv}&quot;&quot;&quot;
                  }
              }
              stage(&apos;stage 2&apos;){
                  steps {
                      echo &quot;&quot;&quot;inside stage 2: mainenv = ${mainenv}&quot;&quot;&quot;
                      echo &quot;&quot;&quot;inside stage 2: subenv = ${subenv}&quot;&quot;&quot; // causing error, because of scope.
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;실제-github-저장소를-사용한-파이프라인-스크립트-작성과-build-steps&quot;&gt;실제 Github 저장소를 사용한 파이프라인 스크립트 작성과 build steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;대시보드에서 new item 선택
    &lt;ul&gt;
      &lt;li&gt;Pipeline 생성
        &lt;ul&gt;
          &lt;li&gt;우측 드롭다운 메뉴에서 샘플 코드를 불러올 수 있음&lt;/li&gt;
          &lt;li&gt;Pipeline Syntax 에서 Snippet Generator 활용하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most common MVN build phases&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: left&quot;&gt;Build Phase&lt;/th&gt;
          &lt;th style=&quot;text-align: left&quot;&gt;Description&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;validate&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Validates that the project is correct and all necessary information is available. This also makes sure the dependencies are downloaded.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;compile&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Compiles the source code of the project.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;test&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Runs the tests against the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;package&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Packs the compiled code in its distributable format. such as a JAR.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;install&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Install the package into the local repository, for use as a dependency in other projects locally.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;deploy&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Copies the final package to the remote repository for sharing with other developers and projects.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;작성한 소스코드&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
      	agent any
      	stages {
          	stage(&apos;Setup&apos;){
              	steps {
                   	// this will delete folder whatever the OS platform is.
                  	dir(&apos;jenkins-spring&apos;){
                      	deleteDir()               
                  	}
              	}
          	}
          	stage(&apos;Build&apos;){
              	steps {
                  	// sh : for Linux
                  	// for Windows (copy github source)
                  	bat &apos;git clone https://github.com/rudihinds/jenkins-spring.git&apos;
                  	bat &apos;mvn clean -f jenkins-spring&apos;
              	}
          	}
          	stage(&apos;Test&apos;){
              	steps {
                  	bat &apos;mvn clean test -f jenkins-spring&apos;
              	}
          	}
          	stage(&apos;Deploy&apos;){
              	steps {
                  	bat &apos;mvn clean package -f jenkins-spring&apos;
              	}
          	}
      	}
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;jenkinsfile-을-이용하여-scm-에-파이프라인-연결&quot;&gt;Jenkinsfile 을 이용하여 SCM 에 파이프라인 연결&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;SCM 을 통해 어떻게 파이프라인을 가져올 수 있는지?&lt;/li&gt;
  &lt;li&gt;복잡한 Microarchitecture 구조가 아니라면 보편적으로 Jenkinsfile 은 프로젝트의 root 디렉토리에 있음&lt;/li&gt;
  &lt;li&gt;Github 에 있는 Jenkinsfile 에 작성된 declarative script 와 지금까지 작성한 스크립트의 차이점
    &lt;ul&gt;
      &lt;li&gt;clone 스테이지가 없음 : Github의 Jenkinsfile을 사용한다는 것은 이미 Jenkins 에게 SCM 에서 소스코드를 가져오라고 지시한 것임&lt;/li&gt;
      &lt;li&gt;setup 스테이지가 없음 : 해당 프로세스는 이미 통합되어 있음 (Jenkins가 핸들링)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실습
    &lt;ul&gt;
      &lt;li&gt;새 파이프라인을 만들고, 파이프라인의 정의를 Pipeline script from SCM 으로 설정
        &lt;ul&gt;
          &lt;li&gt;SCM 종류 Git으로 설정&lt;/li&gt;
          &lt;li&gt;Repository URL 설정&lt;/li&gt;
          &lt;li&gt;브랜치 설정&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Build Now
        &lt;ul&gt;
          &lt;li&gt;Declarative: Checkout SCM 스테이지가 자동생성된 것을 확인할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Jenkins" /><category term="Coursera" /><category term="Rudi Hinds" /><category term="Jenkins" /><category term="Declarative and IaC approaches for DevOps" /><category term="젠킨스" /><category term="CI/CD" /><summary type="html">후기</summary></entry></feed>