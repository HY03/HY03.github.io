<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://bluesplatter.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bluesplatter.com/" rel="alternate" type="text/html" /><updated>2023-01-26T11:42:03+09:00</updated><id>https://bluesplatter.com/feed.xml</id><title type="html">Bluesplatter</title><subtitle>전문적이지 않은 정보들, 감상, 즉흥적인 내용들</subtitle><author><name>HY03</name><email>hyunik03@gmail.com</email></author><entry><title type="html">Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020</title><link href="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/" rel="alternate" type="text/html" title="Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020" /><published>2023-01-26T11:00:00+09:00</published><updated>2023-01-26T11:00:00+09:00</updated><id>https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020</id><content type="html" xml:base="https://bluesplatter.com/wine/Textbook_Napa_Cabernet_Sauvignon_2020/">&lt;h2 id=&quot;1-구매정보&quot;&gt;1. 구매정보&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;빈티지 : 2020&lt;/li&gt;
  &lt;li&gt;가격 : 40,000 원 (할인 및 페이백 후 추정가)&lt;/li&gt;
  &lt;li&gt;구매처 : 편의점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-이미지&quot;&gt;2. 이미지&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/Textbook_Napa_Cabernet_Sauvignon.jpg&quot; alt=&quot;Textbook_Napa_Cabernet_Sauvignon.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;about:blank&quot; target=&quot;_blank&quot;&gt;링크없음&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-감상&quot;&gt;3. 감상&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;1일차
    &lt;ul&gt;
      &lt;li&gt;레드 와인&lt;/li&gt;
      &lt;li&gt;향이 강하지 않음&lt;/li&gt;
      &lt;li&gt;맛이 굉장히 가벼운 느낌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-총점&quot;&gt;3. 총점&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;3/5
    &lt;ul&gt;
      &lt;li&gt;많은 기대를 한 와인이었으나, 기대에 못미침&lt;/li&gt;
      &lt;li&gt;가볍고 경쾌한 느낌이어서 많이 마실 수 있었음&lt;/li&gt;
      &lt;li&gt;하지만 이번 와인을 계기로 난 와인보다 양주나 정종이 더 맞다는 결론을 내림.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Wine" /><category term="와인" /><summary type="html">1. 구매정보 빈티지 : 2020 가격 : 40,000 원 (할인 및 페이백 후 추정가) 구매처 : 편의점</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Sequences_TimeSeries_and_Prediction_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 분석 학습을 위한 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 01. Week 1</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 01. Week 1" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/">&lt;h2 id=&quot;lesson-1-the-k-armed-bandit-problem&quot;&gt;Lesson 1: The K-Armed Bandit Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Define reward&lt;/li&gt;
  &lt;li&gt;Understand the temporal nature of the bandit problem&lt;/li&gt;
  &lt;li&gt;Define k-armed bandit&lt;/li&gt;
  &lt;li&gt;Define action-values&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-2-what-to-learn-estimating-action-values&quot;&gt;Lesson 2: What to Learn? Estimating Action Values&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Define action-value estimation methods&lt;/li&gt;
  &lt;li&gt;Define exploration and exploitation&lt;/li&gt;
  &lt;li&gt;Select actions greedily using an action-value function&lt;/li&gt;
  &lt;li&gt;Define online learning&lt;/li&gt;
  &lt;li&gt;Understand a simple online sample-average action-value estimation method&lt;/li&gt;
  &lt;li&gt;Define the general online update equation&lt;/li&gt;
  &lt;li&gt;Understand why we might use a constant step-size in the case of non-stationarity&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-3-exploration-vs-exploitation-tradeoff&quot;&gt;Lesson 3: Exploration vs. Exploitation Tradeoff&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Define epsilon-greedy&lt;/li&gt;
  &lt;li&gt;Compare the short-term benefits of exploitation and the long-term benefits of exploration&lt;/li&gt;
  &lt;li&gt;Understand optimistic initial values&lt;/li&gt;
  &lt;li&gt;Describe the benefits of optimistic initial values for early exploration&lt;/li&gt;
  &lt;li&gt;Explain the criticisms of optimistic initial values&lt;/li&gt;
  &lt;li&gt;Describe the upper confidence bound action selection method&lt;/li&gt;
  &lt;li&gt;Define optimism in the face of uncertainty&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">Lesson 1: The K-Armed Bandit Problem</summary></entry><entry><title type="html">Fundamentals of Reinforcement Learning - 00. 강좌소개</title><link href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning - 00. 강좌소개" /><published>2022-11-18T14:00:00+09:00</published><updated>2022-11-18T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About%20this%20course</id><content type="html" xml:base="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_00_About-this-course/">&lt;h1 id=&quot;강좌에-대한-설명&quot;&gt;강좌에 대한 설명&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_FundamentalsofReinforcementLearning.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;강화학습의 기반 강좌&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Reinforcement Learning" /><category term="Coursera" /><category term="Alberta Machine Intelligence Institute" /><category term="앨버타 대학교" /><category term="강화학습" /><category term="Martha White" /><category term="Adam White" /><summary type="html">강좌에 대한 설명</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 04. Real-world time series data</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 04. Real-world time series data" /><published>2022-11-07T14:00:00+09:00</published><updated>2022-11-07T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_04_Real-world_time_series_data/">&lt;h1 id=&quot;real-world-time-series-data&quot;&gt;Real-world time series data&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;실사용 데이터 (태양의 흑점활동) 이용하기&lt;/li&gt;
  &lt;li&gt;Conv1D, LSTM, DNN 을 결합한 모델을 활용할 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions&quot;&gt;Convolutions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, // 32개의 필터를 학습할 1D Conv
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9)

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bi-directional-lstms&quot;&gt;Bi-directional LSTMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), // LSTM 의 입력값을 재구성하는 Lambda 레이어를 없앰
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500) // epoch 늘리기
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;windowed_dataset 헬퍼 함수&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):

          ds = tf.data.Dataset.from_tensor_slices(series)
          ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
          ds = ds.flat_map(lambda w: w.batch(window_size + 1))
          ds = ds.shuffle(shuffle_buffer)
          ds = ds.map(lambda w: (w[:-1], w[-1]))

          return ds.batch(batch_size).prefetch(1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bi-directional LSTM&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, 
                              strides=1, padding=&quot;casual&quot;,
                              activation=&quot;relu&quot;,
                              input_shape=[None, 1]), 
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), // 양방향
          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 200)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_late=1e-5, momentum=0.9) // 플로팅한 결과 불안정해지기 전의 학습률

      model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

      model.fit(dataset, epochs=500)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;결과수치는 긍정적이나 검증세트에 예측을 플로팅 해보면 과적합이 보여 일부 파라미터에 변화를 줄 필요가 있음&lt;/li&gt;
      &lt;li&gt;MAE 로 손실을 플로팅하면 문제점을 확인할 수 있음
        &lt;ul&gt;
          &lt;li&gt;스파이크 현상은 배치 크기가 작아서 무작위 노이즈가 많기 때문임&lt;/li&gt;
          &lt;li&gt;배치사이즈를 줄이거나 늘이는 것에 따라 학습과 예측이 달라짐&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-batch-sizing&quot;&gt;More on batch sizing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;신경망을 더 빠르게 학습하도록 하는 최적화 알고리즘
    &lt;ul&gt;
      &lt;li&gt;머신러닝 : 잘 작동되는 모델을 찾기 위해 많은 훈련을 거쳐야 하는 반복적인 과정
        &lt;ul&gt;
          &lt;li&gt;모델을 빠르게 훈련시키는 것이 매우 중요함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;딥러닝은 빅데이터에서 가장 잘 작동됨 -&amp;gt; 훈련이 어려움 (큰 데이터 세트에서 훈련하는 것은 매우 느린과정)&lt;/li&gt;
      &lt;li&gt;좋은 최적화 알고리즘을 찾는 것은 효율성을 향상시켜준다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;미니배치 경사 하강법
    &lt;ul&gt;
      &lt;li&gt;벡터화 : m개의 샘플에 대한 계산을 효율적으로 만들어줌. 명시적인 반복문 없이도 훈련 세트를 진행할 수 있도록 함.&lt;/li&gt;
      &lt;li&gt;훈련 샘플을 받아서 큰 벡터에 저장함
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^m]&lt;/code&gt; : shape : (n_x,m)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = [y^1, y^2, ..., y^m]&lt;/code&gt; : shape : (1,m)&lt;/li&gt;
          &lt;li&gt;하지만 m 의 수치가 크면 여전히 학습은 느리다.&lt;/li&gt;
          &lt;li&gt;예를들어 m 의 수치가 500만 이라면?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;전체 훈련 세트에 대한 경사 하강법을 구현하면 경사 하강법의 작은 한 단계를 밟기 전에 모든 훈련 세트를 처리해야 함
        &lt;ul&gt;
          &lt;li&gt;즉, 경사하강법의 다음 단계를 밟기 전에 500만 개의 전체 훈련 샘플을 처리해야 함&lt;/li&gt;
          &lt;li&gt;500만 개의 전체 훈련 샘플을 모두 훈련하기 전에 경사 하강법이 진행되도록 하면 더 빠른 알고리즘을 얻을 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;훈련 세트를 더 작은 훈련세트 (미니배치) 로 나눔
        &lt;ul&gt;
          &lt;li&gt;mini-batch 가 1000개의 샘플을 갖는다고 가정&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = [x^1, x^2, ..., x^1000 | x^1001, ..., x^2000 | ... | ... x^m]&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X = X^{1},                   X^{2}, ... ,               X^{5000}&lt;/code&gt; : shape : (n_x, 1000)&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y = Y^{1},                   Y^{2}, ... ,               Y^{5000}&lt;/code&gt; : shape : (1, 1000)&lt;/li&gt;
          &lt;li&gt;Mini-batch t : X^{t}, Y^{t}&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;표기법 정의
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x^(i)&lt;/code&gt; : i 번째 훈련 샘플&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z^[l]&lt;/code&gt; : l 번째 신경망의 z 값&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X^{t}&lt;/code&gt; : t 번째 미니배치 X&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Batch Gradient Descent : 일반적인 경사하강법, 모든 훈련 세트를 동시에 훈련시킴, 훈련 샘플의 모든 배치를 진행시킨다는 관점&lt;/li&gt;
      &lt;li&gt;미니배치 : 전체 훈련 세트 X,Y 를 한번에 진행시키지 않고, 하나의 미니배치 X^{t}, Y^{t} 를 동시에 진행시키는 알고리즘
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  for t=1 ,..., 5000 : 총 미니배치의 수 (5000개)
      // 1 step of gradient descent using X^{t}, Y^{t}
      // (as if m=1000)
      // 모든 1000개의 샘플에 대해 명시적인 반복문을 갖는 것보다 벡터화를 사용해 모든 1000개의 샘플을 동시에 진행함
      Forward prop on X^{t}
          Z^[1] = W^[1] * X^{t} + b^[1] : Vectorized Implementation (1000 examples)
          A^[1] = g^[1] * (Z^[1]) : Vectorized Implementation (1000 examples)
          ...
          A^[l] = g^[l] * (Z^[l]) : Vectorized Implementation (1000 examples)
      Compute cost J^{t} = 1/1000 * Sum (( i = 1 to l) Loss(expect(y^(i)), y^(i)))
          + 정규화 항
      Backprop to compute Gradients cost J^(t) using X^{t}, Y^{t}
          W^[l] = W^[l] - adW^[l], b^[l] = b^[l] - adb^[l]
      ...
      1 epoch : pass through training set (5000 개의 경사하강단계)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm-notebook&quot;&gt;Convolutions with LSTM notebook&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutions-with-lstm&quot;&gt;Convolutions with LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;real-data---sunspots&quot;&gt;Real data - sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;소스 내려받기 : 케글에서 내려받거나, 이번학습을 위한 데이터 제공 저장소를 사용 (후자)&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv -O /tmp/sunspots.csv&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CSV 읽기&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      import csv
      time_step = []
      sunspots = []

      with open(&apos;/tmp/sunspots.csv&apos;) as csvfile:
          reader = csv.reader(csvfile, delimiter=&apos;,&apos;)
          next(reader)
          for row in reader :
              sunspots.append(float(row[2]))
              time_step.append(int(row[0]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;numpy 배열로 전환&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      series = np.array(sunspots) // numpy 에 항목을 추가할 때마다 목록을 복제하는데,
      time = np.array(time_step) // 메모리 관리 과정이 많이 진행되기 때문에 데이터 양이 많으면 느려질 수 있음
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;시계열을 훈련 및 검증 데이터 세트로 분할&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      split_time = 1000
      time_train = time[:split_time]
      x_train = series[:split_time]
      time_valid = time[split_time:]
      x_valid = series[split_time:]

      window_size = 20
      batch_size = 32
      shuffle_buffer_size = 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이전 windowed_dataset 함수와 동일 코드를 사용&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
          dataset = tf.data.Dataset.from_tensor_slices(series)
          dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
          dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
          dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
          dataset = dataset.batch(batch_size).prefetch(1)
          return dataset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-and-tune-the-model&quot;&gt;Train and tune the model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;이전 수업에서 배웠던 모델로 예측하여 플로팅하면, 결과는 괜찮아보이나 MAE 가 매우 큼&lt;/li&gt;
  &lt;li&gt;이는 이전 window_size 가 20 (여기서는 약 2년이 안되는 시간) 이나, 사실 흑점 데이터의 주기는 11년 혹은 22년으로 추정됨&lt;/li&gt;
  &lt;li&gt;window_size 를 11년에 해당하는 132로 두고 다시 훈련을 하면 차트는 더 잘 나오나 MAE는 더 커짐
    &lt;ul&gt;
      &lt;li&gt;데이터를 되돌아보면 11년 주기의 계절성을 갖지만 창 안에 계절 전체가 있어야 할 필요는 없음&lt;/li&gt;
      &lt;li&gt;플롯을 확대해보면 전형적인 시계열 형태 데이터임&lt;/li&gt;
      &lt;li&gt;나중에 오는 값이 앞선 값과 연관이 있지만 노이즈가 많음&lt;/li&gt;
      &lt;li&gt;그래서 훈련 시에 창 크기가 클 필요는 없을 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;data 의 분할을 1000을 훈련, 2500을 검증으로 설정하였는데 이는 좋지 못한 분할임
    &lt;ul&gt;
      &lt;li&gt;3500과 500으로 지정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망 설계와 파라미터의 크기 변경
    &lt;ul&gt;
      &lt;li&gt;10, 10, 1 레이어 를 30, 15, 1 로 값을 바꿔서 훈련 (입력 Shape 값이 30)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예측
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.predict(series[3205:3235][np.newaxis])&lt;/code&gt; : 7.077 개의 흑점 예상 (실 데이터 8.7 개)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;설정 변경 : MAE 13.7&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  split_time = 3000
  window_size = 60
	
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(20, input_shape=[window_size], activation=&quot;relu&quot;),
      tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
      tf.keras.layers.Dense(1)
  ])
	
  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;8.13 예측 (실제값 8.7)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots-notebooks-lab-2--lab-3&quot;&gt;Sunspots notebooks (Lab 2 &amp;amp; Lab 3)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 으로 대체&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sunspots&quot;&gt;Sunspots&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;입력 창의 크기를 60으로 함&lt;/li&gt;
  &lt;li&gt;DNN 을 Dense 20, 10, 1 로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;combining-our-tools-for-analysis&quot;&gt;Combining our tools for analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      window_size = 60
      batch_size = 64
      train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

      model = tf.keras.models.Sequential([
          tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=&quot;casual&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]),
          tf.keras.layers.LSTM(32, return_sequences=True),
          tf.keras.layers.LSTM(32),
          tf.keras.layers.Dense(30, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 400)
      ])

      lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch / 20))
      optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-8, momentum=0.9)
      model.compile(loss=tf.keras.losses.Huber(), optimizer = optimizer, metrics=[&quot;mae&quot;])
      history = model.fit(train-set, epochs=100, callbacks=[lr_schedule])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;batch_size 의 변경 (256) : loss 에 노이즈가 생길 경우 고려해볼 파라미터&lt;/li&gt;
  &lt;li&gt;하이퍼파라미터를 다양하게 실험해 봐야 함&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Real-world time series data</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 03. Recurrent Neural Networks for Time Series</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 03. Recurrent Neural Networks for Time Series" /><published>2022-10-25T14:00:00+09:00</published><updated>2022-10-25T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_03_Recurrent_Neural_Networks_for_Time_Series/">&lt;h1 id=&quot;recurrent-neural-networks-for-time-series&quot;&gt;Recurrent Neural Networks for Time Series&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Recurrent Neural Networks 와 Long Short Term Memory Networks 는 시계열 데이터의 예측과 분류에 매우 유용함&lt;/li&gt;
  &lt;li&gt;Lambda Layer : 신경망 내 임의의 코드를 레이어로 활용할 수 있음 (전처리와 후처리)
    &lt;ul&gt;
      &lt;li&gt;명시적인 전처리 단계로 데이터를 스케일링한 다음 신경망에 넣는 게 아니라 Lambda 레이어를 사용할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conceptual-overview&quot;&gt;Conceptual overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN : 순환 레이어를 포함한 신경망
    &lt;ul&gt;
      &lt;li&gt;시퀀스 입력값을 순차적으로 처리하도록 설계&lt;/li&gt;
      &lt;li&gt;입력값의 형태 : 배치 사이즈, 타임스탬프 (윈도우사이즈), 컬럼디멘전 (다변량) = 3차원
        &lt;ul&gt;
          &lt;li&gt;지금까지 사용한 입력값 형태 : 배치 사이즈, 입력값 특징 수 (윈도우 사이즈)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN Cell
    &lt;ul&gt;
      &lt;li&gt;겉으로 보기에는 셀이 많은 것 같지만, 셀은 하나 뿐이고 이를 반복적으로 사용하여 출력값을 산출&lt;/li&gt;
      &lt;li&gt;입력값이 2개 (X 값과 상태벡터 H 값) - 상태벡터값을 이용해 이전 입력값의 잔존 데이터를 전달받음&lt;/li&gt;
      &lt;li&gt;입력차원 (예: 타임스탬프가 30개) 만큼 반복&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rnn-notebook&quot;&gt;RNN Notebook&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shape-of-the-inputs-to-the-rnn&quot;&gt;Shape of the inputs to the RNN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터의 형태, 데이터를 분할한 배치
    &lt;ul&gt;
      &lt;li&gt;예시
        &lt;ul&gt;
          &lt;li&gt;Window size 가 30 : 시간 단계가 30&lt;/li&gt;
          &lt;li&gt;4개로 일괄 처리 : 배치값 4&lt;/li&gt;
          &lt;li&gt;입력 형태는 4 * 30 * 1&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;셀의 관점
        &lt;ul&gt;
          &lt;li&gt;하나의 셀은 고정된 시간 단계에서 (Batch Size : 4 * 1) 의 입력을 받음&lt;/li&gt;
          &lt;li&gt;레이어 내 메모리셀이 3개의 뉴런으로 구성된다면&lt;/li&gt;
          &lt;li&gt;출력값 행렬은 4 * 3&lt;/li&gt;
          &lt;li&gt;출력 형태는 4(Batch Size) * 30(Window Size) * 3(Unit Size)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;단순 RNN 에서의 상태 출력값 H 는 출력값 행렬 Y 와 동일함&lt;/li&gt;
      &lt;li&gt;일부 경우에는 시퀀스를 입력하되, 출력값의 경우 배치 내 각 인스턴스에 대한 단일 벡터를 얻고 싶은 경우가 있음
        &lt;ul&gt;
          &lt;li&gt;마지막 (마지막 시퀀스 스텝-Window) 을 제외하고 모든 출력값을 무시&lt;/li&gt;
          &lt;li&gt;시퀀스 출력값을 도출하려면 레이어를 생성할 때 return_sequences 를 True 로 지정해야 함
            &lt;ul&gt;
              &lt;li&gt;하나의 RNN 레이어를 다른 레이어 위에 스태킹 할때 이 작업이 반드시 필요&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outputting-a-sequence&quot;&gt;Outputting a sequence&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;적층 예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  model = tf.keras.models.Sequential([
	tf.keras.layers.SimpleRNN(40, return_sequences=True, input_shape = [None,1]),
	tf.keras.layers.SimpleRNN(40),
	tf.keras.layers.Dense(1),
  ])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;타 RNN 레이어에 입력으로 들어가야 하는 RNN 레이어에 return_sequences 를 True 로 설정&lt;/li&gt;
      &lt;li&gt;Dense 레이어에 입력으로 들어가야 하는 RNN 레이어는 마지막 시퀀스 단계의 결과값만을 출력&lt;/li&gt;
      &lt;li&gt;input_shape (배치 사이즈) 를 설정하지 않음 : 어떤 크기든 상관이 없으니 정의할 필요가 없음&lt;/li&gt;
      &lt;li&gt;Timestamp 값을 None 으로 설정 : 시퀀스 길이와 관계 없이 입력값을 받음&lt;/li&gt;
      &lt;li&gt;마지막 차원이 1로 되어있는 이유 : 일변량 시계열을 다루기 때문&lt;/li&gt;
      &lt;li&gt;두번째 층 RNN 레이어에 return_sequences 값을 True 로 설정할 경우
        &lt;ul&gt;
          &lt;li&gt;시퀀스 값이 출력됨&lt;/li&gt;
          &lt;li&gt;Keras 는 각 시간 단계별로 동일한 Dense 레이어를 독립적으로 활용함&lt;/li&gt;
          &lt;li&gt;입력값이 시퀀스이고 출력값 또한 시퀀스일 경우 : 시퀀스 to 시퀀스 RNN&lt;/li&gt;
          &lt;li&gt;차원의 값은 RNN 레이어의 유닛 값에 따라 변동될 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lambda-layers&quot;&gt;Lambda layers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  model = tf.keras.models.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),
                        input_shape=[window_size]),
    tf.keras.layers.SimpleRNN(40, return_sequences=True),
    tf.keras.layers.SimpleRNN(40),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;첫 lambda 레이어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.expand_dims(x, axis=-1)&lt;/code&gt; : 기존 window 생성 function 을 그대로 활용하기 위해 차원을 하나 늘림 (2차원-&amp;gt;3차원)&lt;/li&gt;
      &lt;li&gt;마지막 lambda 레이어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lambda x: x * 100.0&lt;/code&gt; : RNN 의 기본 활성함수 tanh 의 출력값 -1 ~ 1 &amp;gt; 시계열 값은 10개 단위로 구성되고, 비슷한 값으로 출력값을 올리면 학습에 도움이 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adjusting-the-learning-rate-dynamically&quot;&gt;Adjusting the learning rate dynamically&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  train_set = windowed_dataset(x_train, window_size, batch_size=128,
      shuffle_buffer=shuffle_buffer_size)

  model = tf.keras.models.Sequential([
          tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1), input_shape=[None]),
          tf.keras.layers.SimpleRNN(40, return_sequences=True),
          tf.keras.layers.SimpleRNN(40),
          tf.keras.layers.Dense(1),
          tf.keras.layers.Lambda(lambda x: x * 100.0)
      ])

  lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))

  optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)

  model.compile(loss=tf.kears.losses.Huber(),
                  optimizer=optimizer,
                  metrics=[&quot;mae&quot;])

  history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;callback 함수를 활용, epoch 진행 별로 학습률을 약간 변경&lt;/li&gt;
      &lt;li&gt;Huber 손실함수 : 이상치에 덜 민감하게 반응하는 손실함수, 데이터에 노이즈가 많이 섞여있을 때 시도해볼만 함
        &lt;ul&gt;
          &lt;li&gt;squared error loss 보다 이상치에 덜 민감함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lstm&quot;&gt;LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN
    &lt;ul&gt;
      &lt;li&gt;X 가 셀에 투입되면 Y 결과값과 H 상태벡터가 출력되고, 이는 다음 셀에 영향을 줌&lt;/li&gt;
      &lt;li&gt;Step 이 진행되면서 초기 H 상태벡터의 영향도는 점점 작아짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM
    &lt;ul&gt;
      &lt;li&gt;전체 훈련 기간 동안 상태를 유지해주는 셀 상태를 추가함&lt;/li&gt;
      &lt;li&gt;상태 값이 셀 간에 이동을 하고 Step 사이를 이동하면서 더 잘 유지될 수 있게함 - 앞 단계에 있던 데이터가 전체 추정치에 더 큰 영향을 줌&lt;/li&gt;
      &lt;li&gt;상태는 양방향으로 움직일 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;coding-lstms&quot;&gt;Coding LSTMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.backend.clear_session()&lt;/code&gt; : 내부 변수를 초기화. 이후 버전에 영향을 주지 않고 여러 모델을 시험해 볼 수 있음&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))&lt;/code&gt; : 32개 셀의 단일 LSTM 레이어 추가. 예측에 미치는 영향을 파악할 수 있도록 양방향으로 만듦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예시&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  tf.keras.backend.clear_session()
  dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

  model = tf.keras.models.Sequential([
      tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
      tf.keras.layers.Dense(1),
      tf.keras.layers.Lambda(lambda x: x * 100.0)
  ])	

  model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
  model.fit(dataset, epochs=100, verbose=0)
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))&lt;/code&gt; : LSTM 레이어를 한층 더 쌓음, retrun_sequences 를 True 로 설정해야만 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Recurrent Neural Networks for Time Series</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 02. Deep Neural Networks for Time Series</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 02. Deep Neural Networks for Time Series" /><published>2022-10-21T14:00:00+09:00</published><updated>2022-10-21T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_02_Deep_Neural_Networks_for_Time_Series/">&lt;h1 id=&quot;deep_neural_networks_for_time_series&quot;&gt;Deep_Neural_Networks_for_Time_Series&lt;/h1&gt;

&lt;h2 id=&quot;preparing-features-and-labels&quot;&gt;Preparing features and labels&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;시계열에서의 Input 값과 Label 값
    &lt;ul&gt;
      &lt;li&gt;Features : 입력되는 값
        &lt;ul&gt;
          &lt;li&gt;예 : 이전 30개의 시점의 값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Labels : 예측할 값 (정답)
        &lt;ul&gt;
          &lt;li&gt;예 : 미래 시점의 값&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)
	dataset = tf.data.Dataset.range(10)

	# Window the data but only take those with the specified size
	dataset = dataset.window(5, shift=1, drop_remainder=True)

	# Flatten the windows by putting its elements in a single batch
	dataset = dataset.flat_map(lambda window: window.batch(5))

	# Create tuples with features (first four elements of the window) and labels (last element)
	dataset = dataset.map(lambda window: (window[:-1], window[-1]))

	# Shuffle the windows
	dataset = dataset.shuffle(buffer_size=10)

	# Create batches of windows ( 한번에 여러 데이터 처리를 위함 )
	dataset = dataset.batch(2).prefetch(1)

	# Print the results
	for x,y in dataset:
	  print(&quot;x = &quot;, x.numpy())
	  print(&quot;y = &quot;, y.numpy())
	  print()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;feeding-windowed-dataset-into-neural-network&quot;&gt;Feeding windowed dataset into neural network&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
	    &quot;&quot;&quot;Generates dataset windows

	    Args:
	      series (array of float) - contains the values of the time series
	      window_size (int) - the number of time steps to include in the feature
	      batch_size (int) - the batch size
	      shuffle_buffer(int) - buffer size to use for the shuffle method

	    Returns:
	      dataset (TF Dataset) - TF Dataset containing time windows
	    &quot;&quot;&quot;
	  
	    # Generate a TF Dataset from the series values
	    dataset = tf.data.Dataset.from_tensor_slices(series)
	    
	    # Window the data but only take those with the specified size
	    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
	    
	    # Flatten the windows by putting its elements in a single batch
	    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))

	    # Create tuples with features and labels 
	    dataset = dataset.map(lambda window: (window[:-1], window[-1]))

	    # Shuffle the windows 
	    dataset = dataset.shuffle(shuffle_buffer) // shuffle buffer 의 크기만큼 이동하면서 buffer 내에서 무작위로 하나씩 선택 (선택속도증가)
	    
	    # Create batches of windows
	    dataset = dataset.batch(batch_size).prefetch(1)
	    
	    return dataset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;single-layer-neural-network&quot;&gt;Single layer neural network&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	# Generate the dataset windows
	dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

	# Build the single layer neural network
	l0 = tf.keras.layers.Dense(1, input_shape=[window_size])
	model = tf.keras.models.Sequential([l0])

	# Print the initial layer weights
	print(&quot;Layer weights: \n {} \n&quot;.format(l0.get_weights()))

	# Print the model summary
	model.summary()

	# Set the training parameters
	model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))

	# Train the model
	model.fit(dataset,epochs=100)

	# Print the layer weights
	print(&quot;Layer weights {}&quot;.format(l0.get_weights()))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;machine-learning-on-time-windows&quot;&gt;Machine learning on time windows&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-single-layer-neural-network&quot;&gt;More on single layer neural network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-neural-network-training-tuning-and-prediction&quot;&gt;Deep neural network training, tuning and prediction&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
	dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
	model = tf.keras.models.Sequential([
		tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
		tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
		tf.keras.layers.Dense(1)
	])

	model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
	model.fit(dataset, epochs=100, verbose=0)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;우리가 선택한 학습률이 아니라 최적의 학습률을 선택할 수 있다면 더 좋은 결과가 나올 것
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;callback 을 활용하는 기법&lt;/p&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;		
      dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
      model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
          tf.keras.layers.Dense(1)
      ])
		
      // 각 epoch 종료 시마다 callback 에서 호출, epoch 숫자값을 기준으로 학습률을 값으로 변경
      // 
      lr_schedule = tf.keras.callbacks.LearningRateScheduler(
          lambda epoch: 1e-8 * 10**(epoch / 20))
		
      optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)
		
      model.compile(loss=&quot;mse&quot;, optimizer=optimizer)
			
      history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])
		
      // 트레이닝을 마치고 나면 epoch 당 학습률에 대한 epoch 당 오차를 플로팅
      // x 축은 learning rate, y축은 epoch 의 손실
      lrs = 1e-8 (10 ** (np.arange(100) / 20))
      plt.semilogx(lrs, history.history[&quot;loss&quot;])
      plt.axis([1e-8, 1e-3, 0, 300])
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;위에서 구한 learning_rate (7e-6) 로 재훈련&lt;/p&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
      window_size = 30
      dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
      model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(10, input_shape=[window_size], activation=&quot;relu&quot;)
          tf.keras.layers.Dense(10, activation=&quot;relu&quot;)
          tf.keras.layers.Dense(1)
      ])

      optimizer = tf.keras.optimizers.SGD(learning_rate=7e-6, momentum=0.9)
      model.compile(loss=&quot;mse&quot;, optimizer=optimizer)
      model.fit(dataset, epochs=500)

      // 훈련 도중 산출한 손실 플로팅 코드
      loss = history.history[&apos;loss&apos;]
      epochs = range(len(loss))
      plt.plot(epochs, loss, &apos;b&apos;, label=&apos;Training Loss&apos;)
      plt.show()

      // 초기손실 (왜곡값) 자르기
      loss = history.history[&apos;loss&apos;]
      epochs = range(10, len(loss))
      plot_loss = loss[10:]
      print(plot_loss)
      plt.plot(epochs, plot_loss, &apos;b&apos;, label=&apos;Training Loss&apos;)
      plt.show()

      // mean absolute error 값 확인
      tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-neural-network&quot;&gt;Deep neural network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 참조&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Deep_Neural_Networks_for_Time_Series</summary></entry><entry><title type="html">Sequences, Time Series and Prediction - 01. Sequences and Prediction</title><link href="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction/" rel="alternate" type="text/html" title="Sequences, Time Series and Prediction - 01. Sequences and Prediction" /><published>2022-10-20T14:00:00+09:00</published><updated>2022-10-20T14:00:00+09:00</updated><id>https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction</id><content type="html" xml:base="https://bluesplatter.com/ai/time%20series/Sequences_TimeSeries_and_Prediction_01_Sequences_and_Prediction/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;순차 시계열 데이터 (Sequential time series data)
    &lt;ul&gt;
      &lt;li&gt;값이 시간에 따라 변하는 것&lt;/li&gt;
      &lt;li&gt;예
        &lt;ul&gt;
          &lt;li&gt;주식거래의 종가&lt;/li&gt;
          &lt;li&gt;특정 일의 기온&lt;/li&gt;
          &lt;li&gt;웹사이트의 방문자 수&lt;/li&gt;
          &lt;li&gt;스프레드시트에 기록할 수 있는 데이터&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다룰 내용
    &lt;ul&gt;
      &lt;li&gt;미래 시점의 값 예측을 위한 다양한 방법론&lt;/li&gt;
      &lt;li&gt;위의 내용의 구현법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-a-conversation-with-andrew-ng&quot;&gt;Introduction, A conversation with Andrew Ng&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터의 합성 시퀀스를 만들기&lt;/li&gt;
  &lt;li&gt;데이터 시계열에서 공통 속성을 살펴보기
    &lt;ul&gt;
      &lt;li&gt;계절성 : 날씨의 경우 6월이 1월보다 따듯하고, 11월은 10월보다 습할 수 있음&lt;/li&gt;
      &lt;li&gt;경향성 : 주식의 종가처럼 시간이 가면서 상승, 혹은 하강&lt;/li&gt;
      &lt;li&gt;노이즈 : 무작위 요소&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;구현 : 흑점 활동 모니터
    &lt;ul&gt;
      &lt;li&gt;흑점 활동
        &lt;ul&gt;
          &lt;li&gt;11년, 혹은 22년의 주기 (계절성)&lt;/li&gt;
          &lt;li&gt;노이즈&lt;/li&gt;
          &lt;li&gt;250년 전부터 측정해온 데이터 활용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;sequences-and-prediction&quot;&gt;Sequences and Prediction&lt;/h1&gt;

&lt;h2 id=&quot;time-series-examples&quot;&gt;Time series examples&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 (Time Series) 이란 무엇인가?
    &lt;ul&gt;
      &lt;li&gt;오랜 시간에 걸쳐 균등한 간격으로 순서가 지정된 시퀀스로 나타나는 값&lt;/li&gt;
      &lt;li&gt;다변량 시계열 : 각 시점에서 복수 개의 값이 표시된 경우
        &lt;ul&gt;
          &lt;li&gt;데이터에 추가값을 더하여 상관관계를 파악할 수 있음
            &lt;ul&gt;
              &lt;li&gt;시간의 흐름에 따른 기온과 이산화탄소 배출량의 상관관계&lt;/li&gt;
              &lt;li&gt;자동차의 이동경로 (동시간의 간격 (속도), 위도, 경도 등)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;machine-learning-applied-to-time-series&quot;&gt;Machine learning applied to time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;머신러닝으로 가능한 시계열 관련 작업
    &lt;ul&gt;
      &lt;li&gt;데이터를 기반으로 한 예측 작업&lt;/li&gt;
      &lt;li&gt;이미 가지고 있는 데이터보다 이전 시점의 데이터를 예측&lt;/li&gt;
      &lt;li&gt;실질적으로 존재하지 않는 데이터의 시점의 데이터를 예측&lt;/li&gt;
      &lt;li&gt;변칙 감지에 활용&lt;/li&gt;
      &lt;li&gt;패턴의 발견 (예: 음파를 인식)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;common-patterns-in-time-series&quot;&gt;Common patterns in time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;흔하게 나타나는 시계열 패턴 (눈으로 보고 인식하는데 유용)
    &lt;ul&gt;
      &lt;li&gt;추세 : 특정한 방향으로 움직이는 경우 (예: 무어의 법칙)&lt;/li&gt;
      &lt;li&gt;계절성 : 패턴이 예측 가능한 간격으로 반복됨 (예: 쇼핑사이트 방문자수 (주말에 올라감))&lt;/li&gt;
      &lt;li&gt;노이즈 : 전혀 예측이 불가능한 임의의 값들로 구성된 세트&lt;/li&gt;
      &lt;li&gt;자기상관관계 : 시간의 흐름에 따라 과거 혹은 현재의 값이 미래에 영향을 주는 것&lt;/li&gt;
      &lt;li&gt;복합적으로 나타나는 경우&lt;/li&gt;
      &lt;li&gt;비정상시계열 : 명확한 패턴을 보이다가 큰 이벤트로 인해 패턴이 깨지는 경우
        &lt;ul&gt;
          &lt;li&gt;특정 경향성을 보이는 경우에 특정 구간만 학습&lt;/li&gt;
          &lt;li&gt;하지만 현실의 데이터는 단순하지 않음 (패턴이 깨지며 경향이 나타났으나 다시 과거 패턴으로 회귀)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-time-series&quot;&gt;Introduction to time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Colab 파일&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-validation-and-test-sets&quot;&gt;Train, validation and test sets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;예측 모델의 성능 측정
    &lt;ul&gt;
      &lt;li&gt;Fixed Partitioning (고정 파티셔닝) : 시계열을 훈련기간, 검증 기간, 테스트 기간으로 분할
        &lt;ul&gt;
          &lt;li&gt;시계열이 계절성이 있는 경우 각각의 기간에 계절 전체를 포함하고 싶음&lt;/li&gt;
          &lt;li&gt;시간이 지남에 따라 검증 기간의 데이터를 훈련에 사용, 테스트 기간의 데이터로 검증을 하고, 새로운 테스트 기간으로 테스트를 함&lt;/li&gt;
          &lt;li&gt;테스트 기간은 현재 데이터에 가장 영향을 많이 줄 수 있는 데이터. 따라서 테스트 세트를 포기하는 경우가 흔함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;롤 포워드 파티셔닝
        &lt;ul&gt;
          &lt;li&gt;짧은 훈련기간을 가지고, 점점 증가시켜 (한번에 하루, 한번에 한 주) 반복수행&lt;/li&gt;
          &lt;li&gt;검증기간에는 다음 달이나 다음 주를 예측&lt;/li&gt;
          &lt;li&gt;고정 파티셔닝을 여러 번 시행하고 모델을 계속 다듬는 과정&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;metrics-for-evaluating-performance&quot;&gt;Metrics for evaluating performance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;성능을 계산할 지표
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      errors = forcasts - actual // 오차산출
      mse = np.square(errors).mean() // 평균제곱오차 :  가장 일반적인 지표 (음수제거를 하여 에러간 상쇄가 없도록 함)
      rmse = np.sqrt(mse) // 평균제곱근오차 : 원래 에러 규모와 동일한 규모를 만들기 위해 제곱근 계
      mae = np.abs(errors).mean() // 평균절대오(편)차 :  제곱 대신 절대값을 사용
      mape = np.abs(errors / x valid).mean() // 평균절대백분율오차 : 절대 오차와 절대값의 평균 비율 (값 대비 오차의 크기를 파악)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;작은 오차보다 큰 오차가 생겼을 경우 비용이 훨씬 크다면 MSE&lt;/li&gt;
      &lt;li&gt;손익이 단순 오차의 크기에 비례한다면 MAE
        &lt;ul&gt;
          &lt;li&gt;케라스에서의 구현
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keras.metrics.mean_absolute_error(x_valid, native_forecast).numpy()&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;moving-average-and-differencing&quot;&gt;Moving average and differencing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;이동 평균을 계산 : 일반적이고 간단한 예측 방법
    &lt;ul&gt;
      &lt;li&gt;노이즈를 제거하고, 원본 시계열을 대략적으로 유추하는 곡선이 도출&lt;/li&gt;
      &lt;li&gt;추세나 계절성을 예측하지는 않음 : 현재 시점에서 미래를 예측하고자 하는 기간 이후에는 단순 예측보다 결과가 저조할 수 있음&lt;/li&gt;
      &lt;li&gt;차분은 이를 피하는 방법 중 하나 : 시계열에서 추세와 계절성을 제거
        &lt;ul&gt;
          &lt;li&gt;즉, 시계열 자체를 연구하는게 아니라 T 시점의 값과 이전 기간의 값 사이의 차이를 연구&lt;/li&gt;
          &lt;li&gt;차분에 이동평균선을 예측하면 이는 차분에 대한 예측일 뿐이고, 원본 시계열에 대한 것은 아님
            &lt;ul&gt;
              &lt;li&gt;뺀 값 (이전 기간의 값) 을 다시 더해줘야 함&lt;/li&gt;
              &lt;li&gt;하지만 이전의 값을 더해줄 때 노이즈도 같이 생기게 됨. &amp;gt; 이동 평균을 이용하여 과거의 노이즈를 제거&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trailing-versus-centered-windows&quot;&gt;Trailing versus centered windows&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Trailing Window (현재 값의 이동 평균을 산출할때)
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t1000 = (t970 + t971 + ... + t999) / 30&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Centered Window (과거 값의 이동 평균을 산출할때) : 정확도가 Trailing Window 보다 높음
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t635 = (t630 + t631 + ... + t640) / 11&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forecasting&quot;&gt;Forecasting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;week-1-working-with-time-series&quot;&gt;Week 1: Working with time series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter notebook 자료&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="AI" /><category term="Time Series" /><category term="Coursera" /><category term="deeplearning.ai" /><category term="Laurence Moroney" /><category term="timeseries" /><category term="시계열" /><category term="tensorflow" /><summary type="html">Introduction</summary></entry><entry><title type="html">Jenkins Pipeline - Declarative and IaC approaches for DevOps</title><link href="https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps/" rel="alternate" type="text/html" title="Jenkins Pipeline - Declarative and IaC approaches for DevOps" /><published>2022-09-30T14:00:00+09:00</published><updated>2022-09-30T14:00:00+09:00</updated><id>https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps</id><content type="html" xml:base="https://bluesplatter.com/jenkins/JenkinsPipeline_Declarative_and_IaC_approaches_for_DevOps/">&lt;h1 id=&quot;후기&quot;&gt;후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_JenkinsPipeline-DeclarativeandIaCapproachesforDevOps_course_info.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;해당 강좌는 영상플레이 시간 2시간 (하지만 영어 강좌여서 3시간 이상) 소요 되는 강좌입니다.&lt;/li&gt;
  &lt;li&gt;장점은 부담되는 양의 자료 (책, 강의 등) 를 보기 전에 짧은 시간 훑어보기에 좋은 강의라는 점입니다.&lt;/li&gt;
  &lt;li&gt;대부분 Declarative Script 로 Jenkinsfile 을 작성하여 활용할 것으로 생각하는 바, 기초에 좋은 강의일듯 싶습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;강좌&quot;&gt;강좌&lt;/h1&gt;

&lt;h2 id=&quot;파이프라인이란-무엇인가&quot;&gt;파이프라인이란 무엇인가?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;정의와 기능
    &lt;ul&gt;
      &lt;li&gt;SCM(Source Code Management) 의 Continuous Delivery 절차를 위한 플러그인 집합&lt;/li&gt;
      &lt;li&gt;제품 개발 라이프사이클 (Submitting Code -&amp;gt; Testing -&amp;gt; Staging -&amp;gt; Deployment …) 에 연관&lt;/li&gt;
      &lt;li&gt;각 단계의 성공 / 실패 여부 제공&lt;/li&gt;
      &lt;li&gt;다양한 타 환경에서의 운영 지원&lt;/li&gt;
      &lt;li&gt;저장소 단계에서 실 환경 배포까지의 자동화&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;파이프라인 작성법
    &lt;ul&gt;
      &lt;li&gt;Pipeline script 를 Jenkins UI 에서 작성&lt;/li&gt;
      &lt;li&gt;Jenkins file 을 통한 작성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;파이프라인 언어
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;declarative&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;scripted&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;jenkinsfile 의 장점
    &lt;ul&gt;
      &lt;li&gt;IaC (Infrastructure as Code)
        &lt;ul&gt;
          &lt;li&gt;application code 와 마찬가지로 취급되어 저장소에 committed 됨&lt;/li&gt;
          &lt;li&gt;저장소의 이점을 누리며, 동시에 어떤 구조로 되어있는지 구성원들이 시각적으로 확인할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;파이프라인-스크립트의-형태와-전역변수&quot;&gt;파이프라인 스크립트의 형태와 전역변수&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;젠킨스 서버 구동
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;java -jar jenkins.war httpPort=8080&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;브라우저에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:8080&lt;/code&gt; 으로 접속&lt;/li&gt;
      &lt;li&gt;기본 계정 로그인 : admin / admin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;new items
    &lt;ul&gt;
      &lt;li&gt;네이밍&lt;/li&gt;
      &lt;li&gt;Pipeline 생성&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pipeline 섹션으로 이동&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Jenkinsfile (Declarative Pipeline)
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  pipeline {
      agent any // Execute this Pipeline or any of its stages, on any available agent.
      stages {
          // stage : conceptually distinct subset or tasks performed throughout the entire pipeline
          stage(&apos;Build&apos;) { // Defines the &quot;Build&quot; stage. (Keyword is flexible)
              // Steps represents a single task.
              // It tells Jenkins what to do at a particular point in time of a particular step in the process.
              steps {
                  //  Perform some steps related to the &quot;Build&quot; stage.
              }
          }
          stage(&apos;Test&apos;) { // Defines the &quot;Test&quot; stage.
              steps {
                  // Perform some steps related to the &apos;&quot;Test&quot; stage.
              }
          }
          stage(&apos;Deploy&apos;) { Defines the &quot;Deploy&quot; stage. 
              steps {
                  // Perform some steps related to the &quot;Deploy&quot; stage.
              }
          }
      }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Environment Variables (Global Variables)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8080/pipeline-syntax/globals#env&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;첫-파이프라인-스크립트-작성과-환경변수-삽입&quot;&gt;첫 파이프라인 스크립트 작성과 환경변수 삽입&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;pipeline 섹션에 코드 작성&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          stages {
              stage(&apos;stage 1&apos;){
                  steps {
                      echo &quot;hello world&quot;
                      echo BUILD_ID // Global Variable
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;Build Now&lt;/li&gt;
      &lt;li&gt;Console Output 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;environment 변수 삽입&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;environment 변수는 최상단에 위치할 수도, stage 안에 존재할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          environment {
              mainenv = &apos;dev&apos;
          }
          stages {
              stage(&apos;stage 1&apos;){
                  steps {
                      echo &quot;hello world&quot;
                      echo BUILD_ID
                      echo &quot;&quot;&quot;mainenv = ${mainenv}&quot;&quot;&quot; // enject template, literals, variables
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
          agent any
          environment {
              mainenv = &apos;dev&apos;
          }
          stages {
              stage(&apos;stage 1&apos;){
                  environment{
                      subenv = &quot;prod&quot;
                  }
                  steps {
                      echo &quot;&quot;&quot;inside stage 1: mainenv = ${mainenv}&quot;&quot;&quot; // enject template, literals, variables
                      echo &quot;&quot;&quot;inside stage 1: subenv = ${subenv}&quot;&quot;&quot;
                  }
              }
              stage(&apos;stage 2&apos;){
                  steps {
                      echo &quot;&quot;&quot;inside stage 2: mainenv = ${mainenv}&quot;&quot;&quot;
                      echo &quot;&quot;&quot;inside stage 2: subenv = ${subenv}&quot;&quot;&quot; // causing error, because of scope.
                  }
              }
          }
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;실제-github-저장소를-사용한-파이프라인-스크립트-작성과-build-steps&quot;&gt;실제 Github 저장소를 사용한 파이프라인 스크립트 작성과 build steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;대시보드에서 new item 선택
    &lt;ul&gt;
      &lt;li&gt;Pipeline 생성
        &lt;ul&gt;
          &lt;li&gt;우측 드롭다운 메뉴에서 샘플 코드를 불러올 수 있음&lt;/li&gt;
          &lt;li&gt;Pipeline Syntax 에서 Snippet Generator 활용하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most common MVN build phases&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: left&quot;&gt;Build Phase&lt;/th&gt;
          &lt;th style=&quot;text-align: left&quot;&gt;Description&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;validate&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Validates that the project is correct and all necessary information is available. This also makes sure the dependencies are downloaded.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;compile&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Compiles the source code of the project.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;test&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Runs the tests against the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;package&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Packs the compiled code in its distributable format. such as a JAR.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;install&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Install the package into the local repository, for use as a dependency in other projects locally.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;deploy&lt;/td&gt;
          &lt;td style=&quot;text-align: left&quot;&gt;Copies the final package to the remote repository for sharing with other developers and projects.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;작성한 소스코드&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      pipeline {
      	agent any
      	stages {
          	stage(&apos;Setup&apos;){
              	steps {
                   	// this will delete folder whatever the OS platform is.
                  	dir(&apos;jenkins-spring&apos;){
                      	deleteDir()               
                  	}
              	}
          	}
          	stage(&apos;Build&apos;){
              	steps {
                  	// sh : for Linux
                  	// for Windows (copy github source)
                  	bat &apos;git clone https://github.com/rudihinds/jenkins-spring.git&apos;
                  	bat &apos;mvn clean -f jenkins-spring&apos;
              	}
          	}
          	stage(&apos;Test&apos;){
              	steps {
                  	bat &apos;mvn clean test -f jenkins-spring&apos;
              	}
          	}
          	stage(&apos;Deploy&apos;){
              	steps {
                  	bat &apos;mvn clean package -f jenkins-spring&apos;
              	}
          	}
      	}
      }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;jenkinsfile-을-이용하여-scm-에-파이프라인-연결&quot;&gt;Jenkinsfile 을 이용하여 SCM 에 파이프라인 연결&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;SCM 을 통해 어떻게 파이프라인을 가져올 수 있는지?&lt;/li&gt;
  &lt;li&gt;복잡한 Microarchitecture 구조가 아니라면 보편적으로 Jenkinsfile 은 프로젝트의 root 디렉토리에 있음&lt;/li&gt;
  &lt;li&gt;Github 에 있는 Jenkinsfile 에 작성된 declarative script 와 지금까지 작성한 스크립트의 차이점
    &lt;ul&gt;
      &lt;li&gt;clone 스테이지가 없음 : Github의 Jenkinsfile을 사용한다는 것은 이미 Jenkins 에게 SCM 에서 소스코드를 가져오라고 지시한 것임&lt;/li&gt;
      &lt;li&gt;setup 스테이지가 없음 : 해당 프로세스는 이미 통합되어 있음 (Jenkins가 핸들링)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실습
    &lt;ul&gt;
      &lt;li&gt;새 파이프라인을 만들고, 파이프라인의 정의를 Pipeline script from SCM 으로 설정
        &lt;ul&gt;
          &lt;li&gt;SCM 종류 Git으로 설정&lt;/li&gt;
          &lt;li&gt;Repository URL 설정&lt;/li&gt;
          &lt;li&gt;브랜치 설정&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Build Now
        &lt;ul&gt;
          &lt;li&gt;Declarative: Checkout SCM 스테이지가 자동생성된 것을 확인할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Jenkins" /><category term="Coursera" /><category term="Rudi Hinds" /><category term="Jenkins" /><category term="Declarative and IaC approaches for DevOps" /><category term="젠킨스" /><category term="CI/CD" /><summary type="html">후기</summary></entry><entry><title type="html">Jenkins - Automating your delivery pipeline</title><link href="https://bluesplatter.com/jenkins/Jenkins_Automating_your_dilivery_pipeline/" rel="alternate" type="text/html" title="Jenkins - Automating your delivery pipeline" /><published>2022-09-29T16:00:00+09:00</published><updated>2022-09-29T16:00:00+09:00</updated><id>https://bluesplatter.com/jenkins/Jenkins_Automating_your_dilivery_pipeline</id><content type="html" xml:base="https://bluesplatter.com/jenkins/Jenkins_Automating_your_dilivery_pipeline/">&lt;h1 id=&quot;후기&quot;&gt;후기&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/coursera_Automatingyourdeliverypipeline_course_info.png&quot; alt=&quot;강좌에 대한 설명&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;해당 강좌는 영상플레이 시간 1시간 (하지만 영어 강좌여서 2시간 이상) 소요 되는 강좌입니다.&lt;/li&gt;
  &lt;li&gt;개인적으로 추천하고 싶지는 않은데, 내용을 떠나 구축된 Cloud 환경의 젠킨스가 구 버전이어서 플러그인과 호환이 되질 않습니다.&lt;/li&gt;
  &lt;li&gt;실습의 절반정도는 영상을 보는 것으로만 하였습니다.&lt;/li&gt;
  &lt;li&gt;장점은 부담되는 양의 자료 (책, 강의 등) 를 보기 전에 짧은 시간 훑어보기에 좋은 강의라는 점입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;jenkins-란&quot;&gt;Jenkins 란?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;DevOps 환경에서 Continuous integration and Delivery 를 가능하게 하는 툴&lt;/li&gt;
  &lt;li&gt;Continuous Delivery
    &lt;ul&gt;
      &lt;li&gt;개발자가 개발한 새 소스코드를 즉시 소스코드 저장소에 반영&lt;/li&gt;
      &lt;li&gt;빌딩, 테스팅, 패키징이 등이 일어나 배포 가능 버전이 생성됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이점
    &lt;ul&gt;
      &lt;li&gt;배포속도가 빨라짐&lt;/li&gt;
      &lt;li&gt;피드백을 빨리 받을 수 있음&lt;/li&gt;
      &lt;li&gt;초기 단게에서 결함을 발견할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;실습&quot;&gt;실습&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;젠킨스 서버에서 사용하기 위한 Maven 설정
    &lt;ul&gt;
      &lt;li&gt;DashBoard 에서 Global Tool Configuration 설정&lt;/li&gt;
      &lt;li&gt;하단의 Maven 설치 (네이밍 포함)&lt;/li&gt;
      &lt;li&gt;Maven : 자바용 프로젝트 빌드, 관리에 사용되는 도구&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;라이브러리의 추가, 라이브러리 버전 동기화의 어려움을 해소&lt;/li&gt;
      &lt;li&gt;프로젝트 생성, 테스트 빌드, 배포 등의 작업을 위함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Spring pet clinic 어플리케이션의 첫번째 Job 생성
    &lt;ul&gt;
      &lt;li&gt;Spring pet clinic 의 깃헙 소스코드 fork 하기&lt;/li&gt;
      &lt;li&gt;Jenkins 의 파이프라인 첫 단계는 compile stage 혹은 build stage 가 될 것&lt;/li&gt;
      &lt;li&gt;New Item&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Freestyle project : 어떠한 제약, 제한조건 없는 프로젝트
        &lt;ul&gt;
          &lt;li&gt;네이밍하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;섹션들
        &lt;ul&gt;
          &lt;li&gt;General&lt;/li&gt;
          &lt;li&gt;Source Code Management&lt;/li&gt;
          &lt;li&gt;Build Triggers&lt;/li&gt;
          &lt;li&gt;Build Environment&lt;/li&gt;
          &lt;li&gt;Build&lt;/li&gt;
          &lt;li&gt;Post-build Actions&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Source Code Management 설정
        &lt;ul&gt;
          &lt;li&gt;Git URL 값 입력&lt;/li&gt;
          &lt;li&gt;Branch 입력 (소스코드가 위치하는 경로)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Build 설정
        &lt;ul&gt;
          &lt;li&gt;Add build step -&amp;gt; Invoke top-level Maven targets : Maven 을 통한 build task 수행&lt;/li&gt;
          &lt;li&gt;Maven 버전 설정 : Global Tool Configuration 에서 네이밍한 Maven 선택&lt;/li&gt;
          &lt;li&gt;Goals 설정 : compile 입력&lt;/li&gt;
          &lt;li&gt;Save
            &lt;ul&gt;
              &lt;li&gt;Build Now&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;#1 빌드 수행&lt;/li&gt;
      &lt;li&gt;버튼 색에 따른 상태
        &lt;ul&gt;
          &lt;li&gt;Blue : 성공(완료)&lt;/li&gt;
          &lt;li&gt;Blinking : 현재 실행중&lt;/li&gt;
          &lt;li&gt;Rend : 실패&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;버튼 옆 arrow -&amp;gt; Console Output 으로 결과 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Delivery Pipeline 생성하기 (Build, Test Stage 생성하기)
    &lt;ul&gt;
      &lt;li&gt;Jenkins -&amp;gt; New Item&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Freestyle project
        &lt;ul&gt;
          &lt;li&gt;네이밍하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Source Code Management 설정
        &lt;ul&gt;
          &lt;li&gt;Git URL 값 입력&lt;/li&gt;
          &lt;li&gt;Branch 입력 (소스코드가 위치하는 경로)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Build 설정
        &lt;ul&gt;
          &lt;li&gt;Add build step -&amp;gt; Invoke top-level Maven targets : Maven 을 통한 build task 수행&lt;/li&gt;
          &lt;li&gt;Maven 버전 설정 : Global Tool Configuration 에서 네이밍한 Maven 선택&lt;/li&gt;
          &lt;li&gt;Goals 설정 : test 입력&lt;/li&gt;
          &lt;li&gt;Save
            &lt;ul&gt;
              &lt;li&gt;Build Now&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;#1 빌드 수행&lt;/li&gt;
      &lt;li&gt;Console Output 에서 테스트 케이스에 대한 성공/실패여부를 볼 수 있음
    - Test Job 의 Configure (Build 수행 후 Test 를 진행하고 싶다)&lt;/li&gt;
      &lt;li&gt;Build Triggers : 어떻게 특정 Job 을 Trigger 할지 설정
        &lt;ul&gt;
          &lt;li&gt;Build after other projects are built
            &lt;ul&gt;
              &lt;li&gt;Projects to watch : Build Job 입력&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Save
            &lt;ul&gt;
              &lt;li&gt;Pipeline 전체를 살펴보기 위해 Plugin 설치하기&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Jenkins -&amp;gt; Manage Jenkins -&amp;gt; Manage Plugins
        &lt;ul&gt;
          &lt;li&gt;Available tab -&amp;gt; search Build pipeline -&amp;gt; install&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Jenkins Dashboard 의 작은 + 버튼 클릭 -&amp;gt; Build Pipeline View
        &lt;ul&gt;
          &lt;li&gt;네이밍&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Pipeline Flow -&amp;gt; Upstream / downstream config
        &lt;ul&gt;
          &lt;li&gt;Initial Job (첫 시작 Job) 설정&lt;/li&gt;
          &lt;li&gt;초록색 Job : 이미 실행된 Job&lt;/li&gt;
          &lt;li&gt;파란색 Job : 아직 실행되지 않은 Job&lt;/li&gt;
          &lt;li&gt;노란색 Job : 실행중인 Job&lt;/li&gt;
          &lt;li&gt;빨간색 Job : Job 실패&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Run&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Codify Pipeline
    &lt;ul&gt;
      &lt;li&gt;Build pipeline 의 코드화 필요성&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;위 활동에서의 Configuration Setup 을 직접 수행할 경우 신뢰성이 떨어짐&lt;/li&gt;
      &lt;li&gt;Jenkins 는 Groovy Script 를 사용함
    - Pipeline Maven integration plugin 설치&lt;/li&gt;
      &lt;li&gt;메이븐과 젠킨스의 통합을 쉽게 해줌&lt;/li&gt;
      &lt;li&gt;scripted pipeline 에서 사용할 많은 Wrapper methods 를 제공
    - New item&lt;/li&gt;
      &lt;li&gt;Pipeline
        &lt;ul&gt;
          &lt;li&gt;Pipeline Tab&lt;/li&gt;
        &lt;/ul&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  node{
      stage(&apos;Checkout&apos;){
          git branch: &apos;main&apos;, url:&apos;https://github.com/HY03/SpringPetClinic.git&apos;
      }
      stage(&apos;Build&apos;){
          withMaven(maven: &apos;M3&apos;){
              sh &apos;mvn compile&apos;
          }
      }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  - node : where you run your job
      + master : where jenkins installed
      + slave : for the distributed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;declarative pipeline
    &lt;ul&gt;
      &lt;li&gt;필요성&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Jenkins 와 엮여있는 Groovy Script 의 어려움
    - New item&lt;/li&gt;
      &lt;li&gt;Pipeline
        &lt;ul&gt;
          &lt;li&gt;Pipeline Tab&lt;/li&gt;
        &lt;/ul&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  pipeline{
      agent{label &apos;master&apos;}
      tools{maven &apos;M3&apos;}
      stages{
          stage(&apos;Checkout&apos;){
              steps{
                  git branch: &apos;main&apos;, url:&apos;https://github.com/HY03/SpringPetClinic.git&apos;&apos;
              }
          }
          stage(&apos;Build&apos;){
              steps{
                  sh &apos;mvn compile&apos;   
              }
          }
          stage(&apos;Test&apos;){
              steps{
                  sh &apos;mvn test&apos;
              }
          }
          stage(&apos;Package&apos;){
              steps{
                  sh &apos;mvn package&apos;
              }
          }
          stage(&apos;Deploy&apos;){
              steps{
                  sh &apos;java -jar /var/lib/jenkins/workspace/PetClinicDeclarativePipeline/target/*.jar&apos;
              }
          }
      }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;

        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  - `sh &apos;mvn [goal]&apos;` : 실행할 mvn goal 설정
  - 메이븐은 여러 플러그인으로 구성되어 있으며, 각각의 플러그인은 하나 이상의 goal(명령, 작업)을 포함하고 있다. 
  - Goal은 Maven의 실행 단위이다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Declarative Pipeline 을 Jenkins File 로 저장시켜 저장소에 저장하기
    &lt;ul&gt;
      &lt;li&gt;소스코드 저장소에 Jenkins Pipeline 스크립트를 복사&lt;/li&gt;
      &lt;li&gt;Github 에서 파일 생성 : Jenkinsfile (파일명은 고정)&lt;/li&gt;
      &lt;li&gt;Job 의 Pipeline 설정에서 Pipeline script 대신 Pipeline script from SCM 선택&lt;/li&gt;
      &lt;li&gt;Git 경로 및 Branch 설정&lt;/li&gt;
      &lt;li&gt;Script Path : jenkinsfile 위치 설정&lt;/li&gt;
      &lt;li&gt;Lightweight Checkout 설정 : jenkinsfile 먼저 checkout 한 뒤, jenkinsfile 의 모든 스테이지를 수행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>HY03</name><email>hyunik03@gmail.com</email></author><category term="Jenkins" /><category term="Coursera" /><category term="Anju M Dominic" /><category term="Jenkins" /><category term="Automating your delivery pipeline" /><category term="젠킨스" /><category term="CI/CD" /><summary type="html">후기</summary></entry></feed>