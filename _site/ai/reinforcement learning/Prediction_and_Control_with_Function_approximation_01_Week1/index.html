<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.20.2 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Prediction and Control with Function Approximation - 01. Week 1. On-policy Prediction with Approximation | Bluesplatter</title>
<meta name="description" content="강의 개요 (과정 로드맵)">


  <meta name="author" content="HY03">
  
  <meta property="article:author" content="HY03">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Bluesplatter">
<meta property="og:title" content="Prediction and Control with Function Approximation - 01. Week 1. On-policy Prediction with Approximation">
<meta property="og:url" content="http://localhost:4000/ai/reinforcement%20learning/Prediction_and_Control_with_Function_approximation_01_Week1/">


  <meta property="og:description" content="강의 개요 (과정 로드맵)">







  <meta property="article:published_time" content="2023-08-31T10:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/ai/reinforcement%20learning/Prediction_and_Control_with_Function_approximation_01_Week1/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "HY03",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Bluesplatter Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->


    
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo/logo.png" alt=""></a>
        
        <a class="site-title" href="/">
          세상에 남기는 작은 흔적
          <span class="site-subtitle">Bluesplatter</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/posts/">연도별 포스트</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="http://localhost:4000/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#ai" itemprop="item"><span itemprop="name">Ai</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#reinforcement-learning" itemprop="item"><span itemprop="name">Reinforcement learning</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Prediction and Control with Function Approximation - 01. Week 1. On-policy Prediction with Approximation</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/profile/photo.jpg" alt="HY03" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">HY03</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>저는 전문가가 아니고 이것 저것 알아보는 비전문가 입니다. <br /> 틀린 내용이 기술되어도 너그러이 봐주시고 댓글을 남겨주셨으면 좋겠습니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Republic of Korea</span>
        </li>
      

      
        
          
        
          
            <li><a href="http://bluesplatter.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
        
          
        
          
            <li><a href="https://github.com/HY03" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      
        <li>
          <a href="mailto:hyunik03@gmail.com">
            <meta itemprop="email" content="hyunik03@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">이메일</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Prediction and Control with Function Approximation - 01. Week 1. On-policy Prediction with Approximation">
    <meta itemprop="description" content="강의 개요 (과정 로드맵)">
    <meta itemprop="datePublished" content="2023-08-31T10:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Prediction and Control with Function Approximation - 01. Week 1. On-policy Prediction with Approximation
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-08-31T10:00:00+09:00">August 31, 2023</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="강의-개요-과정-로드맵">강의 개요 (과정 로드맵)</h2>

<ul>
  <li>가치함수를 테이블로 표현할 수 있는 경우</li>
</ul>

<p><img src="/assets/images/posts/categorize_reinforcement_learning_1.png" alt="categorize_reinforcement_learning_1" /></p>

<ul>
  <li>가치함수를 테이블로 표현할 수 없는 경우</li>
</ul>

<p><img src="/assets/images/posts/categorize_reinforcement_learning_2.png" alt="categorize_reinforcement_learning_2" /></p>

<h2 id="관련-자료-rlbook-pages-197-209">관련 자료 (RLbook Pages 197-209)</h2>
<ul>
  <li>On-policy Prediction with Approximation (개요)
    <ul>
      <li>이 장에서는 강화학습의 함수 근사의 학습을 다룬다.
        <ul>
          <li>함수 근사는 On-policy 데이터의 상태가치함수를 추정하기 위해 사용</li>
          <li>즉, 알고있는 정책 $\pi$ 로 생성한 경험으로 $v_\pi$ 를 근사하고자 한다.</li>
        </ul>
      </li>
      <li>여기에서 눈여겨볼 점은 가치 함수 근사는 테이블로 나타내지 않고, 파라미터화된 함수의 형태 (벡터 $\textbf{w}$ $\in \mathbb{R}^d$) 를 사용한다는 점이다.</li>
      <li>$\hat{v}(s,$ $\textbf{w}) \approx v_\pi(s)$
        <ul>
          <li>상태 $s$ 와 주어진 가중치 벡터 $\textbf{w}$ 의 근사된 값을 나타냄</li>
          <li>$\hat{v}$ 는 상태의 요소에 대한 선형함수일 수 있고, $\textbf{w}$ 는 요소에 대한 가중치 벡터일 수 있음.</li>
          <li>더 일반적으로 $\hat{v}$ 는 다중 인공 신경망 (multi-layer artificial neural network) 에 의해 계산되는 함수일 수 있고, $\textbf{w}$ 는 모든 층에 연결된 가중치일 수 있음.</li>
          <li>혹은 $\hat{v}$ 는 의사결정나무 (decision tree) 에 의해 계산되는 함수일수 있고, $\textbf{w}$ 는 모든 분기점과 트리의 리프 값을 정의하는 모든 숫자일 수 있음.</li>
        </ul>
      </li>
      <li>일반적으로 가중치의 수 ($\textbf{w}$ 의 차원 수) 는 상태의 수보다 훨씬 작고 ($d \ll | \mathcal{S} |$), 하나의 가중치의 변화는 다수의 상태의 예측값을 바꾸게 된다.
        <ul>
          <li>따라서 하나의 상태가 업데이트되면 일반화되어, 다른 다수의 상태의 값에 영향을 주게 된다.</li>
          <li>이러한 일반화 (generalization) 는 학습을 더욱 강력하게 하는 동시에 이해와 관리를 어렵게 한다.</li>
        </ul>
      </li>
      <li>놀랍게도 강화학습을 함수 근사로 확장하면 전체 상태를 사용할 수 없는 부분적으로 관찰이 가능한 문제에도 적용할 수 있다.
        <ul>
          <li>$\hat{v}$ 에 대한 파라미터화된 함수 형식이 추정값이 상태의 특정 요소에 의존하는 것을 허용하지 않으면, 이는 해당 측면을 관찰할 수 없는 것과 같다.</li>
          <li>실제로 이 책의 이 부분에 제시된 함수 근사를 사용하는 이론적 결과물은 부분적 관찰의 케이스에도 잘 적용된다.</li>
        </ul>
      </li>
      <li>그러나 함수 근사가 할 수 없는 것은 과거 관찰에 대한 기억으로 상태의 표현을 강화하는 것이다.
        <ul>
          <li>이러한 부분의 일부는 섹션 17.3 에서 간략하게 논의하기로 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Value-function Approximation
    <ul>
      <li>이 책의 모든 예측 방법 (prediction methods) 은 특정 상태의 값을 backed-up value 혹은 update target 으로 이동시키는, 추정 가치 함수 (estimated value function) 의 업데이트이다.</li>
      <li>개개의 업데이트를 표기법 $s \mapsto u$ 로 나타내도록 한다.
        <ul>
          <li>$s$ 는 상태 업데이트를 뜻하고, $u$ 는 $s$ 의 추정값이 향하는 update target 을 뜻한다.</li>
          <li>Monte Carlo update : $S_t \mapsto G_t$</li>
          <li>TD update : $S_t \mapsto R_{t+1} + \gamma \hat{v} (S_{t+1},$ ${\textbf{w}}_t)$</li>
          <li>n-step TD update : $S_t \mapsto G_{t:t+n}$</li>
          <li>DP policy-evaluation update : $s \mapsto$ $E_{\pi}$ $[ R_{t+1} + \gamma \hat{v} (S_{t+1},$ ${\textbf{w}}_t)$ $| S_t = s ]$
            <ul>
              <li>$s$ 는 임의의 상태를 뜻하며, $S_t$ 는 실제 경험에서 마주하게 된 상태를 의미한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>지금까지의 실제 업데이트는 $s$ 의 추정 값에 대한 테이블 항목이 $u$ 방향으로 약간 이동되며, 다른 모든 상태의 추정값은 변경되지 않은 채로 남아 있었음.</li>
      <li>이제는 업데이트를 구현하기 위해 임의로 복잡하고 정교한 방법을 허용, $s$ 에서의 업데이트는 다른 많은 상태의 추정 값도 변경되도록 일반화한다.
        <ul>
          <li>이러한 방식으로 입력-출력 예제를 모방하는 방법을 학습하는 기계 학습 방법을 지도 학습 방법 (Supervised learning) 이라고 한다.</li>
          <li>출력이 $u$ 와 같은 숫자인 경우 이 프로세스를 종종 함수 근사라고 한다.</li>
          <li>우리는 각 업데이트의 $s \mapsto g$ 를 훈련 예제로 전달하여 값의 예측을 위해 이러한 방법을 사용한다.</li>
          <li>그런 다음 그들이 생성하는 근사 함수를 가치 추정 함수로 본다.</li>
        </ul>
      </li>
      <li>이러한 방식으로 각 업데이트를 전통적인 학습 예제로 보면, 가치 예측을 위한 광범위한 함수 근사 방법을 사용할 수 있다.
        <ul>
          <li>원칙적으로, 인공신경망, 의사결정트리, 다양한 종류의 다변량 회귀 등 다양한 지도학습의 학습 방법을 모두 사용 가능</li>
          <li>그러나 모든 함수 근사 방법이 강화 학습에 사용하기 적합한 것은 아님.
            <ul>
              <li>정교한 인공신경망과 통계 방법은 다중 패스가 이루어지는 정적 훈련 세트를 가정한다.</li>
              <li>그러나 강화학습에서는 에이전트가 환경 혹은 환경 모델과 상호작용하는 동안 학습이 온라인으로 이루어질수 있다는 것이 중요</li>
              <li>이를 위해 점진적으로 획득된 데이터로부터 효율적으로 학습할 수 있는 방법이 필요함.</li>
            </ul>
          </li>
          <li>또한 강화학습에서는 일반적으로 비정상성 (nonstationary target) (시간에 따라 변하는 목표) 를 처리할 수 있는 함수 근사 방법이 필요함.
            <ul>
              <li>예를 들어 GPI (generalized policy iteration) 기반 방법에서는 $\pi$ 가 변경되는 동안 $q_\pi$ 를 학습하려고 하는 경우가 많음.</li>
              <li>정책이 동일하게 유지되더라도 훈련 예제의 목표 값이 부트스트래핑 방법 (DP, TD 등) 으로 생성된 경우 비정상성임.</li>
              <li>이러한 비정상성을 쉽게 처리할 수 없는 방법은 강화학습에 적합하지 않다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The Prediction Objective ($\overline{VE}$)
    <ul>
      <li>예측을 위한 명시적인 목표
        <ul>
          <li>Tabular case 의 경우
            <ul>
              <li>예측을 위한 명시적인 목표를 지정하지 않았음.</li>
              <li>학습된 가치함수가 실 가치함수와 정확하게 동일해질 수 있기 때문에 예측 품질에 대한 지속적인 측정이 필요하지 않음.</li>
              <li>각 상태에서 학습된 값들은 서로 분리되어있었음 (한 상태의 업데이트가 다른 상태에 영향을 주지 않음)</li>
            </ul>
          </li>
          <li>Function Approximation 의 경우
            <ul>
              <li>근사를 사용하면 한 상태의 업데이트가 다른 많은 상태에 영향을 미치므로 모든 상태의 값을 정확하게 얻는 것은 불가능</li>
              <li>가중치보다 훨씬 더 많은 상태가 있으므로, 한 상태의 추정치를 정확하게 만드는 것은 다른 상태의 추정치를 덜 정확하게 만드는 것을 의미</li>
              <li>이는 어떤 상태에 집중할 지를 정해야 함을 의미</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Mean Squared Value Error, $\overline{VE}$
        <ul>
          <li>즉, $\mu (s)$ 를 정의해야 함.
            <ul>
              <li>상태의 분포 $\mu (s) \geq 0, \sum_s \mu(s) = 1$</li>
              <li>각 상태 $s$ 의 오차에 대해 얼마나 신경 써야할지를 의미</li>
            </ul>
          </li>
          <li>상태 $s$ 에서의 오차는 근사값 $\hat{v} (s,$ $\textbf{w})$ 와 참 값 $v_\pi(s)$ 의 차의 제곱을 의미한다.
            <ul>
              <li>이를 각 상태별 $\mu$ 를 이용해 가중치를 부여하면 아래의 식이 도출됨.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/9_2_1_VE.png" alt="9_2_1_VE" /></p>

        <ul>
          <li>$\overline{VE}($$\textbf{w}) \doteq \sum_{s \in S} \mu(s)$ $[ v_\pi(s) - \hat{v} (s,$ $\textbf{w}) ]^2$
            <ul>
              <li>이 측정값의 제곱근, root $\overline{VE}$ 는 근사값과 실제 값이 얼마나 다른지에 대한 대략적 측정값을 제공하며 도표에서 자주 사용됨.</li>
              <li>위 식에 이미 상태의 분포 값이 들어가 있기에, MSE 값과 연관이 있다고 할 수 있음.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$\mu (s)$ 에 대해
        <ul>
          <li>종종 $\mu (s)$ 는 상태 $s$ 에서 보낸 시간의 비율값이 선택됨.</li>
          <li>on-policy 학습에서 이는 on-policy distribution(분포) 라고 하며, 이 장에서는 이 케이스에 초점을 맞춤</li>
          <li>Continuing tasks 에서 on-policy distribution 은 $\pi$ 아래에서 정상성 분포이다.
            <ul>
              <li>정상성 분포를 가진다고 가정하는 것.</li>
              <li>에피소드에서 무한한 상태를 수집할 수는 없으니, 일정 정도를 수집하여 분포를 구하고 이 분포가 정상 분포인 것을 가정하는 것이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>The on-policy distribution in episodic tasks</p>

        <p><img src="/assets/images/posts/9_2_2_on_policy_distribution.png" alt="9_2_2_on_policy_distribution" /></p>

        <ul>
          <li>에피소드 작업에서 on-policy distribution 은 에피소드의 초기 상태가 선택되는 방식에 따라 약간 다르다.</li>
          <li>$\eta(s) = h(s) + \sum_{\bar{s}} \eta(\bar{s}) \sum_a \pi (a | \bar{s} ) p(s | \bar{s},a)$, for all $s \in S$. (9.2)
            <ul>
              <li>$h(s)$ : 각 상태 s 가 첫 에피소드 시작일 확률</li>
              <li>$\eta(s)$ : 에피소드에서 상태 $s$ 에 머무는 비율 (s에 방문하는 time step 수)</li>
              <li>에피소드가 $s$ 에서 시작하거나 이전 상태 $\bar{s}$ 에서 $s$ 로 전이가 일어난 경우</li>
              <li>위 식은 이전 상태의 time-step 비율을 이용해 현 상태의 비율을 구하는 식이다.</li>
            </ul>
          </li>
          <li>$\eta(s)$ 를 정규화 (on-policy distribution)
            <ul>
              <li>$\mu(s) = \frac{\eta(s)}{\sum_{s’} \eta(s’)}$, for all $s \in S$. (9.3)
                <ul>
                  <li>$s’$ : 상태 $s$ 를 포함한 다른 모든 상태를 의미</li>
                </ul>
              </li>
              <li>이것은 할인이 없는 경우에 대한 것임.
                <ul>
                  <li>할인이 있는 경우 9.2의 식의 두번째 항에 $\gamma$ 를 적용해야 함.</li>
                  <li>여기에서 할인의 역할은 에피소드의 확률적인 시작과 종료를 다루기위한 요소로 사용되는 것임.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>연속적인 케이스와 에피소드 케이스 두 가지의 경우 비슷하게 동작하나, 근사하는 경우 완전히 분리해서 생각해야 한다. (향후 언급)</li>
      <li>위의 $\overline{VE}$ 로 학습목표 지정이 완료
        <ul>
          <li>그러나 $\overline{VE}$ 가 강화학습에 적합한 성능의 목표인지 명확하지 않음.</li>
          <li>가치함수를 학습하는 궁극적인 이유는 더 나은 정책을 찾는 것임.
            <ul>
              <li>이 목적을 위한 최고의 가치함수가 반드시 $\overline{VE}$ 를 최소화하는데 최적인 것은 아님.</li>
            </ul>
          </li>
          <li>지금은 $\overline{VE}$ 에 중점을 둔다.</li>
        </ul>
      </li>
      <li>$\overline{VE}$ 학습목표의 이상적인 목표와 현실적 목표
        <ul>
          <li>이상적인 목표 : 가능한 모든 $\textbf{w}$ 에 대해 $\overline{VE}(w^{*}) \leq \overline{VE}(w)$ 인 전역 최적의 가중치 벡터 $\textbf{w*}$ 를 찾는 것</li>
          <li>위 목표를 찾는 것은 선형과 같은 간단한 함수 근사에서는 가능하나, 인공신경망이나 의사결정트리와 같은 복잡한 함수근사에서는 거의 불가능하다.
            <ul>
              <li>복잡한 함수근사에서는 이에 미치지 못하는, 지역 최적, $\textbf{w*}$ 와 유사한 수렴값을 찾게 된다.</li>
              <li>즉, 모든 상태에 대해 오차를 구해 합한 값 $\overline{VE}$ 의 변수 $\textbf{w}$ 에 대해, 전역최소값을 가지는 $\textbf{w}$ 는 구하기 힘들다는 이야기이다.</li>
            </ul>
          </li>
          <li>이러한 수렴의 보장은 약간만 안심할 수 있지만, 일반적으로 비선형 함수 근사의 가능한 최선으로 여겨지고, 대부분의 경우 이정도로 충분하다.
            <ul>
              <li>강화학습의 관심있는 많은 케이스의 경우 최적에 대한 수렴을 보장하지 않고, 또한 최적치에 대한 지정된 범위 내의 수렴 또한 보장하지 않는다.</li>
              <li>어떠한 방법의 경우 $\overline{VE}$ 로의 무한한 접근이 한계치를 넘어 발산하기도 한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>마지막 두 섹션은 광범위한 가치 추정을 위한 강화학습 방법에 광범위한 함수 근사 방법을 결합하는 프레임워크이다.
        <ul>
          <li>전자의 업데이트를 사용하여 후자에 대한 훈련 예제를 생성한다.</li>
          <li>또한 $\overline{VE}$ 성능 측정을 통한 최소화의 촉진에 대해서 살펴본다.</li>
          <li>가능한 함수 근사 방식의 범위는 모두 다루기에 광범위하며, 대부분의 경우 신뢰할 수 있는 평가나 권장사항을 만들기에 알려진 정보가 너무 적다.
            <ul>
              <li>우리는 몇 가지 가능성에 대해서만 살펴본다.</li>
            </ul>
          </li>
          <li>이 챕터의 나머지 부분에서 경사 원리를 기본으로 한 함수 근사 방법에 집중하며, 이 중에서도 선형 경사 하강법에 중점을 둔다.
            <ul>
              <li>우리는 이러한 방법이 특히 유망하다고 생각하며, 핵심적인 이론적 문제를 드러낸다고 생각한다. (또한 단순하기도 하다.)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Stochastic-gradient and Semi-gradient Methods
    <ul>
      <li>Stochastic Gradient Descent (SGD) : 확률적 경사하강법
        <ul>
          <li>모든 함수 근사화 방법 중 가장 널리 사용되는 방법으로 온라인 강화 학습에 특히 적합함</li>
          <li>확률적이라는 뜻은, 모든 표본을 구해 경사하강법을 진행하는 것이 아닌, 일부 표본 데이터(상태)에 대해서만 적용한다는 의미임.</li>
          <li>경사 하강법에서 가중치 벡터는 고정된 실수 값을 구성요소로 가진 열 벡터이다.
            <ul>
              <li>$\textbf{w} \doteq (w_1, w_2, …, w_d)^\top$
                <ul>
                  <li>여기서 $\top$ 는 전치행렬을 의미한다. 수평의 row 벡터를 수직의 column 벡터로 전환하는 것을 의미</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>근사가치함수 $\hat{v} (s, \textbf{w} )$ 는 모든 $s \in S$ 에 대한 $\textbf{w}$ 의 미분 가능한 함수이다.</li>
          <li>우리는 $\textbf{w}$ 를 매 이산 time-step 의 시리즈 $t = 0,1,2,3,…$ 각각에 업데이트를 할 것이므로, $\textbf{w}_t$ 라는 표기법이 필요하다.</li>
          <li>지금부터 우리는 각 스텝에서 새 예시 $S_t \mapsto v_\pi (S_t)$ (아마도 무작위로 선택된 $S_t$ 와 정책 하의 참 값) 를 관측한다고 가정한다.
            <ul>
              <li>위의 상태 값은 환경과의 상호작용을 통한 연속적인 상태가 되겠지만, 여기에서는 그렇게 가정하지 않는다.</li>
            </ul>
          </li>
          <li>비록 각 상태 $S_t$ 에 정확하게 옳은 참 값 $v_\pi (S_t)$ 이 제공된다 해도 여전히 어려운 문제가 있다.
            <ul>
              <li>우리의 함수 근사기는 한정된 자원과 그로 인한 한정된 해결책을 가지고 있다.</li>
              <li>특히 일반적으로 모든 상태를 가지거나, 정확히 옳은 샘플들을 가지는 $\textbf{w}$ 는 존재하지 않는다.</li>
              <li>추가적으로 우리는 예제에 보여지지 않은 다른 모든 상태에 대해서도 일반화를 해야 한다.</li>
            </ul>
          </li>
          <li>우리는 (9.1) 에 주어진 $\overline{VE}$ 를 최소화 하는동안 각각의 상태가 동일 분포 $\mu$ 로 나타난다고 가정한다.
            <ul>
              <li>이러한 케이스에서 좋은 전략은 관측된 예제에서 에러값을 최소화하는 것을 시도해보는 것이다.</li>
            </ul>
          </li>
          <li>
            <p>확률적 경사 하강법 (SGD, Stochastic Gradient Descent) 은 각 예제 후에 해당 예제에서의 에러를 가장 많이 줄이는 방향으로 가중치 벡터를 소량 조정함으로서 위 방법을 수행한다.</p>

            <p><img src="/assets/images/posts/9_3_1_sgd_vector_adjust_1.png" alt="9_3_1_sgd_vector_adjust_1" /></p>

            <ul>
              <li>$\alpha$ : 양수의 step-size 파라미터</li>
              <li>$w_{t+1} \doteq w_t - \frac{1}{2} \alpha \nabla [  v_\pi(S_t) - \hat{v}(S_t, w_t) ]^2$
                <ul>
                  <li>$v_\pi (S_t)$ : 상태 $S_t$ 에서의 실 가치값</li>
                  <li>$\hat{v} (S_t, w_t)$ : 함수 근사를 통해 예측된 가치</li>
                  <li>손실함수 $\textbf{L}$
                    <ul>
                      <li>$\frac{1}{2} [ v_\pi(S_t) - \hat{v}(S_t, w_t) ]^2$</li>
                      <li>분산 : 편차의 제곱 평균</li>
                    </ul>
                  </li>
                  <li>즉, 위 식은 가중치 벡터를 학습률을 반영한 손실함수의 경사만큼 뺀 값
                    <ul>
                      <li>손실함수의 값을 최소화 하는 방향으로 이동 (기울기의 반대방향)</li>
                    </ul>
                  </li>
                  <li>즉, 위 식에서 구하고자 하는 값은 $w_t -\alpha \nabla \textbf{L}$ 이 된다.</li>
                </ul>
              </li>
              <li>$= w_t + \alpha [ v_\pi (S_t) - \hat{v} (S_t, w_t) ] \nabla \hat{v} (S_t, w_t)$
                <ul>
                  <li>위 식 중 $\nabla \textbf{L}$ 을 구한다는 것의 의미는 손실함수에 대한 $w_t$ 의 기울기 함수를 구한다는 의미 (편미분)</li>
                  <li>$\textbf{L}$ 은 $v_\pi (S_t)$ 와 $\hat{v}(S_t,w_t)$ 의 함수이고, 후자는 $w_t$ 에 의존한다.</li>
                  <li>$\frac{\partial L}{\partial w_t} = \frac{\partial L}{\partial \hat{v}(S_t,w_t)} \cdot \frac{\partial \hat{v}(S_t,w_t)}{\partial w_t}$</li>
                  <li>각 항에 대한 미분
                    <ul>
                      <li>$\frac{\partial L}{\partial \hat{v}(S_t,w_t)} = 2 \cdot \frac{1}{2} [ v_\pi (S_t) - \hat{v}(S_t,w_t) ] \cdot (-1) = \hat{v}(S_t,w_t)-v_\pi (S_t)$</li>
                      <li>$\frac{\partial \hat{v}(S_t,w_t)}{\partial w_t}$</li>
                    </ul>
                  </li>
                  <li>즉, $[ \hat{v} (S_t, w_t) - v_\pi (S_t) ] \nabla \hat{v} (S_t, w_t)$ 이 된다.</li>
                </ul>
              </li>
            </ul>

            <p><img src="/assets/images/posts/9_3_2_sgd_vector_adjust_2.png" alt="9_3_2_sgd_vector_adjust_2" /></p>

            <ul>
              <li>$f( \textbf{w} )$ : 벡터 w 의 함수.</li>
              <li>$\nabla f( \textbf{w} )$ : 벡터의 구성요소에 대한 편미분
                <ul>
                  <li>위 식은 벡터 $\textbf{w}$ 에 대한 함수 $f( \textbf{w} )$ 를 미분하려면 벡터의 구성요소들로 편미분 해야 함을 나타낸다.</li>
                </ul>
              </li>
              <li>이 파생 벡터는 $\textbf{w}$ 에 대한 $f$ 의 경사이다.</li>
            </ul>
          </li>
          <li>SGD 방식은 경사 하강 (gradient descent) 방식이라 하는데, $w_t$ 의 전반적인 단계가 예시의 제곱 에러 (9.4) 의 음의 기울기 값에 비례하기 때문
            <ul>
              <li>이는 오차가 가장 빠르게 감소하는 방향이다.</li>
            </ul>
          </li>
          <li>확률적으로 선택되었을 수 있는 단일 예에서 업데이트가 완료되면 경사 하강법을 확률적이라고 한다.
            <ul>
              <li>많은 예를 통해 작은 단계만큼 수행하다 보면, 전반적인 효과는 $\overline{VE}$ 와 같은 평균 측정값을 최소화할 수 있다.</li>
              <li>한번에 큰 단계로 움직이거나 경사 방향으로 완전히 이동해 해당 예시에서의 에러를 완전히 없앨 수는 없다.
                <ul>
                  <li>모든 상태에 대해 에러가 없을 수 없고, 각각의 다른 상태에서 오직 에러의 균형을 근사화할 수밖에 없기 때문.</li>
                </ul>
              </li>
              <li>$\alpha$ 의 값이 충분히 작다면, SGD 방식은 지역 최적값으로 수렴하는 것을 보장한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Stochastic Gradient Descent (SGD) 의 Bootstrapping 개념에 대해</p>

        <ul>
          <li>t 번째 학습 예제의 목표값을 $U_t \in \mathbb{R}$ 로 정의하면, $S_t \mapsto U_t$ 가 되며, 이때 $v_\pi (S_t)$ 의 참 값은 될 수 없지만, 임의의 근사치일 수는 있다.
            <ul>
              <li>예를 들어 $U_t$ 는 $v_\pi (S_t)$ 의 노이즈 버전일 수 있고, 혹은 $\hat{v}$ 의 부트스트래핑된 목표일 수 있다.</li>
              <li>우리는 이 경우 (9.5) 의 정확한 업데이트를 수행할 수 없으나, $v_\pi (S_t)$ 를 $U_t$ 로 대체함으로써 근사 업데이트를 진행할 수 있다.</li>
            </ul>

            <p><img src="/assets/images/posts/9_3_3_sgd_u_t_update.png" alt="9_3_3_sgd_u_t_update" /></p>

            <ul>
              <li>만약 $U_t$ 가 편향되지 않은 추정값이라면, 만약 각각의 $t$ 에 $\mathbb{E} [U_t | S_t=s ] = v_\pi (S_t)$ 라면 $w_t$ 는 지역 최적에 수렴함을 보장한다. ($\alpha$ 가 충분히 작을 경우)
                <ul>
                  <li>예를 들어 예시의 상태가 정책 $\pi$ 하에 환경과 상호작용 (혹은 시뮬레이션) 된 것이라면, 상태의 참 값의 정의는 리턴 값의 추측 값이기에, 몬테카를로 목표 $U_t \doteq G_t$ 는 편향이 없는 $v_\pi (S_t)$ 의 추정치이다.</li>
                </ul>
              </li>
              <li>따라서 몬테카를로 상태값 예측의 경사하강법 버전은 국소적으로 최적의 솔루션을 찾는 것이 보장된다.</li>
            </ul>
          </li>
          <li>
            <p>gradient monte carlo algorithm 의 psuedo code</p>

            <p><img src="/assets/images/posts/9_3_4_gradient_monte_carlo_algorithm_psuedo_code.png" alt="9_3_4_gradient_monte_carlo_algorithm_psuedo_code" /></p>

            <ul>
              <li>$\hat{v} : S \times \mathbb{R}^d \to \mathbb{R}$
                <ul>
                  <li>$S$ : 상태 공간을 의미</li>
                  <li>$\mathbb{R}^d$ : 실수의 $d$ 차원 벡터 공간을 의미</li>
                  <li>$\mathbb{R}$ : 실수를 의미</li>
                  <li>따라서 $\hat{v}$ 함수는 상태 $s \in S$ 와 실수 벡터 $w \in \mathbb{R}^d$ 의 입력을 받아 실수를 출력한다는 의미이다.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Semi-gradient (bootstrapping) methods
            <ul>
              <li>$v_\pi (S_t)$ 의 부트스트래핑 추정 (9.7) 에서 목표가 $U_t$ 로 사용되는 경우 동일한 보장을 얻지 못한다.
                <ul>
                  <li>부트스트래핑 목표 n-step 반환값 $G_{t:t+n}$ 또는 DP 목표 $\sum_{a,s’,r} \pi (a|S_t) p(s’,r | S_t,a) [ r+\gamma \hat{v} (s’,w_t) ]$ 모두 가중치 벡터 $w_t$ 의 현재 값에 따라 달라짐. 이는 해당 대상이 편향될 것이며 실제 경사하강법을 생성하지 않음을 의미</li>
                  <li>이는 (9.4), (9.5) 의 식에서 주요 단계가 $w_t$ 와 독립적인 대상에 의존하는 것에서 알 수 있음.</li>
                  <li>따라서 $v_\pi (S_t)$ 대신 부트스트래핑 추정이 사용된 경우 이 단계는 유효하지 않게 됨.</li>
                  <li>가중치 벡터 $w_t$ 를 변경하면 추정치에 미치는 영향을 고려하지만, 목표에 미치는 영향은 무시함.</li>
                </ul>
              </li>
              <li>따라서 부트스트래핑 방법은 실제로 진정한 경사하강법 사례가 아니며, 경사의 일부만 포함되므로 Semi-gradient 방법이라고 한다.</li>
            </ul>
          </li>
          <li>Semi-gradient (bootstrapping) methods 의 장점
            <ul>
              <li>Semi-gradient (bootstrapping) 방법은 gradient 방법만큼 견고하게 수렴하지는 않지만, 다음 섹션에서 설명하는 선형 사례와 같은 중요한 경우 안정적으로 수렴한다.</li>
              <li>6장과 7장에서 살펴본 것처럼 일반적으로 훨씬 더 빠른 학습을 가능하게 한다.</li>
              <li>에피소드가 끝날 때까지 기다리지 않고도 학습이 지속적이고 온라인으로 이루어질 수 있다.</li>
            </ul>
          </li>
          <li>Semi-gradient TD(0) 의 psuedo code
            <ul>
              <li>목표값 : $U_t \doteq R_{t+1} + \gamma \hat{v} (S_{t+1},w)$</li>
            </ul>

            <p><img src="/assets/images/posts/9_3_5_semi_gradient_td_0_psuedo_code.png" alt="9_3_5_semi_gradient_td_0_psuedo_code" /></p>
          </li>
        </ul>
      </li>
      <li>State Aggregation (Chat GPT 검색내용)
        <ul>
          <li>강화학습에서 Function Approximation을 사용하는 방법 중 하나로 “State Aggregation”이라는 기법이 있다.
            <ul>
              <li>이 방법은 상태 공간(state space)을 더 작은 그룹으로 나누는 것을 의미</li>
              <li>각 그룹은 하나의 상태로 취급되며, 이 방법은 연속적인 상태 공간을 이산적으로 변환하여 함수 근사를 적용할 때 유용</li>
            </ul>
          </li>
          <li>주요 특징과 원리
            <ul>
              <li>상태 그룹화 (State Aggregation): State Aggregation은 상태 공간을 작은 그룹 또는 상태 집합으로 나누는 프로세스. 연속적인 상태를 몇 가지 이산적인 상태로 집계하는 것과 유사</li>
              <li>상태 유사성: 유사한 특성 또는 관찰을 갖는 상태들이 동일한 그룹으로 집계. 이러한 유사성은 도메인 지식 또는 경험에 근거하여 결정</li>
              <li>함수 근사 적용: 이제 상태 그룹화를 통해 작은 상태 집합을 얻었으므로, 각 그룹에 대한 가치 함수를 추정할 수 있음.
                <ul>
                  <li>Function Approximation 기법 중 하나인 선형 함수 근사(linear function approximation) 또는 비선형 함수 근사를 사용하여 각 그룹의 가치 함수를 근사</li>
                </ul>
              </li>
              <li>업데이트 규칙: 상태 그룹화를 사용하면 상태 공간이 단순화되므로 각 그룹의 가치 함수 업데이트가 빨라질 수 있음. 에이전트는 각 상태 그룹의 가치 함수를 개별적으로 업데이트함.</li>
              <li>함수 근사 오류: 그러나 이런 단순화는 정보 손실을 의미하기도 한다. 각 상태 그룹 내에서 상태의 차이나 다양성을 잃을 수 있음. 따라서, State Aggregation을 사용할 때 어떤 정보 손실이 발생할 수 있음을 염두에 두어야 한다.</li>
            </ul>
          </li>
          <li>주의점
            <ul>
              <li>State Aggregation은 강화학습에서 상태 공간이 커서 함수 근사를 적용하기 어려운 경우 유용한 방법 중 하나이다.</li>
              <li>그러나 주의해야 할 점은 그룹화 과정에서 상태 유사성을 올바르게 판단하고, 정보 손실을 최소화하도록 설계해야 한다는 것이다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>예제 9.1: State Aggregation on the 1000-state Random Walk
        <ul>
          <li>문제의 가정
            <ul>
              <li>1000 개의 상태를 가진 random walk (예제 6.2, 예제 7.1 참고) 문제를 가정한다.</li>
              <li>상태 : 왼쪽에서 오른쪽으로 1 부터 1000까지 넘버링된 상태, 모든 에피소드는 중앙 (상태 500) 에서 시작</li>
              <li>상태 전이 : 현 상태 값에서 좌측 100 개, 우측 100개의 상태 내의 값 (전이 확률은 모두 동일)</li>
              <li>현 상태가 끝쪽에 가깝다면, 100 개의 이웃 상태보다 더 적은 상태가 있을 것이고, 이 경우 종료 상태에 확률이 합산된다.
                <ul>
                  <li>상태 1에서 왼쪽으로 이동해 종료될 확률 (상태 0) : 0.5</li>
                  <li>상태 950 에서 오른쪽이로 이동해 종료될 확률 (상태 1001) : 0.25</li>
                </ul>
              </li>
              <li>왼쪽 상태 (0) 에서 끝날 경우 보상은 -1, 오른쪽 상태 (1001) 에서 끝날 경우 보상 +1, 기타 다른 상태에서의 보상은 0</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/9_3_6_figure_9_1_1000_state_random_walk_state_aggregation.png" alt="9_3_6_figure_9_1_1000_state_random_walk_state_aggregation" /></p>

        <ul>
          <li>결과의 해석
            <ul>
              <li>Figure 9.1 은 이 작업의 참 가치 함수 $v_\pi$ 를 보여준다.
                <ul>
                  <li>이것은 거의 직선이지만, 종료상태 옆 마지막 100 개의 상태에서 수평으로 굽는 형상이다.</li>
                </ul>
              </li>
              <li>또한 state aggregation 을 한, gradient Monte-Carlo 알고리즘을 통한 최종 근사 가치함수를 보여준다.
                <ul>
                  <li>100000 에피소드, step size $\alpha = 2 \times 10^{-5}$</li>
                  <li>state aggregation 은 1000 개의 상태에서 100개의 상태 묶음 10개 그룹으로 진행 (1-100, 101-200, …)</li>
                </ul>
              </li>
              <li>계단 형태의 결과는 전형적인 state aggregation 의 효과이다.
                <ul>
                  <li>각 그룹에서 근사 가치는 상수이며, 하나의 그룹에서 다음 그룹으로 이동 시 이 상수 값이 급격히 변한다.</li>
                  <li>이 근사 값은 $\overline{VE}$ 의 전역 최소값 (9.1) 과 가깝다.</li>
                </ul>
              </li>
              <li>근사값의 일부는 상태 분포 $\mu$ 에 영향을 받게 된다.
                <ul>
                  <li>예를 들어 가장 극단의 값 (1) 보다 가장 왼쪽 그룹 중 상태 (100) 이 3배 이상 더 강한 가중치가 부여된다.</li>
                  <li>따라서 그룹에 대한 추정치는 상태 (1) 의 참 값보다 상태 (100) 의 참 값 쪽으로 편향되게 된다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Linear Methods
    <ul>
      <li>가치함수가 가중치에 대해 선형인 경우
        <ul>
          <li>함수 근사의 가장 중요한 특수 사례 중 하나는 근사함수 $\hat{v} (\cdot , \textbf{w} )$ 가 가중치 벡터 $\textbf{w}$ 에 대해 선형함수인 경우이다.</li>
          <li>모든 상태에 대응하는, $\textbf{w}$ 와 동일한 수의 구성요소를 가진, 실수 값 벡터 $\textbf{x} (s) \doteq (x_1(s), x_2(s), …, x_d(s))^\top$ 가 있다.</li>
          <li>이때, 선형 근사 상태-가치 함수는 $\textbf{w}$ 와 $\textbf{x} (s)$ 의 내적으로 구한다.</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_1_linear_approximate_inner_product_between_w_and_x.png" alt="9_4_1_linear_approximate_inner_product_between_w_and_x" /></p>

        <ul>
          <li>이러한 경우, 근사가치함수는 가중치에 대해 선형이다, 혹은 선형이다 라고 표현한다.</li>
        </ul>
      </li>
      <li>feature vector
        <ul>
          <li>벡터 $\textbf{x} (s)$ 는 상태 s의 feature vector 라 한다.
            <ul>
              <li>$\textbf{x} (s)$ 의 각각의 구성요소 $x_i (s)$ 는 $x_i : \mathcal{S} \to \mathbb{R}$ 이다.</li>
            </ul>
          </li>
          <li>선형 방법에서 feature 는 근사함수 세트에서 선형 기반을 형성하므로, basis function (기저함수) 가 된다.
            <ul>
              <li>즉, 각각의 feature 가 각각의 차원을 이룬다.</li>
              <li><em>d</em>-dimensional feature vector 를 생성하는 것은 <em>d</em> basis function 을 선택하는 것과 같다.</li>
              <li>features 들은 많은 서로 다른 방식으로 정의되며, 우리는 다음 장에서 몇 가지 가능성에 대해 다룬다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>선형 가치근사함수와 SGD
        <ul>
          <li>선형 가치근사에서 SGD 업데이트를 사용하는 것은 자연스럽다.
            <ul>
              <li>$\nabla \hat{v}(s,\textbf{w}) = \textbf{x}(s)$</li>
            </ul>
          </li>
          <li>이는 선형 근사 상태-가치 함수가 $\textbf{w}$ 와 $\textbf{x} (s)$ 의 내적으로 구하기 때문임.</li>
          <li>그러므로 선형 케이스의 경우 SGD 업데이트 (9.7) 가 간단한 형태로 바뀌게 된다.
            <ul>
              <li>$w_{t+1} \doteq w_t + \alpha [ U_t - \hat{v}(S_t,w_t) ] \textbf{x} (S_t)$</li>
            </ul>
          </li>
          <li>매우 간단하기 때문에 선형 SGD 사례는 수학적 분석에 가장 유리한 사례 중 하나이다.</li>
          <li>모든 종류의 학습 시스템에 대한 거의 모든 유용한 수렴 결과는 선형(또는 더 간단한) 함수 근사 방법에 대한 것이다.
            <ul>
              <li>(이 주장은 선형 가치 함수 근사가 간단하면서도 많은 문제에서 효과적으로 동작한다는 아이디어를 강조한 것입니다. 하지만 모든 문제에 대해 선형 근사가 항상 최적인 것은 아닙니다. 일부 문제에서는 더 복잡한 함수 근사가 필요할 수 있습니다.)</li>
            </ul>
          </li>
          <li>선형의 경우 단 하나의 최적값 (혹은 최적값의 한 세트) 만 있으므로 지역최적값이나 전역최적값에 수렴하는 것이 자동적으로 보장된다.
            <ul>
              <li>즉, 근사함수가 선형인 경우 $\frac{1}{2} [ v_\pi(s) - \hat{v}(s,w) ]^2$ 오차 값이 볼록한 형태를 가지고 하나의 최적값이 존재하게 된다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>선형 가치함수근사와 Gradient Monte Carlo 알고리즘의 경우
        <ul>
          <li>예를 들어 이전 섹션에서 제시된 gradient Monte Carlo 알고리즘은 시간이 지남에 따라 $\alpha$ 가 감소하면 선형함수근사 하에 $\overline{VE}$ 의 전역최적으로 수렴한다.</li>
        </ul>
      </li>
      <li>선형 가치함수근사와 semi-gradient TD(0) 알고리즘의 경우
        <ul>
          <li>마찬가지로 앞서 다룬 semi-gradient TD(0) 도 선형함수근사 하에 수렴하나, SGD 의 일반적 결과와는 다르다.
            <ul>
              <li>가중치 벡터는 전역최적이 아닌 지역최적에 수렴한다. (특히 연속적인 케이스에서 중요)</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/9_4_2_update_semi_gradient_td_0.png" alt="9_4_2_update_semi_gradient_td_0" /></p>

        <ul>
          <li>여기에서 우리는 축약 표기 $\textbf{x}_t = \textbf{x}(S_t)$ 를 사용한다.</li>
          <li>시스템이 정상 상태에 도달하면 어떠한 $\textbf{w}_t$ 에 대해서도 다음 가중치 벡터를 아래와 같이 구할 수 있다.</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_3_semi_gradient_td_0_td_fixed_point_1.png" alt="9_4_3_semi_gradient_td_0_td_fixed_point_1" />
  <img src="/assets/images/posts/9_4_4_semi_gradient_td_0_td_fixed_point_2.png" alt="9_4_4_semi_gradient_td_0_td_fixed_point_2" /></p>

        <ul>
          <li>위의 값을 TD fixed point 라고 한다.
            <ul>
              <li>선형 semi-gradient TD(0) 는 이 지점으로 수렴한다.</li>
            </ul>
          </li>
          <li>위 수식에 대한 이해 (Chat GPT 로 정리한 내 생각)
            <ul>
              <li>즉 Semi-gradient 방식은 업데이트 식의 타겟에 학습 중인 가중치 벡터 $w$ 가 포함되어 수렴이 늦어지나</li>
              <li>근사함수가 선형일 경우 현재 목표 $w_{TD}$ 를 아래의 요소로 계산하여 구할 수 있으므로
                <ul>
                  <li>$\textbf{x}_t$</li>
                  <li>$\textbf{x}_{t+1}$</li>
                  <li>$R_{t+1}$</li>
                </ul>
              </li>
              <li>위 요소에 따라 $w_{TD}$ 의 값이 바뀔 수 있으나 학습해야 할 파라미터 $w$ 가 목표에 없으므로 더 빠르고 안정적인 수렴이 가능하다는 이야기.</li>
            </ul>
          </li>
          <li>Proof of Convergence of Linear TD(0)
            <ul>
              <li>아래의 내용을 이해해보려 하였으나, 너무 복잡하여 본문 내용을 붙여넣는 것으로 갈음한다.</li>
            </ul>

            <p><img src="/assets/images/posts/9_4_5_proof_of_convergence_of_linear_td_0_1.png" alt="9_4_5_proof_of_convergence_of_linear_td_0_1" />
  <img src="/assets/images/posts/9_4_5_proof_of_convergence_of_linear_td_0_2.png" alt="9_4_5_proof_of_convergence_of_linear_td_0_2" /></p>

            <ul>
              <li>마지막으로, 이것은 TD(0) 알고리즘이 수렴할 때 가중치 벡터 $w_{TD}$ 에서의 에러가 전체 상태 공간에서의 최적 에러 $w^*$ 에서의 에러에 대한 한계로 나타나며, $\gamma$ 가 충분히 작을 때 $w_{TD}$ 의 에러가 최적 에러에 근접함을 보여준다.
                <ul>
                  <li>$\overline{VE}(w_{TD}) \leq \frac{1}{1-\gamma} \min_w \overline{VE}(w)$</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>가능한 가장 작은 에러 값에 대한 $\overline{VE}$ 의 경계치</p>

            <p><img src="/assets/images/posts/9_4_6_td_fixed_point_ve_error_bounded.png" alt="9_4_6_td_fixed_point_ve_error_bounded" /></p>

            <ul>
              <li>Chat GPT 로 내가 확인한 내용
                <ul>
                  <li>$w_{TD}$ 는 부트스트래핑으로 얻은 특정 상태에 대한 추정값으로, 실제 값과는 오차가 있을 수 있다.</li>
                  <li>$w$ 는 $w_{TD}$ 와는 별개로, 전체 상태에 대한 가중치 값을 학습하는 것으로, 각 상태의 $w_{TD}$ 로부터 추정된 값과 $w$ 로 계산된 값 사이의 오차 합을 최소화하려고 노력한다.</li>
                  <li>부등식 $\overline{VE}(w_{TD}) \leq \frac{1}{1-\gamma} \min_w \overline{VE}(w)$ 는 $\gamma$ 값이 충분히 작을 때 $w_{TD}$ 의 각 상태에 대한 평균 에러가 전역최적 $w$ 의 평균 에러 값으로 계산된 경계 영역보다 작음을 보장한다는 내용이다.</li>
                </ul>
              </li>
              <li>즉, TD 방법은 $\gamma$ 값이 1에 가까워지면, 점근 성능에 상당한 잠재적 손실이 있을 수 있으나, 종종 몬테카를로 방법에 비해 분산이 크게 줄어들어 더 빠르게 수렴할 수 있다.</li>
              <li>9.14 와 유사한 경계는 다른 정책 부트스트래핑 방법에도 작용되며, Sarsa(0) 와 같은 방법이나 에피소드 작업과 같은 방식도 유사한 경계값이 있음.
                <ul>
                  <li>보상, 기능, step-size 매개변수 감소에 대한 몇 가지 기술적 조건이 있으나 여기에서는 생략함.</li>
                </ul>
              </li>
              <li>이러한 수렴 결과에서 중요한 것은 상태가 정책 분포에 따라 업데이트된다는 것이다.
                <ul>
                  <li>다른 업데이트 분포의 경우 함수 근사를 사용하는 부트스트래핑 방법은 무한대로 발산할 수 있다.</li>
                  <li>이에 대한 예와 가능한 해결 방법은 11장에서 다룬다.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>예제 9.2 : Bootstrapping on the 1000-state Random Walk
        <ul>
          <li>이 장에서 관찰한 내용 중 일부를 설명하기 위해 1000개 상태의 랜덤 워크의 예제를 사용 (State aggregation + 선형함수근사)</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_7_figure_9_2_bootstrapping_with_state_aggregation_1000_step_random_walk_example_result.png" alt="9_4_7_figure_9_2_bootstrapping_with_state_aggregation_1000_step_random_walk_example_result" /></p>

        <ul>
          <li>왼쪽 패널은 예제 9.1 과 동일한 State aggregation 을 사용, Semi-gradient TD(0) 에 의한 학습된 최종 값 함수를 보여줌
            <ul>
              <li>TD 근사가 몬테카를로 근사보다 실제 값에서 더 멀다는 것을 알 수 있음</li>
            </ul>
          </li>
          <li>오른쪽 패널은 그림 7.2 (n-step semi-gradient TD + state aggregation) 와 유사한 결과를 보여준다.
            <ul>
              <li>함수근사를 한 것과, tabular 계산을 한 것의 유사성을 보여줌</li>
            </ul>
          </li>
          <li>semi-gradient n-step TD 알고리즘은 tabular n-step TD 알고리즘에서 semi-gradient 함수 근사를 확장한 것으로 psuedocode 는 아래와 같다.</li>
        </ul>

        <p><img src="/assets/images/posts/9_4_8_n_step_semi_gradient_TD_psuedo_code.png" alt="9_4_8_n_step_semi_gradient_TD_psuedo_code" /></p>

        <p><img src="/assets/images/posts/9_4_9_key_equation_n_step_semi_gradient_TD.png" alt="9_4_9_key_equation_n_step_semi_gradient_TD" /></p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="estimating-value-functions-as-supervised-learning">Estimating Value Functions as Supervised Learning</h2>

<h3 id="moving-to-parameterized-functions">Moving to Parameterized Functions</h3>

<ul>
  <li>Tabular Methods
    <ul>
      <li>모든 가능 상태를 표현하는 테이블 형태의 저장공간에 각각의 학습 값을 저장하는 형태</li>
      <li>하지만 실제 세계의 문제들의 경우 이 테이블 저장공간이 추적불가능할 정도로 커지게 된다.
        <ul>
          <li>로봇이 카메라를 통해 세계를 관찰하는 경우, 모든 가능한 이미지를 저장할 수는 없음.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>학습목표
    <ul>
      <li>파라미터화된 함수를 사용하여 근사값을 구하는 법 이해하기</li>
      <li>선형 가치함수근사의 의미를 설명하기</li>
      <li>tabular 케이스 또한 선형 가치함수근사의 특별한 케이스임을 이해하기</li>
      <li>가치함수근사를 파라미터화 하는 많은 방법이 있음에 대해 이해하기</li>
    </ul>
  </li>
  <li>
    <p>다양한 형태로 표현가능한 가치 함수</p>

    <p><img src="/assets/images/posts/value_function_tabular_and_linear_approximation.png" alt="value_function_tabular_and_linear_approximation" /></p>

    <ul>
      <li>좌측은 tabular 형태로 각각의 가치함수값을 가지고 있는 형태
        <ul>
          <li>각각의 상태에 따라 독립된 값을 테이블에 저장하는 형태 (지금까지 학습한 방식)</li>
          <li>학습이 진행됨에 따라 테이블에 저장된 값을 업데이트한다.</li>
        </ul>
      </li>
      <li>우측은 X 와 Y 좌표 값에 따라 X + Y 가치함수를 가진 형태
        <ul>
          <li>이론적으로 우리는 상태를 제공받아 실수를 출력하는 어떠한 함수도 사용할 수 있다.</li>
          <li>하지만 이러한 형태를 가치함수로 사용하기를 원치 않음.
            <ul>
              <li>이 예측치를 수정할 방법이 없음 (학습할 방법이 없음)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Parameterized function
    <ul>
      <li>$\hat{v}(s,\textbf{w}) \approx v_\pi (s)$
        <ul>
          <li>$\textbf{w}$ : weights (가중치) - 함수에 변화를 주기 위한 조정이 가능해짐.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/example_parameterized_value_function.png" alt="example_parameterized_value_function" /></p>

    <ul>
      <li>$\hat{v}$ : 참 가치함수 값을 근사하는 함수의 의미</li>
      <li>$\textbf{w}$ : 함수근사에 대하여 모든 가중치를 파라미터화 한 벡터 값</li>
      <li>여기에서 우리는 모든 상태에 대한 가치함수 값을 저장하는 것이 아닌, 2개의 가중치만을 저장하게 된다.</li>
    </ul>
  </li>
  <li>
    <p>가중치(Weight) 의 변화가 가치함수에 주는 영향</p>

    <p><img src="/assets/images/posts/weight_impact_value_function.png" alt="weight_impact_value_function" /></p>

    <ul>
      <li>tabular case 의 경우 하나의 상태값에 영향을 주지만</li>
      <li>Parameterized function 의 경우 가중치 하나를 변경할 경우 복수 개의 상태에 변화를 준다.</li>
    </ul>
  </li>
  <li>
    <p>Linear Value Function Approximation</p>

    <p><img src="/assets/images/posts/linear_value_function_approximation.png" alt="linear_value_function_approximation" /></p>

    <ul>
      <li>가중치와 어떠한 고정된 속성 (feature) 간의 곱의 합</li>
      <li>위의 식일 간단하게 weight vector $\textbf{w}$ 와 feature vector $\textbf{x} (s)$ 간의 내적 (inner product) 으로 표현한다.</li>
    </ul>
  </li>
  <li>
    <p>Limitations of Linear Value Function Approximation</p>

    <p><img src="/assets/images/posts/limitations_of_linear_value_function_approximation.png" alt="limitations_of_linear_value_function_approximation" /></p>

    <ul>
      <li>위의 linear value function approximation 을 살펴보면 X, Y feature 에 대해서 선형적인 표현만 할 수 있다.</li>
      <li>만약 참 값이 위의 그림과 같다면 X, Y feature 에 대한 선형 함수로는 표현할 방법이 없다.
        <ul>
          <li>외각의 0 을 표현하기 위해서는 $W_1$ 과 $W_2$ 가 0이 되어야만 한다.</li>
          <li>그렇게 된다면 내부의 5를 표현할 방법이 없다.</li>
        </ul>
      </li>
      <li>그러나 우리가 반드시 X, Y 값을 features 로 사용할 필요는 없다.
        <ul>
          <li>즉, Linear value function 은 좋은 특징(features) 값을 가지는 것이 중요하다.
            <ul>
              <li>특징(features) 을 정의하는 데 다양한 효과적인 방법들이 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Tabular case 를 linear function 으로 표현하는 방법</p>

    <p><img src="/assets/images/posts/tabular_value_function_is_linear.png" alt="tabular_value_function_is_linear" /></p>

    <ul>
      <li>각각의 상태를 feature 로 정의한다.</li>
      <li>이에 대응하는 가중치 값과의 내적을 구하면, 왼쪽의 Tabular case 와 동일한 연산이 된다.</li>
    </ul>
  </li>
  <li>
    <p>Nonlinear Function Approximation</p>

    <p><img src="/assets/images/posts/nonlinear_function_approximation_NN.png" alt="nonlinear_function_approximation_NN" /></p>

    <ul>
      <li>신경망 방식 또한 비선형 가치근사 방법 중 하나이다.</li>
      <li>이 또한 parameterized function 중 하나이다.</li>
      <li>상태 S 가 실제 가중치 값들 $\textbf{w}$ 를 통과하며 $\hat{v}(s, \textbf{w})$ 의 연산을 하게 된다.</li>
    </ul>
  </li>
</ul>

<h3 id="generalization-and-discrimination">Generalization and Discrimination</h3>

<ul>
  <li>Generalization 에 대해
    <ul>
      <li>함수 근사에서 가장 중요한 고려사항은 어떻게 상태들을 일반화 (Generalize) 할 것인지 이다.</li>
      <li>Generalization 의 예
        <ul>
          <li>어떤 사람이 특정한 자동차를 운전하는 방법을 배울 경우, 다른 자동차의 운전 방법을 배울 때 처음부터 배우지 않는다.</li>
          <li>혹은 다른 도로에서 운전하거나, 비 오는 도로에서 운전한다고 처음부터 배우지 않는다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>학습목표
    <ul>
      <li>generalization (일반화) 와 discrimination (차별) 의 의미 이해하기</li>
      <li>generalization (일반화) 의 혜택 이해하기</li>
      <li>가치 근사에서 왜 generalization (일반화) 와 discrimination (차별) 모두가 필요한지 설명하기</li>
    </ul>
  </li>
  <li>Generalization
    <ul>
      <li>직관적 의미 : 특정한 상황에서의 지식을 적용하여 광범위한 상황에서의 결론을 도출하는 것</li>
      <li>정책 평가에서의 의미 : 하나의 상태에서 추정값의 업데이트가 다른 상태의 값에도 영향을 주는 것</li>
    </ul>

    <p><img src="/assets/images/posts/generalization_updates_one_state_affect_other_state.png" alt="generalization_updates_one_state_affect_other_state" /></p>

    <ul>
      <li>위의 그림처럼 가령 비슷한 시간이 소요되는, 비슷한 거리의 캔을 수거하러 가는 경우 센서에 의해 다른 값이 읽히더라도 비슷한 값이 도출될 수 있다.
        <ul>
          <li>이러한 경우 위의 두 상태에 대해 가치함수의 일반화를 하길 원한다.</li>
        </ul>
      </li>
      <li>일반화를 통해 더 빠른 학습의 진행이 가능하다. (아직 방문하지 않은 상태에 대해서도 업데이트가 가능함)</li>
    </ul>
  </li>
  <li>Discrimination
    <ul>
      <li>두 개의 상태를 구분하여 두 개의 상태가 다르도록 만드는 능력</li>
    </ul>

    <p><img src="/assets/images/posts/discrimination_make_two_states_different.png" alt="discrimination_make_two_states_different" /></p>

    <ul>
      <li>거리가 같더라도 벽 뒤에 있는지, 벽이 없는지에 따라 상태를 구분해야 한다.</li>
      <li>따라서 비슷한 거리에 있는 캔에 대해 상태를 일반화하는 것도 중요하나, 다른 정보에 따라 상태를 구분하는 것도 중요하다.</li>
    </ul>
  </li>
  <li>
    <p>일반화와 구분에 따른 카테고리화</p>

    <ul>
      <li>Tabular Methods : 구분은 뛰어나나 일반화는 전혀 못함. 각 상태에 대해 독립적인 학습을 진행함.</li>
      <li>Aggregate All States : 모든 상태를 똑같은 상태로 판단. 상태에 대한 학습이 불가함</li>
      <li>$*$ (●:현실적인 목표) : 좋은 구분과 좋은 일반화를 달성한 상태로, 비슷한 상태끼리 학습을 같이 진행하여 빠른 학습을 하고, 상태간 구분을 하여 정확히 근사를 하는 상태
        <ul>
          <li>예를 들어 비슷한 상태그룹을 나타내는 feature 를 표기한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Generalize 방법에 대해</p>

    <p><img src="/assets/images/posts/generalize_chess_how.png" alt="generalize_chess_how" /></p>

    <ul>
      <li>좌측은 극단적인 Generalize 로 모든 체스게임 상태를 동일한 상태로 보고, 승률을 0.5 로 책정한 경우 (안좋은 예측값)</li>
      <li>우측은 Tabular Case 의 경우로 모든 경우의 수에 대해 승률을 책정한 경우 (경우의 수가 너무 많아 불가능)</li>
      <li>우리는 이 사이의 무언가를 원한다.
        <ul>
          <li>비슷한 승률 (비슷한 상태) 끼리의 그룹화는 어려운 질문이다.</li>
          <li>이것은 우리의 알고리즘 성능에 지대한 영향을 미치며, 머신러닝과 강화학습의 중심 화제이다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="framing-value-estimation-as-supervised-learning">Framing Value Estimation as Supervised Learning</h3>

<ul>
  <li>학습목표
    <ul>
      <li>어떻게 가치 측정이 지도학습 문제에 포함될 수 있는지 이해하기</li>
      <li>모든 가치근사방식이 강화학습에 잘 적용될 수 없다는 점을 식별하기</li>
    </ul>
  </li>
  <li>
    <p>Supervised learning</p>

    <p><img src="/assets/images/posts/function_approximation_similarity_supervised_learning.png" alt="function_approximation_similarity_supervised_learning" /></p>

    <ul>
      <li>지도학습도 입력 값과 목표 값을 이용해 함수를 근사시키는 과정은 동일하다.</li>
      <li>학습 세트에 없는 입력값에 대해서, 일반화를 통해 실 가치와 유사한 값을 얻길 원한다.</li>
      <li>이러한 parameterized function 은 여러 형태로 표현될 수 있는데, 그 중 하나가 신경망이다.</li>
    </ul>
  </li>
  <li>
    <p>Monte Carlo 방식과 Supervised Learning 의 유사성</p>

    <p><img src="/assets/images/posts/function_approximation_similarity_MC.png" alt="function_approximation_similarity_MC" /></p>

    <ul>
      <li>Policy Evaluation 에 있어서, Monte Carlo 는 샘플의 리턴값을 이용하여 가치 함수를 추정하는 방식이다.
        <ul>
          <li>이 또한 입력값이 상태, 목표 값이 리턴 값인 지도학습의 일환으로 볼 수 있다.</li>
          <li>또한 모든 상태에서 함수가 참 값과 유사한 예측값을 출력하기를 원한다.</li>
          <li>이는 TD 또한 마찬가지이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>모든 가치근사 방식이 강화학습에 잘 적용될 수 없는 이유</p>

    <ul>
      <li>온라인 업데이트
        <ul>
          <li>에이전트가 환경과 상호작용을 하면서 계속 새 데이터를 만드는 경우 (즉, 처음부터 전체데이터에 접근 가능한 Offline Learning 과 차이가 있음)</li>
          <li>함수 근사를 사용할 때, 해당 방식이 온라인 환경에서 잘 적용될 수 있는지를 생각해 봐야 한다.</li>
          <li>어떠한 근사 방법은 고정된 배치 데이터를 사용해야 하거나, 시간적으로 상관된 데이터 (강화학습은 언제나 상관되어 있다.) 에 맞지 않는 경우가 있다.</li>
        </ul>
      </li>
      <li>부트스트래핑
        <ul>
          <li>타겟 값이 현재의 추측값과 연관이 있을 경우</li>
          <li>계속적으로 타겟 값이 변동되는 경우</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="the-objective-for-on-policy-prediction">The Objective for On-policy Prediction</h2>

<h3 id="the-value-error-objective">The Value Error Objective</h3>

<ul>
  <li>학습목표
    <ul>
      <li>정책 평가를 위한 평균제곱오차 (mean squared value error) 의 목표 이해</li>
      <li>목표에서 상태 분포 (state distribution) 의 역할을 설명하기</li>
    </ul>
  </li>
  <li>An Idealized Scenario
    <ul>
      <li>예를 들어 모든 상태에 대한 참 값을 알 수 있는 상태라고 가정하자.
        <ul>
          <li>${(S_1, v_\pi(S_1)), (S_2, v_\pi(S_2)), (S_3, v_\pi(S_3)), …}$</li>
        </ul>
      </li>
      <li>우리는 이 참 값과 최대한 유사한 값을 출력할 수 있는 근사 함수를 찾아야 한다.
        <ul>
          <li>$\hat{v}(s,\textbf{w}) \approx v_\pi(s)$</li>
        </ul>
      </li>
      <li>하지만 위 근사함수가 모든 상태에서 참 값과 동일한 값을 출력할 수는 없다.
        <ul>
          <li>우리는 가중치 $\textbf{w}$ 값을 조절하여 최대한 좋은 결과를 얻고자 한다.</li>
          <li>우리는 어떠한 측정치를 이용하여 우리의 예측을 보다 정확하게 조절할 필요가 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The Mean Squared Value Error Objective
    <ul>
      <li>문제의 가정
        <ul>
          <li>선형 가치근사함수 $\hat{v}$</li>
          <li>상태는 1차원에 연속적이라고 가정</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/mean_squared_value_error_objective.png" alt="mean_squared_value_error_objective" /></p>

    <ul>
      <li>Squared Value Error
        <ul>
          <li>$[v_\pi(s) - \hat{v}(s, \textbf{w} )]^2$</li>
          <li>참 값과 추정 값 간의 오차를 측정할 수 있는 전통적인 방법</li>
          <li>문제는 하나의 상태에서 오차가 준다면, 다른 상태에서 오차가 늘어날 수도 있다는 점이다.</li>
        </ul>
      </li>
      <li>Sum of Squared Value Error
        <ul>
          <li>$\sum_s [v_\pi(s) - \hat{v}(s, \textbf{w}) ]^2$</li>
          <li>모든 상태에서의 오차 합을 측정</li>
          <li>하지만 과연 모든 상태가 서로 같은 중요도를 가진다고 볼 수 있을까?</li>
        </ul>
      </li>
      <li>Sum of Mean Squared Value Error
        <ul>
          <li>$\sum_s \mu(s) [v_\pi(s) - \hat{v}(s, \textbf{w}) ]^2$</li>
          <li>$\mu(s)$ : 해당 정책 하에 s 상태에 방문한 빈도 수를 전체 빈도수 대비 분수로 나타낸 것
            <ul>
              <li>많이 방문한 상태의 에러 값에 더 많이 집중하고, 드물게 방문한 상태의 에러값에 덜 신경 쓰는 것</li>
              <li>해당 값은 확률분포 값이다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Adapting the Weights to Minimize the Mean Squared Value Error Objective
    <ul>
      <li>$\overline{VE} = \sum_s \mu(s) [v_\pi(s) - \hat{v}(s, \textbf{w})]^2$</li>
      <li>우리는 가중치 $\textbf{w}$ 를 조정하여 Mean Squared Value Error 값을 최대한 작게 만드는 것이 목적이다.</li>
      <li>이 Objective 를 VE bar 라고 한다.</li>
      <li>함수 근사를 이용한 정책의 평가는 특정한 목표값을 정의해야 한다.</li>
      <li>Mean Squared Value Error 는 이러한 목표값 중 하나이다.</li>
    </ul>
  </li>
</ul>

<h3 id="introducing-gradient-descent">Introducing Gradient Descent</h3>

<ul>
  <li>학습목표
    <ul>
      <li>경사하강법의 개념을 이해한다.</li>
      <li>경사하강법이 고정된 한 지점으로 수렴하는 것을 이해한다.</li>
    </ul>
  </li>
  <li>
    <p>Recap : Learning Parameterized Value Functions</p>

    <p><img src="/assets/images/posts/recap_learning_parameterized_value_functions.png" alt="recap_learning_parameterized_value_functions" /></p>

    <ul>
      <li>가중치 $\textbf{w}$ 는 실제 실수 값들로 이루어져 있다.</li>
      <li>위의 연산을 보면, 가중치의 변화는 많은 상태에 영향을 줄 수 있다.</li>
      <li>우리의 목표는 전체 에러값 (Overall value error) 의 최소화이다.</li>
    </ul>
  </li>
  <li>
    <p>Understanding Derivatives (미분 이해하기)</p>

    <p><img src="/assets/images/posts/understanding_derivatives.png" alt="understanding_derivatives" /></p>

    <ul>
      <li>$f$ : function, 여기에서는 위 value error 로 이해</li>
      <li>$W$ : 가중치의 스칼라 파라미터로 이해</li>
      <li>여기에서 미분 값으로 $W$ 값의 지역적 변화에 대해 $f$ 값을 증가시킬지 감소시킬지를 알 수 있다.
        <ul>
          <li>미분 값의 음수, 양수의 여부로 $W$ 포인트에서 $f$ 증가, 감소에 대해 판단</li>
          <li>미분 값의 절대값 크기로 $W$ 포인트에서의 경사 (얼마나 급변하는지) 에 대해 판단</li>
          <li>여기에서 미분 값의 기울기 방향으로 $W$ 를 이동시키는 것은 $f$ 의 값을 증가시키는 방향으로 이동하는 것임</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/gradient_derivatives_in_multiple_dimensions.png" alt="gradient_derivatives_in_multiple_dimensions" /></p>

    <ul>
      <li>$\textbf{w}$ 의 벡터 요소의 수 (차원) 에 따라 여러 차원의 미분값이 존재한다.
        <ul>
          <li>이 각각의 차원들에 대해서도 위의 규칙이 통용된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Example : Gradient of a Linear Value Function</p>

    <p><img src="/assets/images/posts/example_gradient_of_a_linear_value_function_1.png" alt="example_gradient_of_a_linear_value_function_1" /></p>

    <ul>
      <li>이전에 다뤘듯, 선형가치근사함수에서의 가치함수값은 단순 가중치와 상태 feature vector의 내적이었다.</li>
      <li>이 때, feature vector 는 가중치와는 독립적인 값이므로, 미분 값이 해당 상태의 feature vector 그 자체가 되게 된다.</li>
    </ul>

    <p><img src="/assets/images/posts/example_gradient_of_a_linear_value_function_2.png" alt="example_gradient_of_a_linear_value_function_2" /></p>

    <ul>
      <li>목표 값은 $\textbf{w}$ 에 대한 함수이다.
        <ul>
          <li>$\hat{v}$ 가 $\textbf{w}$ 에 대한 함수이기 때문</li>
          <li>우리의 목표는 이 함수 값을 최소화 하는 것이다.</li>
        </ul>
      </li>
      <li>$\alpha$ : 얼마나 움직일지 (step-size) 를 정의. 미분 값은 지역적인 영역에서의 증감만을 보장하기 때문</li>
      <li>적은 양으로 가중치를 조절하다 보면 Gradient 값이 0이 되는 부분이 있는데 이 부분을 지역최소값 (local minumim) 이라 한다.
        <ul>
          <li>해당 가중치 $\textbf{w}$ 가 당장의 근처 값보다 낫다는 것을 보여줌. (하지만 최적의 값은 아닐 수 있음)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Global Minima and Solution Quality
    <ul>
      <li>전역최적값 (global minimum) $\textbf{w}_*$ 에서의 $\hat{v}$ 값이 반드시 참 값일 필요는 없음. (충족하지 않음)
        <ul>
          <li>$\hat{v} \ne v_\pi$</li>
          <li>이것은 function parameterization 의 한계이기도 하고, 목표값(objective)의 설정에도 영향을 받는다고 볼 수 있음</li>
        </ul>
      </li>
      <li>만약 feature vector 값이 상태와 무관하게 언제나 1이라면, Mean Squared Value Error 목표값을 최소화 하는 근사가치함수 (모든 상태에 대해 평균 값을 제공) 를 찾을 수는 있겠지만 이것이 좋은 가치함수라고 볼 수는 없다. (이 경우는 feature vector 가 잘못 설정된 경우)</li>
    </ul>
  </li>
</ul>

<h3 id="gradient-monte-for-policy-evaluation">Gradient Monte for Policy Evaluation</h3>

<ul>
  <li>학습목표
    <ul>
      <li>경사하강법과 확률적 경사하강법을 사용하여 오차값을 최소화 하는 방법 이해하기</li>
      <li>가치 추정을 위한 Gradient Monte Carlo 알고리즘의 이해</li>
    </ul>
  </li>
  <li>
    <p>Gradient of the Mean Squared Value Error Objective</p>

    <p><img src="/assets/images/posts/gradient_of_the_mean_squared_value_error_objective.png" alt="gradient_of_the_mean_squared_value_error_objective" /></p>

    <ul>
      <li>첫번째로 목표값 (Objective) 의 Gradient 를 찾아야 한다.
        <ul>
          <li>위의 경우 Mean Squared Value Error 의 Gradient 를 찾아야 한다.</li>
          <li>Mean Squared Value Error : A weighted sum of the squared error over all states.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/from_gradient_descent_to_stochastic_gradient_descent_1.png" alt="from_gradient_descent_to_stochastic_gradient_descent_1" /></p>

    <ul>
      <li>Mean Squared Value Error 에 대한 Gradient 를 계산하는 것은 모든 상태에 대한 합, 모든 상태의 확률분포에 대한 계산을 의미
        <ul>
          <li>일반적으로 실현이 불가능하다.</li>
          <li>대부분의 경우 분포값 $\mu$ 를 알지 못한다.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/from_gradient_descent_to_stochastic_gradient_descent_2.png" alt="from_gradient_descent_to_stochastic_gradient_descent_2" /></p>

    <ul>
      <li>이상적인 설정 - $v_\pi$ 에 접근이 가능한 경우
        <ul>
          <li>명시적으로 $\mu$ 가 없더라도, 정책을 따름으로서 상태를 샘플링할 수 있다.</li>
          <li>정책을 따르면서 얻은 상태에 대한 가중치의 즉각적인 업데이트가 가능하다.</li>
          <li>하나의 차원으로 볼 때, 상태 샘플에 따라 에러값이 늘어날 수도 있지만 점진적으로 개선되어간다.</li>
        </ul>
      </li>
      <li>위와 같은 업데이트 접근법을 확률적 경사하강법 (Stochastic Gradient Descent) 이라 한다.
        <ul>
          <li>즉, 확률적이란 모든 상태에 대한 업데이트가 아닌, 정책을 따라 얻은 샘플링된 상태에 대한 업데이트를 한다는 뜻</li>
          <li>이 확률적 경사하강법은 경사에 대한 노이즈가 있는 근사라고 볼 수 있다.
            <ul>
              <li>계산비용이 훨씬 저렴함</li>
              <li>최소값까지 꾸준한 발전을 이룰 수 있음</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Gradient Monte Carlo</p>

    <p><img src="/assets/images/posts/gradient_monte_carlo.png" alt="gradient_monte_carlo" /></p>

    <ul>
      <li>위의 Stochastic Gradient Descent 에는 한계점이 있다.
        <ul>
          <li>우리는 $v_\pi$ 에 접근할 수 없다.</li>
          <li>이 $v_\pi$ 값을 정책을 따라 얻은 리턴값으로 대체한다. (Monte Carlo 방식)</li>
          <li>생성된 에피소드 샘플에 대하여 가중치 업데이트를 진행한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="state-aggregation-with-monte-carlo">State Aggregation with Monte Carlo</h3>

<ul>
  <li>학습목표
    <ul>
      <li>가치함수의 근사를 위한 state aggregation (상태 집합) 기법 사용법 이해</li>
      <li>state aggregation (상태 집합) 과 함께 Gradient Monte Carlo 방식 적용</li>
    </ul>
  </li>
  <li>
    <p>Random Walk Example</p>

    <p><img src="/assets/images/posts/random_walk_example.png" alt="random_walk_example" /></p>

    <ul>
      <li>문제의 정의
        <ul>
          <li>좌, 우측에 종료상태, 그리고 1 부터 1000 까지의 상태가 있다.</li>
          <li>좌측 종료상태에서 보상 -1, 우측 종료상태에서 보상 +1, 그 외의 상태는 보상 0</li>
          <li>동작은 좌측 혹은 우측으로 100칸까지 이동 가능하며 좌,우 1~100 칸 이동 확률은 uniform random policy 를 따른다.</li>
          <li>첫 시작 지점은 상태 500에서 시작한다.</li>
          <li>discount gamma 값은 1이다. (할인 없음)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>State Aggregation</p>

    <p><img src="/assets/images/posts/state_aggregation.png" alt="state_aggregation" /></p>

    <ul>
      <li>몇몇 상태를 같은 상태로 취급하는 기법</li>
      <li>위의 예시에서 상태가 8개 있는데, 4개의 상태를 같은 상태로 묶어 2개의 상태로 취급하는 기법임.
        <ul>
          <li>즉, 위의 묶음 중 아무 상태가 업데이트 되어도 나머지 3개의 상태가 같이 업데이트된다.</li>
        </ul>
      </li>
      <li>State Aggregation 은 linear function approximation 의 일종이다.</li>
      <li>상태가 많은 경우 학습의 속도가 느려질 수 있는데, 위 기법을 통해 빠르게 학습할 수 있음.</li>
    </ul>
  </li>
  <li>
    <p>How to Compute the Gradient for Monte Carlo with State Aggregation</p>

    <p><img src="/assets/images/posts/compute_gradient_monte_carlo_with_state_aggregation.png" alt="compute_gradient_monte_carlo_with_state_aggregation" /></p>
  </li>
  <li>
    <p>Constructing a State Aggregation for the Random Walk</p>

    <p><img src="/assets/images/posts/monte_carlo_updates_for_a_single_episode.png" alt="monte_carlo_updates_for_a_single_episode" /></p>

    <ul>
      <li>어떻게 집합으로 묶을 것인가?
        <ul>
          <li>State Aggregation 은 상태를 동일 그룹 군으로 묶어 같은 가치 추정을 하도록 만든다.</li>
          <li>즉, 우리는 상태를 묶을 때 그들의 값이 유사할 것이라고 생각되는 상태들을 그룹군으로 묶어야 한다.</li>
          <li>그룹이 작다면 보다 더 정확한 결과를 얻을 것이나, 학습 시간이 더 오래 걸린다.</li>
          <li>Random Walk 문제에서는 1부터 1000까지의 상태를 100개 단위의 그룹 군으로 묶어본다.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/random_walk_one_episode_result.png" alt="random_walk_one_episode_result" /></p>

    <ul>
      <li>첫 에피소드에 대한 해석
        <ul>
          <li>첫 번째 에피소드는 종료 결과 보상 1을 얻었고, 할인이 없기 때문에 모든 상태에 대한 리턴값은 1이 된다.</li>
          <li>속하는 그룹의 가중치 값이 모두 업데이트 된다.</li>
          <li>여러 상태를 오간 뒤 첫번째 에피소드에 대한 가치 추정의 결과는 위 그림과 같다.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/posts/random_walk_final_value_estimates.png" alt="random_walk_final_value_estimates" /></p>

    <ul>
      <li>최종 에피소드 이후 가치 추정 결과
        <ul>
          <li>각 상태그룹에 따라 동일한 근사 값을 가지는 것을 볼 수 있음</li>
          <li>참 값이 근사 값의 중앙을 관통하는 것은, 상태의 확률분포 (극단지점의 상태보다 그렇지 않은 상태의 분포가 더 크다) 의 영향이다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="the-objective-for-td">The Objective for TD</h2>

<h3 id="semi-gradient-td-for-policy-evaluation">Semi-Gradient TD for Policy Evaluation</h3>

<ul>
  <li>학습목표
    <ul>
      <li>함수 근사를 위한 TD update 의 이해</li>
      <li>가치 추정을 위한 Semi-gradient TD(0) 알고리즘의 개요</li>
    </ul>
  </li>
  <li>
    <p>Gradient Monte Carlo 와의 비교</p>

    <p><img src="/assets/images/posts/td_update_for_function_approximation.png" alt="td_update_for_function_approximation" /></p>

    <ul>
      <li>Gradient Monte Carlo 에서는 리턴값 $G_t$ 를 사용하며 이는 편향되지 않은 값이기에 가중치가 지역 최적값에 수렴한다.
        <ul>
          <li>꼭 리턴값이 아니더라도 다른 타겟을 사용할 수 있으며, 이 값이 편향되지 않다면 수렴을 보장한다.</li>
        </ul>
      </li>
      <li>TD 방식에서는 현재 가치 추정값을 타겟으로 하기에 값이 편향된다.
        <ul>
          <li>추정값이기에 참 가치함수와는 값이 다름.</li>
          <li>그렇기에 해당 알고리즘은 에러 값이 지역 최소값에 수렴한다고 보장할 수 없다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>TD target 의 이점
    <ul>
      <li>샘플의 리턴 값보다 분산이 작아 더 빠르게 수렴한다.</li>
    </ul>
  </li>
  <li>TD target 의 이점 (Chat GPT)
    <ul>
      <li>계산 효율성: 함수 근사를 사용한 TD 업데이트는 매우 큰 상태 공간에서도 적용할 수 있다. 대규모의 상태 공간을 전체적으로 계산하는 것보다 훨씬 효율적임.</li>
      <li>활용 가능한 데이터: 실제 상황에서는 종종 완벽한 정보가 제공되지 않는다. 편향된 추정값이라도 현재 사용 가능한 정보를 기반으로 한 업데이트는 여전히 유용할 수 있다.</li>
      <li>탐색적인 측면: 편향된 추정값을 사용하는 것은 다양한 상황을 탐색하고 경험하는 데 도움을 줄 수 있다. 이는 종종 실제 환경에서 더 나은 행동을 선택하는 데 도움이 될 수 있다.</li>
      <li>일반화 가능성: 함수 근사를 사용한 TD 업데이트는 일반화 가능성을 가질 수 있다. 이는 일부 편향된 추정값이라도 일반적인 상황에서 적용 가능한 모델을 생성할 수 있다는 것을 의미함.</li>
    </ul>
  </li>
  <li>
    <p>TD is a semi-gradient method</p>

    <p><img src="/assets/images/posts/td_is_a_semi_gradient_method.png" alt="td_is_a_semi_gradient_method" /></p>

    <ul>
      <li>TD 의 경우 업데이트의 목표 타겟값이 TD target ($R_{t+1} + \gamma \hat{v} (S_{t+1}, \textbf{w})$) 이다.</li>
      <li>목표 타겟값에 가중치 $\textbf{w}$ 가  포함되어 있어, 미분식이 기존의 TD Update 식과 다르게 된다.
        <ul>
          <li>The TD Update : $-(U_t - \hat{v}(S_t, \textbf{w})) \nabla \hat{v}(S_t, \textbf{w})$</li>
          <li>여기에서 TD Update 란 시간차 학습에 의한 실제값과 기대값의 차이를 줄이기 위한 방법을 의미한다.</li>
          <li>함수 근사에서 사용될 경우 위와같은 형태가 됨.</li>
        </ul>
      </li>
      <li>즉, 실제 값과 기대값의 차이가 아닌 기대값과 기대값 사이 TD 오차에 비례하는 위의 식은 gradient descent 방법과는 다르다.</li>
      <li>위의 차이에도 불구하고, TD 는 많은 케이스에서 수렴한다.</li>
    </ul>

    <p><img src="/assets/images/posts/semi_gradient_td_0_psuedocode.png" alt="semi_gradient_td_0_psuedocode" /></p>

    <ul>
      <li>Semi-Gradient TD(0) 의 psuedocode
        <ul>
          <li>TD(0) 는 에피소드가 끝날 때 까지 기다리지 않고, 매 스텝마다 업데이트를 진행한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="comparing-td-and-monte-carlo-with-state-aggregation">Comparing TD and Monte Carlo with State Aggregation</h3>

<ul>
  <li>학습목표
    <ul>
      <li>TD 가 편항된 가치 추정으로 수렴하는 점을 이해</li>
      <li>TD 가 Gradient Monte Carlo 보다 훨씬 빠르게 수렴하는 점을 이해</li>
    </ul>
  </li>
  <li>
    <p>Gradient Monte Carlo 의 경우</p>

    <p><img src="/assets/images/posts/gradient_monte_carlo_converge_local_minimum.png" alt="gradient_monte_carlo_converge_local_minimum" /></p>

    <ul>
      <li>더 많은 샘플들로 최적화 할 수록 Mean Squared Value Error 의 지역최소값에 수렴한다.</li>
      <li>이는 value error의 경사로 편향되지 않은 추정값을 사용하기 때문이다.</li>
      <li>이론대로라면, 우리는 이 알고리즘을 긴 시간동안, step-size 파라미터를 decay 하며 진행해야 수렴값을 얻을 수 있다.
        <ul>
          <li>예제에서 상수 step-size 를 사용하여, 지역 최소값에서 계속 진동하는 것을 볼 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Semi-Gradient TD 의 경우</p>

    <p><img src="/assets/images/posts/semi_gradient_td_not_converge_to_a_local_minimum.png" alt="semi_gradient_td_not_converge_to_a_local_minimum" /></p>

    <ul>
      <li>Target 값이 예측값 (정확하지 않은 값) 이므로, 업데이트 값에 편향이 생길 수 있다.</li>
      <li>우리의 가치 근사가 경계값 내에서도 완벽할 수 없으므로, Target 은 편향된 상태로 남게 된다.</li>
      <li>따라서 Semi-Gradient TD 의 Mean Squared Value Error 가 지역 최소값으로 수렴한다는 것을 보장할 수 없다.</li>
      <li>물론 이 편향은 추정이 개선될 수록 줄게 된다.</li>
    </ul>
  </li>
  <li>
    <p>State Aggregation 을 이용한 1000 State Random Walk 문제에서 MC 방식과 TD 방식의 결과 비교</p>

    <p><img src="/assets/images/posts/1000_state_random_walk_semi_gradient_td.png" alt="1000_state_random_walk_semi_gradient_td" /></p>

    <ul>
      <li>1000 State Random Walk 를 값이 수렴할 때까지 진행 (1000 Episodes)</li>
      <li>Value Estimate 값의 변화가 멈추었을 때의 결과를 도식화함.</li>
      <li>Monte Carlo 와 비교하여 값이 정확하지 않다. (편향값 때문)</li>
    </ul>

    <p><img src="/assets/images/posts/1000_state_random_walk_30_episodes_td_mc.png" alt="1000_state_random_walk_30_episodes_td_mc" /></p>

    <ul>
      <li>위 문제를 30 Episodes 만 진행</li>
      <li>TD 와 MC 의 $\alpha$ 값에 큰 차이가 있으므로, 0과 1 사이의 100개 구간으로 시험하여 가장 좋은 결과의 $\alpha$ 를 선택
        <ul>
          <li>TD : 0.22</li>
          <li>MC : 0.01</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="doina-precup--building-knowledge-for-ai-agents-with-reinforcement-learning">Doina Precup : Building Knowledge for AI Agents with Reinforcement Learning</h3>

<ul>
  <li>강화학습 에이전트가 습득해야 하는 지식의 종류
    <ul>
      <li>절차적 지식 (일을 수행하는 방법) - 정책은 이에 해당하는 기본적인 예</li>
      <li>우리가 알고 싶어할 수도 있는 다른 지식 - 특정 물체와 상호작용하는 방법, 다른 장소로 이동하는 방법</li>
      <li>기술, 옵션, 혹은 목표 지향적인 행동</li>
      <li>에이전트의 행동에 따라 어떤 일이 일어날 수 있는지를 의미하는 예측 지식, 경험적 지식
        <ul>
          <li>이것은 가치 함수일 뿐만 아니라 모델 예시와 같은 다른 것이기도 하다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이러한 종류의 지식에 대해 우리가 갖고 싶은 특정 특성
    <ul>
      <li>배울만한 지식을 알게 되어, 데이터로부터 그것을 얻고, 표현할 수 있기를 원함</li>
      <li>에이전트가 다양한 것, 다양한 상황에 대해 알수 있기를 원하며 이미 가지고 있는 지식 조각을 더 큰 조각으로 구성할 수 있기를 바람</li>
    </ul>
  </li>
  <li>지식 표현의 요소
    <ul>
      <li>타임 스케일에 따른 에이전트의 행동 측면에서의 일반화</li>
      <li>세상에 대해 인식, 추론하는 에이전트의 능력에서의 일반화</li>
    </ul>
  </li>
  <li>상태 추상화와 함수 근사 (위의 필요성에서 등장)
    <ul>
      <li>다른 타임 스케일에서의 절차적 지식의 일반화
        <ul>
          <li>에이전트가 생성될 때, 할 수 있는 행동은 제한적이고 이러한 행동들은 항상 한 번의 time step 동안 지속된다.</li>
          <li>이는 MDP (Markov Decision Process) 프레임워크와 일치시키기 위한 것이다.</li>
          <li>강화학습에서 상태, 행동, 보상 등은 시간 단게에 따라 발생하며 에이전트가 처음 생성될 때 행동의 다양성이나 지속 시간 등이 제한될 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>에이전트의 행동의 기간을 단일 단계가 아닌 여러 단계로 확장하는 방법
    <ul>
      <li>옵션이라는 개념의 사용
        <ul>
          <li>초기화 단계 : 옵션이 시작될 수 있는 조건</li>
          <li>내부 정책 단계 : 옵션 실행 중 취할 행동</li>
          <li>종료 단계 : 언제 종료되는지 결정하는 조건</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>추상화의 개념으로 본 옵션
    <ul>
      <li>MDP : 에이전트가 단일 행동에 대한 보상과 상태 전이에 대한 정보를 가지고 있음</li>
      <li>옵션 : 일련의 행동들에 대한 일종의 전략이며 각 행동마다 보상과 상태 전이 확률을 내재하고 있는 개념 (행동의 집합, 패턴)
        <ul>
          <li>시간의 추상화 : 옵션이 언제 시작되고 언제 종료되는지는 가변적이다. 또한 이를 통해 고정된 time step 에서 벗어나게 된다. (Semi-MDP)</li>
          <li>상태의 추상화 : 구체적인 좌표값이 아닌 공항에 대한 추론을 한다.</li>
          <li>행동의 추상화 : 각 근육의 조절이 아닌 공항으로 가는 행동에 대해 생각한다.</li>
          <li>이는 MDP에서의 행동보다는 더 큰 시간적, 행동적, 그리고 상태적인 추상화 수준을 제공한다.</li>
          <li>위의 추상 항목들은 모두 별개이며 함께 잘 작동하도록 조율하는 방법은 연구가 필요한 문제이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>옵션을 학습하는 강화학습 방법
    <ul>
      <li>어떤 옵션을 사용할지 또한 선택이며, 이 또한 학습의 대상이다.</li>
      <li>옵션을 학습하는 강화학습 방법 중 하나, “Option-Critic Architecture”
        <ul>
          <li>옵션 선택기 (Option Selector): 에이전트가 주어진 상태에서 어떤 옵션을 선택할지 결정하는 부분. 이 선택기는 가능한 옵션들의 가치를 추정하여 가치가 높은 옵션을 선택하도록 학습됨</li>
          <li>옵션 평가자 (Option Evaluator): 선택된 옵션이 얼마나 좋은지, 즉 해당 옵션의 예상 보상이 어떤지를 평가하는 부분. 이 평가자는 선택된 옵션의 가치를 추정하고, 이를 통해 선택된 옵션이 잘 수행될 것인지를 예측함.</li>
          <li>이 아키텍처를 통해 옵션 선택과 평가를 결합하여 옵션을 효과적으로 학습하고 발전시킬 수 있다. 이러한 접근 방식은 여러 상황에서의 옵션 선택과 실행에 대한 전략을 효과적으로 학습하고 조정할 수 있도록 돕는다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="linear-td">Linear TD</h2>

<h3 id="the-linear-td-update">The Linear TD Update</h3>

<ul>
  <li>학습목표
    <ul>
      <li>선형 함수 근사 를 사용하여 TD-update 도출</li>
      <li>tabular TD(0) 가 linear semi-gradient TD(0) 의 특별한 케이스인 것을 이해하기</li>
      <li>왜 linear TD 를 특수 케이스로 취급하는지 이해하기</li>
    </ul>
  </li>
  <li>
    <p>TD Update with Linear Function Approximation</p>

    <p><img src="/assets/images/posts/td_update_with_linear_function_approximation.png" alt="td_update_with_linear_function_approximation" /></p>

    <ul>
      <li>가중치를 해당 가중치에 해당하는 TD error 와 근사가치함수의 경사값의 곱의 값에 따라 조절한다.</li>
      <li>이 때, 근사가치함수의 경사값은 선형 가치근사함수에 의해 feature vector 의 값이 된다.
        <ul>
          <li>feature 값이 크면 큰 영향을 주게 되고, feature 값이 0이면, 아무런 영향을 주지 않는다.</li>
        </ul>
      </li>
      <li>즉 선형가치근사함수의 경우 feature 값이 잘 선택되면 효율적으로 작동한다.</li>
    </ul>
  </li>
  <li>
    <p>Tabular TD is a special case of linear TD</p>

    <p><img src="/assets/images/posts/tabular_td_is_a_special_case_of_linear_td.png" alt="tabular_td_is_a_special_case_of_linear_td" /></p>

    <ul>
      <li>위의 식과 같이 모든 상태에 대해 각각의 대응하는 가중치값이 존재한다면, 이는 tabular td 와 동일한 형태가 된다.</li>
    </ul>
  </li>
  <li>선형함수근사의 유용성
    <ul>
      <li>선형 방식은 이해하기 쉽고 수학적으로 분석이 가능하다.</li>
      <li>좋은 feature 가 있으면 선형 방식은 학습도 빠르고 좋은 예측 정확도를 보여줄 수 있다.</li>
    </ul>
  </li>
</ul>

<h3 id="the-true-objective-for-td">The True Objective for TD</h3>

<ul>
  <li>학습목표
    <ul>
      <li>linear TD 학습의 고정점 (fixed point) 에 대해 이해</li>
      <li>TD 고정점에서 평균 제곱 오차의 이론적 보증에 대한 설명</li>
    </ul>
  </li>
  <li>
    <p>The Expected TD Update</p>

    <p><img src="/assets/images/posts/linear_function_approximation_expected_td_update.png" alt="linear_function_approximation_expected_td_update" /></p>

    <ul>
      <li>위 Expected TD Update 식은 아래의 연산규칙에 의해 변형이 가능하다.
        <ul>
          <li>위 식에 사칙연산 중 분배법칙이 성립한다.</li>
          <li>스칼라 값 (벡터간 내적곱) 은 전치해도 식이 변형되지 않는다.</li>
        </ul>
      </li>
      <li>위의 규칙에 의해 변형된 식에 대해서
        <ul>
          <li>Matrix A : feature 에 대한 기대값</li>
          <li>vector b : feature 와 보상에 대한 항</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>The TD Fixed Point</p>

    <p><img src="/assets/images/posts/linear_function_approximation_td_fixed_point.png" alt="linear_function_approximation_td_fixed_point" /></p>

    <ul>
      <li>TD 업데이트가 선형인 경우
        <ul>
          <li>테이블 설정에서 벨만 방정식을 해결하는 것이 아닌 (샘플데이터 사용)</li>
          <li>해(solution)를 수식으로 구하는 방식인 선형 TD에 대해 설명하고 있음.</li>
          <li>선형 TD는 TD 업데이트를 선형 함수로 근사하며, 이를 사용하여 벨만 방정식의 해를 직접 구한다.</li>
        </ul>
      </li>
      <li>여기서 해(solution)는 TD 고정점(TD fixed point)을 의미한다.
        <ul>
          <li>이 해는 평균 제곱 오차(Mean Squared Value Error)의 최소값으로 수렴하지는 않지만</li>
          <li>TD의 목적 함수에 기반한 원칙적인 최소값으로 수렴한다는 것을 설명하고 있다.</li>
          <li>따라서 TD의 학습 목표는 평균 제곱 오차의 최소값이 아닌, 벨만 방정식과 관련된 목적 함수의 최소값을 찾는 것이 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>TD Fixed Point 와 Minimum of the Value Error 의 관계</p>

    <p><img src="/assets/images/posts/linear_function_approximation_td_fixed_point_and_minimum_ve.png" alt="linear_function_approximation_td_fixed_point_and_minimum_ve" /></p>

    <ul>
      <li>그럼에도 불구하고, 우리는 여전히 TD에 의해 찾아진 해와 오류를 최소 값으로 만드는 해 사이의 관계를 알고 싶음.
        <ul>
          <li>위 방정식과 같이 $\gamma$ 가 0에 매우 가깝다면, TD Fixed Point 는 Minimum of the Value Error 의 해와 매우 가까워짐</li>
        </ul>
      </li>
      <li>Feature 의 품질과도 연관이 있는데, Feature 가 제한적이라면 TD Fixed Point 나 Minimum of the Value Error 또한 커지게 됨.
        <ul>
          <li>만약 가치함수를 완벽하게 나타낼 수 있다면, $\gamma$ 와 무관하게 TD Fixed Point 는 Minimum of the Value Error 와 동일하게 됨.</li>
          <li>양쪽 모두가 0 가 되기 때문</li>
        </ul>
      </li>
      <li>TD Fixed Point 와 Minimum of the Value Error 의 해와 차이가 발생하는 이유
        <ul>
          <li>함수 근사값의 부트스트래핑 목표를 사용하기 때문</li>
          <li>다음 상태에 대한 추정이 함수 근사로 인해 지속적으로 부정확하다면, 부정확한 대상을 향해 업데이트 됨.</li>
          <li>만약 함수 근사가 좋다면 다음 상태에 추정값은 매우 정확해짐.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#adam-white" class="page__taxonomy-item" rel="tag">Adam White</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#alberta-machine-intelligence-institute" class="page__taxonomy-item" rel="tag">Alberta Machine Intelligence Institute</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#coursera" class="page__taxonomy-item" rel="tag">Coursera</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#martha-white" class="page__taxonomy-item" rel="tag">Martha White</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#on-policy-prediction-with-approximation" class="page__taxonomy-item" rel="tag">On-policy Prediction with Approximation</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5" class="page__taxonomy-item" rel="tag">강화학습</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#%EC%95%A8%EB%B2%84%ED%83%80-%EB%8C%80%ED%95%99%EA%B5%90" class="page__taxonomy-item" rel="tag">앨버타 대학교</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#ai" class="page__taxonomy-item" rel="tag">AI</a><span class="sep">, </span>
    
      
      
      <a href="/categories/#reinforcement-learning" class="page__taxonomy-item" rel="tag">Reinforcement Learning</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2023-08-31T10:00:00+09:00">August 31, 2023</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Prediction+and+Control+with+Function+Approximation+-+01.+Week+1.+On-policy+Prediction+with+Approximation%20http%3A%2F%2Flocalhost%3A4000%2Fai%2Freinforcement%2520learning%2FPrediction_and_Control_with_Function_approximation_01_Week1%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fai%2Freinforcement%2520learning%2FPrediction_and_Control_with_Function_approximation_01_Week1%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fai%2Freinforcement%2520learning%2FPrediction_and_Control_with_Function_approximation_01_Week1%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai/reinforcement%20learning/Sample_based_Learning_Methods_04_Week4/" class="pagination--pager" title="Sample-based Learning Methods - 04. Week 4. Planning, Learning &amp; Acting
">이전</a>
    
    
      <a href="#" class="pagination--pager disabled">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">참고</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Sample_based_Learning_Methods_04_Week4/" rel="permalink">Sample-based Learning Methods - 04. Week 4. Planning, Learning &amp; Acting
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-07-16T10:00:00+09:00">July 16, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">관련 자료 (RLbook Pages 159-166)
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Sample_based_Learning_Methods_03_Week3/" rel="permalink">Sample-based Learning Methods - 03. Week 3. Temporal Difference Learning Methods for Control
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-07-12T10:00:00+09:00">July 12, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">관련 자료 (RLbook Pages 129-134)
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Sample_based_Learning_Methods_02_Week2/" rel="permalink">Sample-based Learning Methods - 02. Week 2. Temporal Difference Learning Methods for Prediction
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-07-07T10:00:00+09:00">July 7, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">관련 자료 (RLbook Pages 119-128)
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1/" rel="permalink">Sample-based Learning Methods - 01. Week 1. Monte Carlo Methods for Prediction &amp; Control
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-06-12T15:00:00+09:00">June 12, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">관련 자료 (RLbook2018 Pages 91-104)
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="검색어를 입력하세요..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<!--
<script>
	(function(d, h, m){
    var js, fjs = d.getElementsByTagName(h)[0];
    if (d.getElementById(m)){return;}
    js = d.createElement(h); js.id = m;
    js.onload = function(){
        window.makerWidgetComInit({
        position: "left",          
        widget: "eqypoqmawbcz3azh-usiy9q7ma4ong1w1-juelcm3zfgcuwjds"                
    })};
    js.src = "https://makerwidget.com/js/embed.js";
    fjs.parentNode.insertBefore(js, fjs)
	}(document, "script", "dhm"))
</script>
-->
<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
          <li><a href="mailto:hyunik03@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
      
        
      
        
          <li><a href="https://github.com/HY03" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 HY03. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/ai/reinforcement%20learning/Prediction_and_Control_with_Function_approximation_01_Week1/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/ai/reinforcement%20learning/Prediction_and_Control_with_Function_approximation_01_Week1"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://bluesplatter.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
