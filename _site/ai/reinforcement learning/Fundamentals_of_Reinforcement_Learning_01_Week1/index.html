<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.20.2 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Fundamentals of Reinforcement Learning - 01. Week 1 | Bluesplatter</title>
<meta name="description" content="Course Introduction">


  <meta name="author" content="HY03">
  
  <meta property="article:author" content="HY03">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Bluesplatter">
<meta property="og:title" content="Fundamentals of Reinforcement Learning - 01. Week 1">
<meta property="og:url" content="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/">


  <meta property="og:description" content="Course Introduction">







  <meta property="article:published_time" content="2023-01-26T18:00:00+09:00">





  

  


<link rel="canonical" href="https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "HY03",
      "url": "https://bluesplatter.com/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Bluesplatter Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->


    
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo/logo.png" alt=""></a>
        
        <a class="site-title" href="/">
          세상에 남기는 작은 흔적
          <span class="site-subtitle">Bluesplatter</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/posts/">연도별 포스트</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="https://bluesplatter.com/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#ai" itemprop="item"><span itemprop="name">Ai</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#reinforcement-learning" itemprop="item"><span itemprop="name">Reinforcement learning</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Fundamentals of Reinforcement Learning - 01. Week 1</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/profile/photo.jpg" alt="HY03" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">HY03</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>저는 전문가가 아니고 이것 저것 알아보는 비전문가 입니다. <br /> 틀린 내용이 기술되어도 너그러이 봐주시고 댓글을 남겨주셨으면 좋겠습니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Republic of Korea</span>
        </li>
      

      
        
          
        
          
            <li><a href="http://bluesplatter.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
        
          
        
          
            <li><a href="https://github.com/HY03" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      
        <li>
          <a href="mailto:hyunik03@gmail.com">
            <meta itemprop="email" content="hyunik03@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">이메일</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Fundamentals of Reinforcement Learning - 01. Week 1">
    <meta itemprop="description" content="Course Introduction">
    <meta itemprop="datePublished" content="2023-01-26T18:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Fundamentals of Reinforcement Learning - 01. Week 1
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-01-26T18:00:00+09:00">January 26, 2023</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="course-introduction">Course Introduction</h2>

<ul>
  <li>Supervised Learning (지도학습)
    <ul>
      <li>학습자가 답이 기입되어 있는 라벨링된 예시에 접근함</li>
      <li>정답이 무엇이었는지 말해주는 선생이 존재</li>
      <li><strong>학습데이터 : 정답 (혹은 정답인 동작) 을 가르쳐주는 정보를 학습한다.</strong></li>
      <li><strong>피드백 : 취해야 할 올바른 조치를 나타낸다. (이번 행동이 얼마나 좋았는지는 알려주지 않는다.)</strong>
        <ul>
          <li>피드백은 행동에 대해 완전 독립적.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Unsupervised Learning (자율학습)
    <ul>
      <li>데이터 기저에 있는 구조를 추출 (데이터 표현법)
        <ul>
          <li>이 데이터 표현법은 지도학습이나 강화학습에 도움을 줄 수 있음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Reinforcement Learning (강화학습)
    <ul>
      <li>학습자에게 최근의 행동에 대한 보상을 제공함</li>
      <li>좋은행동이 어떤것일지 식별해주는 환경이 존재하나 정확히 어떻게 해야하는지는 알려주지 않음</li>
      <li>지도학습이나 자율학습을 통해 일반화 (Input 이 달라져도 출력성능에 영향을 주지 않음) 를 개선할 수 있음</li>
      <li>바뀌는 환경속에서 상호작용하며 학습하는 것에 주안점을 둠
        <ul>
          <li>학습자가 단순히 반복되는 환경에서 계산을 통해 좋은 행동이 무엇인지 학습하는 것이 아님.</li>
          <li>학습자가 시행착오를 통해 변화하는 환경에서 목표를 바꾸어 가며 더 잘하는 것을 추구</li>
        </ul>
      </li>
      <li><strong>학습데이터 : 행동을 수행한 것에대한 평가를 학습한다.</strong></li>
      <li><strong>피드백 : 이번 행동이 얼마나 좋았는지를 피드백한다. (그것이 최선/최악인지는 알 수 없다)</strong>
        <ul>
          <li>피드백은 행동에 완전 종속됨.</li>
          <li>때문에 최선의 행동을 찾아 끊임없이 탐험해야 함.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="관련-자료-rlbook2018-pages-25-36">관련 자료 (RLbook2018 Pages 25-36)</h2>

<ul>
  <li>용어설명
    <ul>
      <li>Stationary (정상성) Probability : 여러 시간 구간마다 통계적 성질이 동일한 것
        <ul>
          <li>예) 주사위 던지기 : 시간에 무관하게 동일한 확률값</li>
        </ul>
      </li>
      <li>Non-Stationary (비정상성) Probability : 시간에 따라 통계적 성질이 변하는 것
        <ul>
          <li>예) 변덕스러운 날씨, 기후 등</li>
        </ul>
      </li>
      <li>Nonassociative setting : 다른 Action(행동), 다른 Environment(환경) 을 가정하지 않는 세팅
        <ul>
          <li>full reinforcement learning problem 의 복잡성을 배제 (다양한 환경, 비정상성, 행동의 보상이 즉각적이지 않음 - 현실세계)</li>
          <li>이미 환경이 어떠한 피드백을 줄지 알고 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Multi-armed Bandits
    <ul>
      <li>간단한 세팅 환경 (K-armed bandit problem 의 단순화 버전)
        <ul>
          <li>한가지 이상 상황에서의 동작을 학습하는 것을 배제 (Nonassociative setting)</li>
          <li>목적1 : 평가 피드백 (Evaluative feedback) 이 정답을 알려주는 피드백 (Instructive Feedback) 과 어떻게 다른지 확인</li>
          <li>목적2 : 둘이 결합될 수 있는지 확인</li>
        </ul>
      </li>
      <li>목표
        <ul>
          <li>기초적 학습법 (Learning methods) 안내</li>
          <li>이 Bandit problem 이 associative (연관성) 성질을 가지게 되었을 때 어떻게 변할지 확인</li>
        </ul>
      </li>
      <li>A k-armed Bandit Problem
        <ul>
          <li>문제에 대한 설명
            <ul>
              <li>반복적으로 k 개의 다른 옵션/행동을 선택</li>
              <li>각 선택 마다 보상 (선택한 행동에 따른 정상성 확률 분산을 가진 보상) 을 받는 문제</li>
              <li>목적 : 특정 기간동안 최대한의 보상을 받는 것</li>
            </ul>
          </li>
          <li>문제에 대한 예시 1
            <ul>
              <li>슬롯머신 혹은 “one-armed bandit” 문제와 동일하나 단지 레버가 k 가임.</li>
              <li>각 행동은 여러 대의 슬롯머신 중 한 대의 레버를 당기는 것과 동일</li>
              <li>가장 이익을 극대화 할 수 있는 레버에 집중하여 보상을 많이 받는 것이 목표</li>
            </ul>
          </li>
          <li>문제에 대한 예시 2
            <ul>
              <li>의사가 심각한 질병의 환자에게 실험적 치료들 중 하나를 선택</li>
              <li>보상 : 환자의 생존/ 치유율</li>
            </ul>
          </li>
          <li>
            <p>수식설명</p>

            <p><img src="/assets/images/posts/2_1_1_value_of_action.png" alt="2_1_1_value_of_action" /></p>

            <ul>
              <li>위 수식은 문제를 풀었을 때 ($q_*$) 의 value of the action 에 대한 수식이다.</li>
              <li>value of the action : 각각의 k 행동들이 가지는 기대/평균 보상값</li>
              <li>$A_t$ : time-step $t$ 시점에 선택한 action</li>
              <li>$R_t$ : $A_t$ 에 상응하는 보상</li>
              <li>$q_*(a)$ : $a$ 가 선택되었을때 기대되는 보상값</li>
              <li>$\doteq$ : is defined as</li>
              <li>If you knew the “value of the each action” : 해당 문제를 풀었다고 볼 수 있음.</li>
              <li>$Q_t(a)$ : $q_*(a)$ 와 유사한 값 (중간값)</li>
              <li>$q_*$ 값은 에이전트가 알고 있는 값이 아님</li>
            </ul>
          </li>
          <li>greedy actions
            <ul>
              <li>action value 를 계속 추정하다 보면 어느 시점에서나 적어도 하나의 가장 큰 예측값을 가지는 action 이 존재</li>
              <li>이것을 greedy actions 라고 함.</li>
              <li>이 greedy action 을 선택하는 것 : 현재 알고 있는 values of the action 값을 exploiting 한다.</li>
              <li>이 greedy action 을 선택하지 않는 것 : 현재 exploring 중이라고 한다.</li>
              <li>Exploitation (이기적 이용)
                <ul>
                  <li>현 step 에서 예측되는 보상값을 최대화 하는 옳은 방법</li>
                </ul>
              </li>
              <li>Exploration (탐색)
                <ul>
                  <li>장기 관점으로 보았을 때 이쪽의 보상 총합이 더 클 수 있음</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Balancing exploration and exploitation
            <ul>
              <li>k-armed bandit 과 이와 유사한 문제의 특정 수학적 공식에 대해 탐색과 이용의 밸런스를 맞출 수 있는 정교한 방법이 존재
                <ul>
                  <li>하지만 이런 방법들은 정상성에대한 강한 가정을 전재하고, full reinforcement learning 환경이나 어플리케이션을 침해하거나  증명할 수 없는 사전지식을 이용한다.</li>
                  <li>즉 활용 불가. (Full reinforcement learning 환경에서 이 밸런스 문제는 도전적인 과제임)</li>
                </ul>
              </li>
              <li>k-armed bandit problem 에서는 균형에 대해서만 고려하고, 단순 Exploitation 하는 것보다 균형을 맞춘 방식이 더 잘 작동한다는 것을 증명할 것임.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Action-value Methods
        <ul>
          <li>행동 선택을 위해 행동(Action) 에 대한 가치를 측정하는 방법을 통상 Action-value method 라 한다.</li>
          <li>
            <p>수식설명 1</p>

            <p><img src="/assets/images/posts/2_2_1_averaging_the_rewards.png" alt="2_2_1_averaging_the_rewards" /></p>

            <ul>
              <li>위 수식은 action-value method 중 보상평균값으로 추정하는 방식에 대한 수식이다.</li>
              <li>Action-value 는 통상적으로 해당 Action 이 선택되었을 때 보상 값의 평균치를 말한다.</li>
              <li>1(predictate) 는 행위를 하였을 때는 1, 아닌 경우 0 (가상의 1/0, 횟수의 개념)</li>
              <li>분모가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.</li>
              <li>이 방식을 sample-average method 라 부르며, 이는 Action-value 를 구하기 위한 많은 방법 중 하나이다.</li>
            </ul>
          </li>
          <li>
            <p>수식설명 2</p>

            <p><img src="/assets/images/posts/2_2_2_greedy_action_selection_method.png" alt="2_2_2_greedy_action_selection_method" /></p>

            <ul>
              <li>위 수식은 greedy action selection method 에 대한 수식이다.</li>
              <li>action selection 에서 가장 단순한 방법은 가장 큰 예측값에 해당하는 action을 선택하는 것이다.</li>
              <li>현 시점 이전까지 가장 탐욕적인 action 으로 정의된 행동을 하는 것.</li>
              <li>$\underset{a}{\arg\max}$ : 후술되는 값이 최대값이 되는 action $a$ 를 의미</li>
              <li>Greedy action selection 은 현 지식을 이용해 당장의 보상을 최대화 하는 방식</li>
              <li>정말 더 나은 방식을 찾기 위해 열등한 방식을 샘플링하는데 시간을 할애하지 않음.</li>
            </ul>
          </li>
          <li>$\varepsilon$-greedy methods
            <ul>
              <li>위와 다른 대안으로 대부분 탐욕스러운 행동을 하되</li>
              <li>아주 작은 확률($\varepsilon$)로 action-value 와 관계 없이 균등한 확률로 $a$ 를 선택하는 방법이 있음.</li>
              <li>이 방법을 통한 샘플링 횟수가 무한하게 커지면 $Q_t(a)$ 의 값은 $q_*(a)$ 의 값에 수렴한다.</li>
              <li>이것은 점근적인 보장일 뿐, 실질적인 효과에 대한 것은 아니다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The 10-armed Testbed
    <ul>
      <li>
        <p>조건 설명</p>

        <p><img src="/assets/images/posts/2_3_1_10_armed_testbed.png" alt="2_3_1_10_armed_testbed" /></p>

        <ul>
          <li>10-armed testbed 의 bandit problem 문제 예시</li>
          <li>각 10 개 action 의 true value $q_*(a)$ 값 은 평균 0, 분산 1 인 정규분포 (표준정규분포) 를 따른다.</li>
          <li>위 조건으로 2000번의 서로 다른 bandit problem 을 수행, 평균 값을 취함.</li>
        </ul>
      </li>
      <li>
        <p>결과</p>

        <p><img src="/assets/images/posts/2_3_2_result_10_armed_testbed.png" alt="2_3_2_result_10_armed_testbed" /></p>

        <ul>
          <li>보상의 분산이 클수록 더 많은 탐험이 필요하며 $\varepsilon$-greedy methods 가 greedy methods 보다 더 잘 작동한다.</li>
          <li>만약 보상의 분산이 0이라면 각각의 action 후에 true value 값을 바로 알 수 있게 된다.</li>
          <li>위의 경우 greedy methods 가 더 잘 작동하게 된다. (탐험할 필요가 없음.)</li>
          <li>그러나 이런 결정론적인 상황에서도 몇가지 가정이 불확실하다면 탐험하는 쪽이 유리하다.
            <ul>
              <li>비정상성 환경 (시간에 따라 true action-value 가 변함.)</li>
              <li>즉 Reinforcement learning 은 탐색과 이용에 균형이 필요함.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Incremental Implementation
    <ul>
      <li>action-value methods 를 획득한 보상의 평균 값으로 추정한다.</li>
      <li>
        <p>이 경우 어떻게 전산화 하여 계산할지?</p>

        <p><img src="/assets/images/posts/2_4_1_estimated_value_of_single_action.png" alt="2_4_1_estimated_value_of_single_action" /></p>

        <ul>
          <li>특정 단일 Action 에 대한 Action-value 를 예측, action 은 $n-1$ 번 선택됨.</li>
          <li>위 명확한 계산법은 모든 이전 기록을 가지고 있어야 하고, 예측값이 필요할 때마다 계산해야함.
            <ul>
              <li>메모리가 많이 필요하고 연산량이 상당해진다.</li>
              <li>위의 방법이 아니라 이전 평균값에서 이번 보상값을 업데이트 하는 방식을 취하는 것이 효율적이다.</li>
            </ul>
          </li>
        </ul>

        <p><img src="/assets/images/posts/2_4_2_update_action_value.png" alt="2_4_2_update_action_value" /></p>

        <ul>
          <li>위의 구현법을 이용하면 메모리는 $Q_n$ 과 $n$ 값만을 저장하고 있고, 작은 계산을 통해 매번 새로운 예측값을 구할 수 있음.</li>
        </ul>

        <p><img src="/assets/images/posts/2_4_3_update_action_value.png" alt="2_4_3_update_action_value" /></p>

        <ul>
          <li>수식의 뜻은 위와 같음</li>
          <li>Target - OldEstimate = error
            <ul>
              <li>위 에러 값은 예측값이 Target 에 가까워질 수록 작아짐</li>
              <li>Target 은 예측이 움직이기 원하는 방향을 가리킴</li>
            </ul>
          </li>
          <li>StepSize : 각 타임스텝마다 변하는 step-size parameter 이며, 이 책에서는 $\alpha$ 혹은 $\alpha_t(a)$ 로 나타낸다.</li>
        </ul>

        <p><img src="/assets/images/posts/2_4_4_simple_bandit_algorithm.png" alt="2_4_4_simple_bandit_algorithm" /></p>

        <ul>
          <li>완성된 simple bandit algorithm</li>
          <li>breaking ties randomly.. 동점 기록일 경우 랜덤하게 선택한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Tracking a Nonstationary Problem
    <ul>
      <li>보상의 확률이 변하는 reinforcement learning 문제에서는 오랜 과거 보상보다 최근 보상에 비중을 더 두는 것이 설득력있다.</li>
      <li>
        <p>이러한 방법 중 하나로 상수 파라미터 (a constant step-size parameter) 를 사용하는 것이 유명하다.</p>

        <p><img src="/assets/images/posts/2_5_1_constant_step_size_parameter.png" alt="2_5_1_constant_step_size_parameter" /></p>

        <ul>
          <li>위의 수식을 weighted average 라고도 하는데 각 가중치의 합이 1이 되기 때문이다.
            <ul>
              <li>$(1-\alpha)^n + \sum_{i=1}^n\alpha(1-\alpha)^{n-i} = 1$</li>
            </ul>
          </li>
          <li>$1-\alpha$ 값이 1보다 작기 때문에 승수가 커질수록 (이전 step 의 값일수록) 가중치 값이 decay 됨</li>
          <li>때로는 위 수식을 exponential recency-weighted average 라고도 함 (지수적 최근성 가중치 평균)</li>
        </ul>
      </li>
      <li>때로는 step 별로 변동하는 step-size parameter 를 사용하는 것이 편할 때가 있음.
        <ul>
          <li>예를 들어 $\frac{1}{n}$ step-size parameter (sample-average method) 는 충분히 큰 step 을 진행할 경우 true action value 로 수렴하는 것을 보장한다.</li>
        </ul>
      </li>
      <li>
        <p>확률적 근사 이론은 확률 1로 수렴을 보장하는 데 필요한 조건을 제공한다.</p>

        <p><img src="/assets/images/posts/2_5_2_conditions_required_to_convergence_with_prob1.png" alt="2_5_2_conditions_required_to_convergence_with_prob1" /></p>

        <ul>
          <li>첫번째 조건은 초기 조건이나 무작위 변동을 극복할 수 있을 정도로 step-size 가 큰 것을 보장</li>
          <li>두번째 조건은 step-size 가 수렴을 확신할 정도로 충분히 작은 것을 보장</li>
          <li>$\frac{1}{n}$ 은 이 두 조건을 모두 만족하나, 상수 step 파라미터는 두번째 조건을 충족하지 않아 가장 최근의 보상값에 의해 완전히 수렴하지 못하게 됨.
            <ul>
              <li>이것은 비정상성 환경에서 필요한 내용이다.</li>
            </ul>
          </li>
          <li>두 조건이 만족하더라도 매우 느리게 수렴하거나, 만족스러운 수렴율을 얻기 위해 파라미터를 튜닝해야 할 수도 있음.</li>
          <li>위 이론은 이론적인 내용에는 자주 사용되나, 실제 적용 환경에서는 잘 사용되지 않음.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimistic Initial Values
    <ul>
      <li>위에 언급한 모든 방법들은 initial action-value estimates ($Q_1(a)$) 에 어느정도 영향을 받는다.</li>
      <li>통계적 표현으로 이러한 방식들은 biased by their initial estimates (초기추정치에 의해 편향된다) 라고 한다.</li>
      <li>예를 들어 sample-average methods 는 이 편향이 모든 action을 한 번 이상씩 수행했을 때 사라진다면
        <ul>
          <li>액션 a=1 to k 에 대해 초기값 $Q(a)$, $N(a)$ 를 초기값으로 쓰고, 한 번이라도 a 가 시도되면 수식에 따른 값으로 변경</li>
          <li>n 으로 나눌 때 0 으로 나눌 수 없으므로..</li>
        </ul>
      </li>
      <li>상수 a 를 사용하는 경우 이 편향은 영구적이다. (시간의 흐름에 따라 점차 감소하지만)
        <ul>
          <li>별도의 초기값 할당이 아니라 처음부터 수식을 사용하되, 수식 내 0번째 스텝의 값이 초기값임.</li>
        </ul>
      </li>
      <li>장점 : 예상할 수 있는 보상 수준에 대한 사전지식을 제공하는 쉬운 방법</li>
      <li>단점 : 모든 매개변수를 0 으로 설정하는 경우 사용자가 반드시 파라미터를 선택해야 한다.</li>
      <li>간단한 탐색 장려의 방법 : 초기 값을 0 대신 +5 로 설정 (10-armed testbed 상황으로 가정)
        <ul>
          <li>초기값 +5 의 값은 매우 낙관적인 수치</li>
          <li>특정 action 을 선택하고 받는 보상 값이 예측치보다 작음 (disappointed with the rewards)</li>
          <li>학습자는 다른 action 을 선택하게 되고 이 상황을 몇번 반복됨. (greedy action 일지라도…)</li>
        </ul>

        <p><img src="/assets/images/posts/2_6_1_optimistic_greedy.png" alt="2_6_1_optimistic_greedy" /></p>

        <ul>
          <li>10-armed bandit testbed 에서 greedy method 를 초기값 $Q_1(a) = +5$ 로 세팅한 결과</li>
          <li>비교군은 $\varepsilon$-greedy method 에 초기값 $Q_1(a) = 0$</li>
          <li>이 trick 은 stationary problem 에서 꽤 효과적이나, 탐색을 장려하는 일반적인 방법은 아님.</li>
          <li>이러한 비판은 sample-average methods 에서도 통용된다.
            <ul>
              <li>초기 시점을 특수한 이벤트로 여긴다.</li>
              <li>모든 보상을 똑같은 가중치로 평균을 구한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Upper-Confidence-Bound Action Selection
    <ul>
      <li>action-value 추정값의 불확실성 때문에 탐험은 반드시 필요하다.</li>
      <li>greedy actions 는 현재 시점에는 가장 최적의 선택이나 다른 action 이 실제로는 더 좋은 것일 수 있다.</li>
      <li>$\varepsilon$-greedy action selection 은 강제적으로 non-greedy action 을 선택하지만 선호도 없이 무차별적인 선택을 하여 greedy 한 선택을 하게 될 수도 있다.
        <ul>
          <li>강제적인 선택을 할 때는 non-greedy 한 선택을 하는 것이 좋음</li>
          <li>추정치가 최대치에 얼마나 가까운지와 추정치의 불확실성을 모두 고려</li>
          <li>실제로 최적일 가능성에 따라 탐욕스럽지 않은 action 을 선택하는 것이 좋음.</li>
        </ul>

        <p><img src="/assets/images/posts/2_7_1_UCB.png" alt="2_7_1_UCB" /></p>

        <ul>
          <li>UCB (Upper-Confidence-Bound) Action Selection 은 그러한 효율적인 방식 중 하나이다.</li>
          <li>$N_t(a)$ 는 t step 이 진행되었을때 a action 이 선택된 횟수로, 많이 선택될 수록 우항의 피연산자 값이 작아진다.</li>
          <li>$\ln t$ 는 step 이 커짐에 따라 값이 무한대까지 증가 (수렴하지 않음) 하나 그 증가폭이 서서히 줄어든다</li>
          <li>$c$ 는 탐험의 정도 (강도) 를 나타내는 수치이다.</li>
          <li>즉 action이 많이 선택될수록 $Q_t(a)$ 의 값은 정확해지고, 우항의 피연산자 값은 작아진다.</li>
          <li>action 이 한번도 선택되지 않을 경우 해당 a 를 maximizing action 으로 여긴다. (해당 a 에 대한 무조건적인 탐험)</li>
        </ul>

        <p><img src="/assets/images/posts/2_7_2_UCB.png" alt="2_7_2_UCB" /></p>

        <ul>
          <li>이러한 UCB 방식은 10-armed testbed 에서 $\varepsilon$-greedy 보다 나은 성과를 보여주기도 한다.</li>
          <li>단 몇가지 단점으로 인해 실용적이지 않은 방식이다.
            <ul>
              <li>bandits 문제에서 reinforcement learning 문제로 확장하기 어렵다.</li>
              <li>nonstationary 한 문제들에는 더 어려운 방식의 action-value method 가 필요하다.</li>
              <li>훨신 더 거대한 환경 (state space) 에서의 적용 (특히 뒤에 배울 function approximation 방식) 이 어렵다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lesson-1-the-k-armed-bandit-problem">Lesson 1: The K-Armed Bandit Problem</h2>

<ul>
  <li>학습목표
    <ul>
      <li>Define reward</li>
      <li>Understand the temporal nature of the bandit problem</li>
      <li>Define k-armed bandit</li>
      <li>Define action-values</li>
    </ul>
  </li>
  <li>Sequential Decision Making with Evaluative Feedback
    <ul>
      <li>불확실성 아래에서의 의사결정
        <ul>
          <li>의사가 3가지 처방약으로 환자에게 실험적 처방을 할 때…</li>
          <li>몇번의 환자 반응을 보고 가장 잘 듣는 약을 고집할 경우
            <ul>
              <li>더이상 다른 약의 데이터를 모을 수 없음</li>
              <li>나머지 두 약이 실제로 나음에도 몇몇 결과가 나쁘게 나왔는지 알 수 없음</li>
            </ul>
          </li>
          <li>다른 약으로 계속 실험할 경우
            <ul>
              <li>나쁜 결과를 계속 초래할 수 있음</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>강화학습에서의 기초적 컨셉 용어
        <ul>
          <li>에이전트 (Agent) : action을 선택하는 존재 - 의사</li>
          <li>액션 (Action) : 선택지 - 3가지 약 중 하나를 선택하는 것</li>
          <li>보상 (Rewards) : 액션에 대한 결과 - 환자의 상태</li>
          <li>값 (Value Function - Action-Value (Function)) : 기대되는 보상 값 - 환자의 혈압 값
            <ul>
              <li>에이전트가 액션을 선택했을 때 그 값 (Action-Value = $q_*$)이 최대화 할 경우 목적을 달성함</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$q_*(a)$ 값 구하기
        <ul>
          <li>각 약의 결과값이 서로 다른 확률분포를 가졌을 경우 $q_*$ 는 각 분포의 평균이 될 수 있다.</li>
        </ul>
      </li>
      <li>Bandits Problem 을 고려하는 이유
        <ul>
          <li>문제와 알고리즘 디자인 선택에 있어 가장 간단한 세팅</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lesson-2-what-to-learn-estimating-action-values">Lesson 2: What to Learn? Estimating Action Values</h2>

<ul>
  <li>학습목표
    <ul>
      <li>Define action-value estimation methods</li>
      <li>Define exploration and exploitation</li>
      <li>Select actions greedily using an action-value function</li>
      <li>Define online learning</li>
      <li>Understand a simple online sample-average action-value estimation method</li>
      <li>Define the general online update equation</li>
      <li>Understand why we might use a constant step-size in the case of non-stationarity</li>
    </ul>
  </li>
  <li>Learning Action Values
    <ul>
      <li>Estimate action values using the sample-average method
        <ul>
          <li>의사가 처방 후 환자가 나아질 경우 1, 그렇지 않은 경우 0 으로 표기하고 3개의 약을 처방해 평균을 구함</li>
          <li>Step 이 많이 진행될 수록 평균 데이터는 더 정확해짐</li>
        </ul>
      </li>
      <li>Describe greedy action selection
        <ul>
          <li>의시가 해당 시점에 가장 기대치가 큰 약을 처방할 경우 이 행동을 Greedy action 이라고 함</li>
          <li>greedy action 을 선택하는 것을 현 지식을 활용한 이용 (exploitation) 이라고 함</li>
          <li>당장의 기대되는 보상을 포기하고 다른 선택을 하는 것을 non-greedy action 이라고 하고 이를 탐험 (exploration) 이라고 함</li>
          <li>탐험을 통해 기대되는 보상을 희생하고 non-greedy action 의 보상에 대한 더 많은 정보를 얻게 됨</li>
        </ul>
      </li>
      <li>Introduce the exploration-exploitation dilemma
        <ul>
          <li>에이전트는 동시에 탐험과 이용을 할 수 없음</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Estimating Action Values Incrementally
    <ul>
      <li>action value 의 점진적 표현법 (sample-average method 를 이용)
        <ul>
          <li>Incremental update rule</li>
          <li>NewEstimate &lt;- OldEstimate + StepSize(Target - OldEstimate)</li>
        </ul>
      </li>
      <li>점진적 업데이트 룰(Incremental update rule)이 더 널리 쓰이는 이유
        <ul>
          <li>모든 이전 값들을 기억할 필요가 없다.</li>
        </ul>
      </li>
      <li>보편적 업데이트 룰을 non-stationary bandit problem 에 적용하는 방법
        <ul>
          <li>Non-stationary Bandit Problem : 의사의 3가지 약중 특정 하나의 약이 겨울이 되면 효율이 높아진다.</li>
          <li>보상의 분포가 시간에 따라 변하게 되는 경우를 Non-stationary 하다 라고 함.</li>
          <li>StepSize 파라미터가 상수 (예:0.1) 일 경우 이전 Step 일수록 영향도가 작아지고, 최신 Step의 보상값을 더 반영함.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="lesson-3-exploration-vs-exploitation-tradeoff">Lesson 3: Exploration vs. Exploitation Tradeoff</h2>

<ul>
  <li>학습목표
    <ul>
      <li>Define epsilon-greedy</li>
      <li>Compare the short-term benefits of exploitation and the long-term benefits of exploration</li>
      <li>Understand optimistic initial values</li>
      <li>Describe the benefits of optimistic initial values for early exploration</li>
      <li>Explain the criticisms of optimistic initial values</li>
      <li>Describe the upper confidence bound action selection method</li>
      <li>Define optimism in the face of uncertainty</li>
    </ul>
  </li>
  <li>What is the trade-off?
    <ul>
      <li>exploration-exploitation 의 등가교환
        <ul>
          <li>Exploration (탐색) : 장기적 이익을 위해 지식을 늘림</li>
          <li>Exploitation (이용) : 단기적 이익을 위해 지식을 이용</li>
          <li>탐색만 하면 단기적 보상이 작아지고, 이용만 하면 타 선택지의 true value 를 모르기 때문에 장기적으로 손해일 수 있음</li>
          <li>한번의 선택에서 탐색 과 이용 둘 중 하나만 가능</li>
        </ul>
      </li>
      <li>Epsilon-Greedy 방식 (탐색과 이용의 균형을 맞추는 쉬운 방법)
        <ul>
          <li>Epsilon-Greedy 는 대부분 Exploitation (Greedy method) 하고, 적은 확률로 Exploration (Random choice) 한다.</li>
        </ul>
      </li>
      <li>각 방식을 비교할 때 한 번의 진행으로는 노이즈가 많아 확인이 어려움.
        <ul>
          <li>1000 개의 에이전트로 보상 데이터를 모아 평균값으로 비교</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimistic Initial Values
    <ul>
      <li>Optimistic initial values 가 초기 탐색을 장려하는 이유
        <ul>
          <li>초기 예상 보상치를 높게 잡고 시작</li>
          <li>첫 선택의 보상이 주어지더라도 평균값으로 인해 값이 떨어짐</li>
          <li>에이전트는 첫 선택에 실망을 하고 선택되지 않은 다른 높은 초기치의 옵션 중 하나를 선택</li>
          <li>초기 탐험을 통해 모든 Action 을 골고루 선택하게 됨.</li>
        </ul>
      </li>
      <li>Optimistic initial values 의 한계
        <ul>
          <li>초기 단계에서만 탐색을 진행한다. 이는 Non-stationary Problems 에서 문제가 됨.</li>
          <li>Maximal Reward 를 시작하기 전 알 방법이 없기에 Optimistic Initial Value 를 어느정도로 설정해야 할지 모른다.</li>
          <li>그럼에도 Optimistic initial values 는 휴리스틱 (충분한 정보 없이 빠르게 사용할 수 있는 직관적인) 한 방법으로 자주 활용됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Upper-Confidence Bound(UCB) Action Selection
    <ul>
      <li>UCB action-selection 방식이 예측의 불확실성을 이용해 탐색을 유도하는 방법</li>
      <li>UCB의 C는 confidence 를 뜻하며, 예측한 $Q(a)$ 값의 오차범주 범위 $(c)$ 내에 실제 값이 들어올 것이라 확신하는 정도의 수치이다.
        <ul>
          <li>범위가 작을수록 결과에 더욱 확신한다는 뜻임</li>
        </ul>
      </li>
      <li>불확실성 (범위) 에 마주했을때 가장 높은 값 (Upper-Confidence Bound) 을 선택</li>
      <li>수식에 대한 풀이
        <ul>
          <li>c : 사용자 정의 파라미터 (얼마나 탐험을 할 건지 컨트롤)</li>
          <li>좌측 피연산자 : Exploit (이용)</li>
          <li>우측 피연산자 : Explore (탐험)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Contextual Bandits for Real World Reinforcement Learning
    <ul>
      <li>Contextual Bandits 는 Reinforcement Learning 이 현실에 배포되는 방식임</li>
      <li>Reinforcement 는 현실과 어떻게 다른가?
        <ul>
          <li>Reinforcement Learning 은 시뮬레이터로 동작한다.
            <ul>
              <li>시뮬레이터는 관측치를 제공</li>
              <li>Learning 알고리즘은 어떤 Action을 선택할 지 정책을 가진다.</li>
              <li>시뮬레이터는 실행하고 보상을 제공한다.</li>
            </ul>
          </li>
          <li>현실이 제공하는 관측값은 시뮬레이터와 다르다.
            <ul>
              <li>같은 정책이라 해도 관측치가 다르기 때문에 시뮬레이터에서의 Action 과 다른 Action 을 수행하게 된다.</li>
              <li>시뮬레이터와 현실은 보상도 다르다.</li>
            </ul>
          </li>
          <li>즉 현실과 시뮬레이터에는 갭이 존재한다.</li>
        </ul>
      </li>
      <li>Real World based Reinforcement Learning : 현실 기반 강화학습을 하려면 우선순위를 변경해야 함
        <ul>
          <li>Temporal Credit Assignment &lt; Generalization
            <ul>
              <li>Temporal Credit Assignment Problem (CAP) : 시간적 기여도 할당문제. 일련의 행동이 모두 끝난 뒤 보상을 얻을 수 있는 환경에서 수행한 행동들 중 어떠한 행동이 기여도가 있고 어떠한 행동이 벌점을 줄 것인지 결정하는 문제.</li>
              <li>Generalization : 다양한 관찰값을 통한 일반화</li>
            </ul>
          </li>
          <li>Control environment &lt; Environment controls
            <ul>
              <li>시뮬레이션 환경에서는 자유자재로 한 스텝을 더 진행하거나 할 수 있다면 (컨트롤 가능)</li>
              <li>현실에서는 환경이 지배함. (환경에 의해 컨트롤당함)</li>
            </ul>
          </li>
          <li>Computational efficiency &lt; Statistical efficiency
            <ul>
              <li>시뮬레이션 환경에서 계산효율이 중요하다면 (학습할 샘플이 많으므로)</li>
              <li>현실에서는 통계적 효율이 중요함 (현실이 주는 샘플만 가질 수 있다.)</li>
            </ul>
          </li>
          <li>State &lt; Features
            <ul>
              <li>시뮬레이션에서는 상태를 생각 (상태값은 결정을 내리는 데 기반이 되는 요소)</li>
              <li>현실에서는 매우 복잡한 관측값을 가지는 경우 (필요한 것보다 정보가 많음) 가 많아 어느 것이 핵심 요인인지가 중요</li>
            </ul>
          </li>
          <li>Learning &lt; Evaluation
            <ul>
              <li>현실에서는 “Off Policy Evaluation” 이 중요하다.
                <ul>
                  <li>Off Policy : 정책 업데이트에 어떤 데이터를 써도 상관이 없는 경우</li>
                  <li>즉, 최신 업데이트 정책에서 수집된 데이터가 아니어도 사용가능</li>
                  <li>학습 알고리즘이 학습 뿐만 아니라 정책 평가에 쓰일 수 있는 부산물 데이터 또한 제공</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Last policy &lt; Every Policy
            <ul>
              <li>시뮬레이션에서는 가장 최신의 정책이 중요</li>
              <li>현실에서는 모든 포인트의 데이터가 세상과의 어느 정도 상호작용이 포함되어 있음</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="chapter-summary-rlbook2018-pages-42-43">Chapter Summary (RLbook2018 Pages 42-43)</h2>

<h2 id="weekly-assessment">Weekly Assessment</h2>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#adam-white" class="page__taxonomy-item" rel="tag">Adam White</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#alberta-machine-intelligence-institute" class="page__taxonomy-item" rel="tag">Alberta Machine Intelligence Institute</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#coursera" class="page__taxonomy-item" rel="tag">Coursera</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#martha-white" class="page__taxonomy-item" rel="tag">Martha White</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5" class="page__taxonomy-item" rel="tag">강화학습</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#%EC%95%A8%EB%B2%84%ED%83%80-%EB%8C%80%ED%95%99%EA%B5%90" class="page__taxonomy-item" rel="tag">앨버타 대학교</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#ai" class="page__taxonomy-item" rel="tag">AI</a><span class="sep">, </span>
    
      
      
      <a href="/categories/#reinforcement-learning" class="page__taxonomy-item" rel="tag">Reinforcement Learning</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2023-01-26T18:00:00+09:00">January 26, 2023</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Fundamentals+of+Reinforcement+Learning+-+01.+Week+1%20https%3A%2F%2Fbluesplatter.com%2Fai%2Freinforcement%2520learning%2FFundamentals_of_Reinforcement_Learning_01_Week1%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fbluesplatter.com%2Fai%2Freinforcement%2520learning%2FFundamentals_of_Reinforcement_Learning_01_Week1%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fbluesplatter.com%2Fai%2Freinforcement%2520learning%2FFundamentals_of_Reinforcement_Learning_01_Week1%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/wine/Textbook_Napa_Cabernet_Sauvignon_2020/" class="pagination--pager" title="Textbook Napa Cabernet Sauvignon 2020. 텍스트북 카베르네소비뇽 2020
">이전</a>
    
    
      <a href="/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_02_Week2/" class="pagination--pager" title="Fundamentals of Reinforcement Learning - 02. Week 2. Markov Decision Processes
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">참고</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Sample_based_Learning_Methods_01_Week1/" rel="permalink">Sample-based Learning Methods - 01. Week 1. Monte Carlo Methods for Prediction &amp; Control
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-06-12T15:00:00+09:00">June 12, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">관련 자료 (RLbook2018 Pages 91-104)
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Sample_based_Learning_Methods_00_About-this-course/" rel="permalink">Sample-based Learning Methods - 00. 강좌소개
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-06-09T14:00:00+09:00">June 9, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">강좌에 대한 설명
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_04_Week4/" rel="permalink">Fundamentals of Reinforcement Learning - 04. Week 4. Dynamic Programming
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-04-20T15:00:00+09:00">April 20, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">관련 자료 (RLbook2018 Pages 73-88)
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_03_Week3/" rel="permalink">Fundamentals of Reinforcement Learning - 03. Week 3. Value Functions &amp; Bellman Equations
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2023-03-16T15:00:00+09:00">March 16, 2023</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">관련 자료 (RLbook2018 Pages 58-67)
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="검색어를 입력하세요..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<!--
<script>
	(function(d, h, m){
    var js, fjs = d.getElementsByTagName(h)[0];
    if (d.getElementById(m)){return;}
    js = d.createElement(h); js.id = m;
    js.onload = function(){
        window.makerWidgetComInit({
        position: "left",          
        widget: "eqypoqmawbcz3azh-usiy9q7ma4ong1w1-juelcm3zfgcuwjds"                
    })};
    js.src = "https://makerwidget.com/js/embed.js";
    fjs.parentNode.insertBefore(js, fjs)
	}(document, "script", "dhm"))
</script>
-->
<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
          <li><a href="mailto:hyunik03@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
      
        
      
        
          <li><a href="https://github.com/HY03" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 HY03. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "https://bluesplatter.com/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/ai/reinforcement%20learning/Fundamentals_of_Reinforcement_Learning_01_Week1"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://bluesplatter.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
